begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2010 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Callable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CompletionService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CopyOnWriteArraySet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutionException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorCompletionService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ThreadPoolExecutor
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicLong
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|locks
operator|.
name|ReentrantReadWriteLock
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|classification
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HRegionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
operator|.
name|KVComparator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|RemoteExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Scan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|HeapSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|CacheConfig
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|Compression
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFileDataBlockEncoder
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFileDataBlockEncoderImpl
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFileScanner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|InvalidHFileException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|NoOpDataBlockEncoder
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|monitoring
operator|.
name|MonitoredTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|StoreScanner
operator|.
name|ScanType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|compactions
operator|.
name|CompactSelection
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|compactions
operator|.
name|CompactionProgress
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|compactions
operator|.
name|CompactionRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|metrics
operator|.
name|SchemaConfigured
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|metrics
operator|.
name|SchemaMetrics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ChecksumType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ClassSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|CollectionBackedScanner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|EnvironmentEdgeManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|FSUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Predicate
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Collections2
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|ImmutableList
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_comment
comment|/**  * A Store holds a column family in a Region.  Its a memstore and a set of zero  * or more StoreFiles, which stretch backwards over time.  *  *<p>There's no reason to consider append-logging at this level; all logging  * and locking is handled at the HRegion level.  Store just provides  * services to manage sets of StoreFiles.  One of the most important of those  * services is compaction services where files are aggregated once they pass  * a configurable threshold.  *  *<p>The only thing having to do with logs that Store needs to deal with is  * the reconstructionLog.  This is a segment of an HRegion's log that might  * NOT be present upon startup.  If the param is NULL, there's nothing to do.  * If the param is non-NULL, we need to process the log to reconstruct  * a TreeMap that might not have been written to disk before the process  * died.  *  *<p>It's assumed that after this constructor returns, the reconstructionLog  * file will be deleted (by whoever has instantiated the Store).  *  *<p>Locking and transactions are handled at a higher level.  This API should  * not be called directly but by an HRegion manager.  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Private
specifier|public
class|class
name|Store
extends|extends
name|SchemaConfigured
implements|implements
name|HeapSize
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|Store
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|protected
specifier|final
name|MemStore
name|memstore
decl_stmt|;
comment|// This stores directory in the filesystem.
specifier|private
specifier|final
name|Path
name|homedir
decl_stmt|;
specifier|private
specifier|final
name|HRegion
name|region
decl_stmt|;
specifier|private
specifier|final
name|HColumnDescriptor
name|family
decl_stmt|;
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|final
name|CacheConfig
name|cacheConf
decl_stmt|;
comment|// ttl in milliseconds.
specifier|private
name|long
name|ttl
decl_stmt|;
specifier|private
specifier|final
name|int
name|minFilesToCompact
decl_stmt|;
specifier|private
specifier|final
name|int
name|maxFilesToCompact
decl_stmt|;
specifier|private
specifier|final
name|long
name|minCompactSize
decl_stmt|;
specifier|private
specifier|final
name|long
name|maxCompactSize
decl_stmt|;
specifier|private
name|long
name|lastCompactSize
init|=
literal|0
decl_stmt|;
specifier|volatile
name|boolean
name|forceMajor
init|=
literal|false
decl_stmt|;
comment|/* how many bytes to write between status checks */
specifier|static
name|int
name|closeCheckInterval
init|=
literal|0
decl_stmt|;
specifier|private
specifier|final
name|int
name|blockingStoreFileCount
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|storeSize
init|=
literal|0L
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|totalUncompressedBytes
init|=
literal|0L
decl_stmt|;
specifier|private
specifier|final
name|Object
name|flushLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|final
name|ReentrantReadWriteLock
name|lock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|verifyBulkLoads
decl_stmt|;
comment|/* The default priority for user-specified compaction requests.    * The user gets top priority unless we have blocking compactions. (Pri<= 0)    */
specifier|public
specifier|static
specifier|final
name|int
name|PRIORITY_USER
init|=
literal|1
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|NO_PRIORITY
init|=
name|Integer
operator|.
name|MIN_VALUE
decl_stmt|;
comment|// not private for testing
comment|/* package */
name|ScanInfo
name|scanInfo
decl_stmt|;
comment|/*    * List of store files inside this store. This is an immutable list that    * is atomically replaced when its contents change.    */
specifier|private
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|storefiles
init|=
literal|null
decl_stmt|;
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesCompacting
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
comment|// All access must be synchronized.
specifier|private
specifier|final
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
name|changedReaderObservers
init|=
operator|new
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|int
name|blocksize
decl_stmt|;
specifier|private
name|HFileDataBlockEncoder
name|dataBlockEncoder
decl_stmt|;
comment|/** Checksum configuration */
specifier|private
name|ChecksumType
name|checksumType
decl_stmt|;
specifier|private
name|int
name|bytesPerChecksum
decl_stmt|;
comment|// Comparing KeyValues
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|comparator
decl_stmt|;
specifier|private
specifier|final
name|Compactor
name|compactor
decl_stmt|;
comment|/**    * Constructor    * @param basedir qualified path under which the region directory lives;    * generally the table subdirectory    * @param region    * @param family HColumnDescriptor for this column    * @param fs file system object    * @param confParam configuration object    * failed.  Can be null.    * @throws IOException    */
specifier|protected
name|Store
parameter_list|(
name|Path
name|basedir
parameter_list|,
name|HRegion
name|region
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Configuration
name|confParam
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
operator|new
name|CompoundConfiguration
argument_list|()
operator|.
name|add
argument_list|(
name|confParam
argument_list|)
operator|.
name|add
argument_list|(
name|family
operator|.
name|getValues
argument_list|()
argument_list|)
argument_list|,
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|getTableNameAsString
argument_list|()
argument_list|,
name|Bytes
operator|.
name|toString
argument_list|(
name|family
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|HRegionInfo
name|info
init|=
name|region
operator|.
name|getRegionInfo
argument_list|()
decl_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
comment|// Assemble the store's home directory.
name|Path
name|p
init|=
name|getStoreHomedir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
comment|// Ensure it exists.
name|this
operator|.
name|homedir
operator|=
name|createStoreHomeDir
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|p
argument_list|)
expr_stmt|;
name|this
operator|.
name|region
operator|=
name|region
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
comment|// 'conf' renamed to 'confParam' b/c we use this.conf in the constructor
name|this
operator|.
name|conf
operator|=
operator|new
name|CompoundConfiguration
argument_list|()
operator|.
name|add
argument_list|(
name|confParam
argument_list|)
operator|.
name|add
argument_list|(
name|family
operator|.
name|getValues
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|blocksize
operator|=
name|family
operator|.
name|getBlocksize
argument_list|()
expr_stmt|;
name|this
operator|.
name|dataBlockEncoder
operator|=
operator|new
name|HFileDataBlockEncoderImpl
argument_list|(
name|family
operator|.
name|getDataBlockEncodingOnDisk
argument_list|()
argument_list|,
name|family
operator|.
name|getDataBlockEncoding
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|comparator
operator|=
name|info
operator|.
name|getComparator
argument_list|()
expr_stmt|;
comment|// Get TTL
name|this
operator|.
name|ttl
operator|=
name|getTTL
argument_list|(
name|family
argument_list|)
expr_stmt|;
comment|// used by ScanQueryMatcher
name|long
name|timeToPurgeDeletes
init|=
name|Math
operator|.
name|max
argument_list|(
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hstore.time.to.purge.deletes"
argument_list|,
literal|0
argument_list|)
argument_list|,
literal|0
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|trace
argument_list|(
literal|"Time to purge deletes set to "
operator|+
name|timeToPurgeDeletes
operator|+
literal|"ms in store "
operator|+
name|this
argument_list|)
expr_stmt|;
comment|// Why not just pass a HColumnDescriptor in here altogether?  Even if have
comment|// to clone it?
name|scanInfo
operator|=
operator|new
name|ScanInfo
argument_list|(
name|family
operator|.
name|getName
argument_list|()
argument_list|,
name|family
operator|.
name|getMinVersions
argument_list|()
argument_list|,
name|family
operator|.
name|getMaxVersions
argument_list|()
argument_list|,
name|ttl
argument_list|,
name|family
operator|.
name|getKeepDeletedCells
argument_list|()
argument_list|,
name|timeToPurgeDeletes
argument_list|,
name|this
operator|.
name|comparator
argument_list|)
expr_stmt|;
name|this
operator|.
name|memstore
operator|=
operator|new
name|MemStore
argument_list|(
name|conf
argument_list|,
name|this
operator|.
name|comparator
argument_list|)
expr_stmt|;
comment|// By default, compact if storefile.count>= minFilesToCompact
name|this
operator|.
name|minFilesToCompact
operator|=
name|Math
operator|.
name|max
argument_list|(
literal|2
argument_list|,
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.min"
argument_list|,
comment|/*old name*/
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"hbase.hstore.compaction.min = "
operator|+
name|this
operator|.
name|minFilesToCompact
argument_list|)
expr_stmt|;
comment|// Setting up cache configuration for this family
name|this
operator|.
name|cacheConf
operator|=
operator|new
name|CacheConfig
argument_list|(
name|conf
argument_list|,
name|family
argument_list|)
expr_stmt|;
name|this
operator|.
name|blockingStoreFileCount
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.blockingStoreFiles"
argument_list|,
literal|7
argument_list|)
expr_stmt|;
name|this
operator|.
name|maxFilesToCompact
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.max"
argument_list|,
literal|10
argument_list|)
expr_stmt|;
name|this
operator|.
name|minCompactSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hstore.compaction.min.size"
argument_list|,
name|this
operator|.
name|region
operator|.
name|memstoreFlushSize
argument_list|)
expr_stmt|;
name|this
operator|.
name|maxCompactSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hstore.compaction.max.size"
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
expr_stmt|;
name|this
operator|.
name|verifyBulkLoads
operator|=
name|conf
operator|.
name|getBoolean
argument_list|(
literal|"hbase.hstore.bulkload.verify"
argument_list|,
literal|false
argument_list|)
expr_stmt|;
if|if
condition|(
name|Store
operator|.
name|closeCheckInterval
operator|==
literal|0
condition|)
block|{
name|Store
operator|.
name|closeCheckInterval
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.close.check.interval"
argument_list|,
literal|10
operator|*
literal|1000
operator|*
literal|1000
comment|/* 10 MB */
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|loadStoreFiles
argument_list|()
argument_list|)
expr_stmt|;
comment|// Initialize checksum type from name. The names are CRC32, CRC32C, etc.
name|this
operator|.
name|checksumType
operator|=
name|getChecksumType
argument_list|(
name|conf
argument_list|)
expr_stmt|;
comment|// initilize bytes per checksum
name|this
operator|.
name|bytesPerChecksum
operator|=
name|getBytesPerChecksum
argument_list|(
name|conf
argument_list|)
expr_stmt|;
comment|// Create a compaction tool instance
name|this
operator|.
name|compactor
operator|=
operator|new
name|Compactor
argument_list|(
name|this
operator|.
name|conf
argument_list|)
expr_stmt|;
block|}
comment|/**    * @param family    * @return    */
name|long
name|getTTL
parameter_list|(
specifier|final
name|HColumnDescriptor
name|family
parameter_list|)
block|{
comment|// HCD.getTimeToLive returns ttl in seconds.  Convert to milliseconds.
name|long
name|ttl
init|=
name|family
operator|.
name|getTimeToLive
argument_list|()
decl_stmt|;
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
condition|)
block|{
comment|// Default is unlimited ttl.
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|ttl
operator|==
operator|-
literal|1
condition|)
block|{
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
else|else
block|{
comment|// Second -> ms adjust for user data
name|ttl
operator|*=
literal|1000
expr_stmt|;
block|}
return|return
name|ttl
return|;
block|}
comment|/**    * Create this store's homedir    * @param fs    * @param homedir    * @return Return<code>homedir</code>    * @throws IOException    */
name|Path
name|createStoreHomeDir
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|homedir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|homedir
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|homedir
argument_list|)
condition|)
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Failed create of: "
operator|+
name|homedir
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
return|return
name|homedir
return|;
block|}
name|FileSystem
name|getFileSystem
parameter_list|()
block|{
return|return
name|this
operator|.
name|fs
return|;
block|}
comment|/**    * Returns the configured bytesPerChecksum value.    * @param conf The configuration    * @return The bytesPerChecksum that is set in the configuration    */
specifier|public
specifier|static
name|int
name|getBytesPerChecksum
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|conf
operator|.
name|getInt
argument_list|(
name|HConstants
operator|.
name|BYTES_PER_CHECKSUM
argument_list|,
name|HFile
operator|.
name|DEFAULT_BYTES_PER_CHECKSUM
argument_list|)
return|;
block|}
comment|/**    * Returns the configured checksum algorithm.    * @param conf The configuration    * @return The checksum algorithm that is set in the configuration    */
specifier|public
specifier|static
name|ChecksumType
name|getChecksumType
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|String
name|checksumName
init|=
name|conf
operator|.
name|get
argument_list|(
name|HConstants
operator|.
name|CHECKSUM_TYPE_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
name|checksumName
operator|==
literal|null
condition|)
block|{
return|return
name|HFile
operator|.
name|DEFAULT_CHECKSUM_TYPE
return|;
block|}
else|else
block|{
return|return
name|ChecksumType
operator|.
name|nameToType
argument_list|(
name|checksumName
argument_list|)
return|;
block|}
block|}
specifier|public
name|HColumnDescriptor
name|getFamily
parameter_list|()
block|{
return|return
name|this
operator|.
name|family
return|;
block|}
comment|/**    * @return The maximum sequence id in all store files.    */
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|this
operator|.
name|getStorefiles
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * @return The maximum memstoreTS in all store files.    */
specifier|public
name|long
name|getMaxMemstoreTS
parameter_list|()
block|{
return|return
name|StoreFile
operator|.
name|getMaxMemstoreTSInList
argument_list|(
name|this
operator|.
name|getStorefiles
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * @param tabledir    * @param encodedName Encoded region name.    * @param family    * @return Path to family/Store home directory.    */
specifier|public
specifier|static
name|Path
name|getStoreHomedir
parameter_list|(
specifier|final
name|Path
name|tabledir
parameter_list|,
specifier|final
name|String
name|encodedName
parameter_list|,
specifier|final
name|byte
index|[]
name|family
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|tabledir
argument_list|,
operator|new
name|Path
argument_list|(
name|encodedName
argument_list|,
operator|new
name|Path
argument_list|(
name|Bytes
operator|.
name|toString
argument_list|(
name|family
argument_list|)
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Return the directory in which this store stores its    * StoreFiles    */
name|Path
name|getHomedir
parameter_list|()
block|{
return|return
name|homedir
return|;
block|}
comment|/**    * @return the data block encoder    */
specifier|public
name|HFileDataBlockEncoder
name|getDataBlockEncoder
parameter_list|()
block|{
return|return
name|dataBlockEncoder
return|;
block|}
comment|/**    * Should be used only in tests.    * @param blockEncoder the block delta encoder to use    */
name|void
name|setDataBlockEncoderInTest
parameter_list|(
name|HFileDataBlockEncoder
name|blockEncoder
parameter_list|)
block|{
name|this
operator|.
name|dataBlockEncoder
operator|=
name|blockEncoder
expr_stmt|;
block|}
name|FileStatus
index|[]
name|getStoreFiles
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|FSUtils
operator|.
name|listStatus
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|homedir
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Creates an unsorted list of StoreFile loaded in parallel    * from the given directory.    * @throws IOException    */
specifier|private
name|List
argument_list|<
name|StoreFile
argument_list|>
name|loadStoreFiles
parameter_list|()
throws|throws
name|IOException
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|FileStatus
name|files
index|[]
init|=
name|getStoreFiles
argument_list|()
decl_stmt|;
if|if
condition|(
name|files
operator|==
literal|null
operator|||
name|files
operator|.
name|length
operator|==
literal|0
condition|)
block|{
return|return
name|results
return|;
block|}
comment|// initialize the thread pool for opening store files in parallel..
name|ThreadPoolExecutor
name|storeFileOpenerThreadPool
init|=
name|this
operator|.
name|region
operator|.
name|getStoreFileOpenAndCloseThreadPool
argument_list|(
literal|"StoreFileOpenerThread-"
operator|+
name|this
operator|.
name|family
operator|.
name|getNameAsString
argument_list|()
argument_list|)
decl_stmt|;
name|CompletionService
argument_list|<
name|StoreFile
argument_list|>
name|completionService
init|=
operator|new
name|ExecutorCompletionService
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storeFileOpenerThreadPool
argument_list|)
decl_stmt|;
name|int
name|totalValidStoreFile
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|files
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
comment|// Skip directories.
if|if
condition|(
name|files
index|[
name|i
index|]
operator|.
name|isDir
argument_list|()
condition|)
block|{
continue|continue;
block|}
specifier|final
name|Path
name|p
init|=
name|files
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Check for empty file. Should never be the case but can happen
comment|// after data loss in hdfs for whatever reason (upgrade, etc.): HBASE-646
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|p
argument_list|)
operator|.
name|getLen
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Skipping "
operator|+
name|p
operator|+
literal|" because its empty. HBASE-646 DATA LOSS?"
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|// open each store file in parallel
name|completionService
operator|.
name|submit
argument_list|(
operator|new
name|Callable
argument_list|<
name|StoreFile
argument_list|>
argument_list|()
block|{
specifier|public
name|StoreFile
name|call
parameter_list|()
throws|throws
name|IOException
block|{
name|StoreFile
name|storeFile
init|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|p
argument_list|,
name|conf
argument_list|,
name|cacheConf
argument_list|,
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|dataBlockEncoder
argument_list|)
decl_stmt|;
name|passSchemaMetricsTo
argument_list|(
name|storeFile
argument_list|)
expr_stmt|;
name|storeFile
operator|.
name|createReader
argument_list|()
expr_stmt|;
return|return
name|storeFile
return|;
block|}
block|}
argument_list|)
expr_stmt|;
name|totalValidStoreFile
operator|++
expr_stmt|;
block|}
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|totalValidStoreFile
condition|;
name|i
operator|++
control|)
block|{
name|Future
argument_list|<
name|StoreFile
argument_list|>
name|future
init|=
name|completionService
operator|.
name|take
argument_list|()
decl_stmt|;
name|StoreFile
name|storeFile
init|=
name|future
operator|.
name|get
argument_list|()
decl_stmt|;
name|long
name|length
init|=
name|storeFile
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|length
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|+=
name|storeFile
operator|.
name|getReader
argument_list|()
operator|.
name|getTotalUncompressedBytes
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded "
operator|+
name|storeFile
operator|.
name|toStringDetailed
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|results
operator|.
name|add
argument_list|(
name|storeFile
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|ExecutionException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
operator|.
name|getCause
argument_list|()
argument_list|)
throw|;
block|}
finally|finally
block|{
name|storeFileOpenerThreadPool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
block|}
return|return
name|results
return|;
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|add
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|add
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|delete
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|delete
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Removes a kv from the memstore. The KeyValue is removed only    * if its key& memstoreTS matches the key& memstoreTS value of the    * kv parameter.    *    * @param kv    */
specifier|protected
name|void
name|rollback
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|this
operator|.
name|memstore
operator|.
name|rollback
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return All store files.    */
name|List
argument_list|<
name|StoreFile
argument_list|>
name|getStorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
return|;
block|}
comment|/**    * This throws a WrongRegionException if the HFile does not fit in this    * region, or an InvalidHFileException if the HFile is not valid.    */
name|void
name|assertBulkLoadHFileOk
parameter_list|(
name|Path
name|srcPath
parameter_list|)
throws|throws
name|IOException
block|{
name|HFile
operator|.
name|Reader
name|reader
init|=
literal|null
decl_stmt|;
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Validating hfile at "
operator|+
name|srcPath
operator|+
literal|" for inclusion in "
operator|+
literal|"store "
operator|+
name|this
operator|+
literal|" region "
operator|+
name|this
operator|.
name|region
argument_list|)
expr_stmt|;
name|reader
operator|=
name|HFile
operator|.
name|createReader
argument_list|(
name|srcPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|srcPath
argument_list|,
name|cacheConf
argument_list|)
expr_stmt|;
name|reader
operator|.
name|loadFileInfo
argument_list|()
expr_stmt|;
name|byte
index|[]
name|firstKey
init|=
name|reader
operator|.
name|getFirstRowKey
argument_list|()
decl_stmt|;
name|Preconditions
operator|.
name|checkState
argument_list|(
name|firstKey
operator|!=
literal|null
argument_list|,
literal|"First key can not be null"
argument_list|)
expr_stmt|;
name|byte
index|[]
name|lk
init|=
name|reader
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|Preconditions
operator|.
name|checkState
argument_list|(
name|lk
operator|!=
literal|null
argument_list|,
literal|"Last key can not be null"
argument_list|)
expr_stmt|;
name|byte
index|[]
name|lastKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|)
operator|.
name|getRow
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"HFile bounds: first="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|firstKey
argument_list|)
operator|+
literal|" last="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|lastKey
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Region bounds: first="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|region
operator|.
name|getStartKey
argument_list|()
argument_list|)
operator|+
literal|" last="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|region
operator|.
name|getEndKey
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|HRegionInfo
name|hri
init|=
name|region
operator|.
name|getRegionInfo
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|hri
operator|.
name|containsRange
argument_list|(
name|firstKey
argument_list|,
name|lastKey
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|WrongRegionException
argument_list|(
literal|"Bulk load file "
operator|+
name|srcPath
operator|.
name|toString
argument_list|()
operator|+
literal|" does not fit inside region "
operator|+
name|this
operator|.
name|region
argument_list|)
throw|;
block|}
if|if
condition|(
name|verifyBulkLoads
condition|)
block|{
name|KeyValue
name|prevKV
init|=
literal|null
decl_stmt|;
name|HFileScanner
name|scanner
init|=
name|reader
operator|.
name|getScanner
argument_list|(
literal|false
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|scanner
operator|.
name|seekTo
argument_list|()
expr_stmt|;
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|prevKV
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|Bytes
operator|.
name|compareTo
argument_list|(
name|prevKV
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|prevKV
operator|.
name|getRowOffset
argument_list|()
argument_list|,
name|prevKV
operator|.
name|getRowLength
argument_list|()
argument_list|,
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getRowOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getRowLength
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
throw|throw
operator|new
name|InvalidHFileException
argument_list|(
literal|"Previous row is greater than"
operator|+
literal|" current row: path="
operator|+
name|srcPath
operator|+
literal|" previous="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|prevKV
operator|.
name|getKey
argument_list|()
argument_list|)
operator|+
literal|" current="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|kv
operator|.
name|getKey
argument_list|()
argument_list|)
argument_list|)
throw|;
block|}
if|if
condition|(
name|Bytes
operator|.
name|compareTo
argument_list|(
name|prevKV
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|prevKV
operator|.
name|getFamilyOffset
argument_list|()
argument_list|,
name|prevKV
operator|.
name|getFamilyLength
argument_list|()
argument_list|,
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getFamilyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getFamilyLength
argument_list|()
argument_list|)
operator|!=
literal|0
condition|)
block|{
throw|throw
operator|new
name|InvalidHFileException
argument_list|(
literal|"Previous key had different"
operator|+
literal|" family compared to current key: path="
operator|+
name|srcPath
operator|+
literal|" previous="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|prevKV
operator|.
name|getFamily
argument_list|()
argument_list|)
operator|+
literal|" current="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|kv
operator|.
name|getFamily
argument_list|()
argument_list|)
argument_list|)
throw|;
block|}
block|}
name|prevKV
operator|=
name|kv
expr_stmt|;
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * This method should only be called from HRegion.  It is assumed that the    * ranges of values in the HFile fit within the stores assigned region.    * (assertBulkLoadHFileOk checks this)    */
name|void
name|bulkLoadHFile
parameter_list|(
name|String
name|srcPathStr
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|srcPath
init|=
operator|new
name|Path
argument_list|(
name|srcPathStr
argument_list|)
decl_stmt|;
comment|// Move the file if it's on another filesystem
name|FileSystem
name|srcFs
init|=
name|srcPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|srcFs
operator|.
name|equals
argument_list|(
name|fs
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"File "
operator|+
name|srcPath
operator|+
literal|" on different filesystem than "
operator|+
literal|"destination store - moving to this filesystem."
argument_list|)
expr_stmt|;
name|Path
name|tmpPath
init|=
name|getTmpPath
argument_list|()
decl_stmt|;
name|FileUtil
operator|.
name|copy
argument_list|(
name|srcFs
argument_list|,
name|srcPath
argument_list|,
name|fs
argument_list|,
name|tmpPath
argument_list|,
literal|false
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Copied to temporary path on dst filesystem: "
operator|+
name|tmpPath
argument_list|)
expr_stmt|;
name|srcPath
operator|=
name|tmpPath
expr_stmt|;
block|}
name|Path
name|dstPath
init|=
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|homedir
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Renaming bulk load file "
operator|+
name|srcPath
operator|+
literal|" to "
operator|+
name|dstPath
argument_list|)
expr_stmt|;
name|StoreFile
operator|.
name|rename
argument_list|(
name|fs
argument_list|,
name|srcPath
argument_list|,
name|dstPath
argument_list|)
expr_stmt|;
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|dstPath
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|dataBlockEncoder
argument_list|)
decl_stmt|;
name|passSchemaMetricsTo
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|sf
operator|.
name|createReader
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Moved hfile "
operator|+
name|srcPath
operator|+
literal|" into store directory "
operator|+
name|homedir
operator|+
literal|" - updating store file list."
argument_list|)
expr_stmt|;
comment|// Append the new storefile into the list
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newFiles
operator|.
name|add
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|newFiles
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// We need the lock, as long as we are updating the storefiles
comment|// or changing the memstore. Let us release it before calling
comment|// notifyChangeReadersObservers. See HBASE-4485 for a possible
comment|// deadlock scenario that could have happened if continue to hold
comment|// the lock.
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Successfully loaded store file "
operator|+
name|srcPath
operator|+
literal|" into store "
operator|+
name|this
operator|+
literal|" (new location: "
operator|+
name|dstPath
operator|+
literal|")"
argument_list|)
expr_stmt|;
block|}
comment|/**    * Get a temporary path in this region. These temporary files    * will get cleaned up when the region is re-opened if they are    * still around.    */
specifier|private
name|Path
name|getTmpPath
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|region
operator|.
name|getTmpDir
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Close all the readers    *    * We don't need to worry about subsequent requests because the HRegion holds    * a write lock that will prevent any more reads or writes.    *    * @throws IOException    */
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|result
init|=
name|storefiles
decl_stmt|;
comment|// Clear so metrics doesn't find them.
name|storefiles
operator|=
name|ImmutableList
operator|.
name|of
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|result
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// initialize the thread pool for closing store files in parallel.
name|ThreadPoolExecutor
name|storeFileCloserThreadPool
init|=
name|this
operator|.
name|region
operator|.
name|getStoreFileOpenAndCloseThreadPool
argument_list|(
literal|"StoreFileCloserThread-"
operator|+
name|this
operator|.
name|family
operator|.
name|getNameAsString
argument_list|()
argument_list|)
decl_stmt|;
comment|// close each store file in parallel
name|CompletionService
argument_list|<
name|Void
argument_list|>
name|completionService
init|=
operator|new
name|ExecutorCompletionService
argument_list|<
name|Void
argument_list|>
argument_list|(
name|storeFileCloserThreadPool
argument_list|)
decl_stmt|;
for|for
control|(
specifier|final
name|StoreFile
name|f
range|:
name|result
control|)
block|{
name|completionService
operator|.
name|submit
argument_list|(
operator|new
name|Callable
argument_list|<
name|Void
argument_list|>
argument_list|()
block|{
specifier|public
name|Void
name|call
parameter_list|()
throws|throws
name|IOException
block|{
name|f
operator|.
name|closeReader
argument_list|(
literal|true
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
block|}
argument_list|)
expr_stmt|;
block|}
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|result
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|Future
argument_list|<
name|Void
argument_list|>
name|future
init|=
name|completionService
operator|.
name|take
argument_list|()
decl_stmt|;
name|future
operator|.
name|get
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|ExecutionException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
operator|.
name|getCause
argument_list|()
argument_list|)
throw|;
block|}
finally|finally
block|{
name|storeFileCloserThreadPool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Closed "
operator|+
name|this
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Snapshot this stores memstore.  Call before running    * {@link #flushCache(long, SortedSet<KeyValue>)} so it has some work to do.    */
name|void
name|snapshot
parameter_list|()
block|{
name|this
operator|.
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
block|}
comment|/**    * Write out current snapshot.  Presumes {@link #snapshot()} has been called    * previously.    * @param logCacheFlushId flush sequence number    * @param snapshot    * @param snapshotTimeRangeTracker    * @param flushedSize The number of bytes flushed    * @param status    * @return Path The path name of the tmp file to which the store was flushed    * @throws IOException    */
specifier|private
name|Path
name|flushCache
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
parameter_list|,
name|TimeRangeTracker
name|snapshotTimeRangeTracker
parameter_list|,
name|AtomicLong
name|flushedSize
parameter_list|,
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If an exception happens flushing, we let it out without clearing
comment|// the memstore snapshot.  The old snapshot will be returned when we say
comment|// 'snapshot', the next time flush comes around.
return|return
name|internalFlushCache
argument_list|(
name|snapshot
argument_list|,
name|logCacheFlushId
argument_list|,
name|snapshotTimeRangeTracker
argument_list|,
name|flushedSize
argument_list|,
name|status
argument_list|)
return|;
block|}
comment|/*    * @param cache    * @param logCacheFlushId    * @param snapshotTimeRangeTracker    * @param flushedSize The number of bytes flushed    * @return Path The path name of the tmp file to which the store was flushed    * @throws IOException    */
specifier|private
name|Path
name|internalFlushCache
parameter_list|(
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|,
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
name|TimeRangeTracker
name|snapshotTimeRangeTracker
parameter_list|,
name|AtomicLong
name|flushedSize
parameter_list|,
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
name|StoreFile
operator|.
name|Writer
name|writer
decl_stmt|;
comment|// Find the smallest read point across all the Scanners.
name|long
name|smallestReadPoint
init|=
name|region
operator|.
name|getSmallestReadPoint
argument_list|()
decl_stmt|;
name|long
name|flushed
init|=
literal|0
decl_stmt|;
name|Path
name|pathName
decl_stmt|;
comment|// Don't flush if there are no entries.
if|if
condition|(
name|set
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Scan
name|scan
init|=
operator|new
name|Scan
argument_list|()
decl_stmt|;
name|scan
operator|.
name|setMaxVersions
argument_list|(
name|scanInfo
operator|.
name|getMaxVersions
argument_list|()
argument_list|)
expr_stmt|;
comment|// Use a store scanner to find which rows to flush.
comment|// Note that we need to retain deletes, hence
comment|// treat this as a minor compaction.
name|InternalScanner
name|scanner
init|=
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|Collections
operator|.
name|singletonList
argument_list|(
operator|new
name|CollectionBackedScanner
argument_list|(
name|set
argument_list|,
name|this
operator|.
name|comparator
argument_list|)
argument_list|)
argument_list|,
name|ScanType
operator|.
name|MINOR_COMPACT
argument_list|,
name|this
operator|.
name|region
operator|.
name|getSmallestReadPoint
argument_list|()
argument_list|,
name|HConstants
operator|.
name|OLDEST_TIMESTAMP
argument_list|)
decl_stmt|;
try|try
block|{
comment|// TODO:  We can fail in the below block before we complete adding this
comment|// flush to list of store files.  Add cleanup of anything put on filesystem
comment|// if we fail.
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": creating writer"
argument_list|)
expr_stmt|;
comment|// A. Write the map out to the disk
name|writer
operator|=
name|createWriterInTmp
argument_list|(
name|set
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|writer
operator|.
name|setTimeRangeTracker
argument_list|(
name|snapshotTimeRangeTracker
argument_list|)
expr_stmt|;
name|pathName
operator|=
name|writer
operator|.
name|getPath
argument_list|()
expr_stmt|;
try|try
block|{
name|List
argument_list|<
name|KeyValue
argument_list|>
name|kvs
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|hasMore
decl_stmt|;
do|do
block|{
name|hasMore
operator|=
name|scanner
operator|.
name|next
argument_list|(
name|kvs
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|kvs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
for|for
control|(
name|KeyValue
name|kv
range|:
name|kvs
control|)
block|{
comment|// If we know that this KV is going to be included always, then let us
comment|// set its memstoreTS to 0. This will help us save space when writing to disk.
if|if
condition|(
name|kv
operator|.
name|getMemstoreTS
argument_list|()
operator|<=
name|smallestReadPoint
condition|)
block|{
comment|// let us not change the original KV. It could be in the memstore
comment|// changing its memstoreTS could affect other threads/scanners.
name|kv
operator|=
name|kv
operator|.
name|shallowCopy
argument_list|()
expr_stmt|;
name|kv
operator|.
name|setMemstoreTS
argument_list|(
literal|0
argument_list|)
expr_stmt|;
block|}
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
name|flushed
operator|+=
name|this
operator|.
name|memstore
operator|.
name|heapSizeChange
argument_list|(
name|kv
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|kvs
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
do|while
condition|(
name|hasMore
condition|)
do|;
block|}
finally|finally
block|{
comment|// Write out the log sequence number that corresponds to this output
comment|// hfile.  The hfile is current up to and including logCacheFlushId.
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": appending metadata"
argument_list|)
expr_stmt|;
name|writer
operator|.
name|appendMetadata
argument_list|(
name|logCacheFlushId
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": closing flushed file"
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|flushedSize
operator|.
name|set
argument_list|(
name|flushed
argument_list|)
expr_stmt|;
name|scanner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Flushed "
operator|+
literal|", sequenceid="
operator|+
name|logCacheFlushId
operator|+
literal|", memsize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|flushed
argument_list|)
operator|+
literal|", into tmp file "
operator|+
name|pathName
argument_list|)
expr_stmt|;
block|}
return|return
name|pathName
return|;
block|}
comment|/*    * @param path The pathname of the tmp file into which the store was flushed    * @param logCacheFlushId    * @return StoreFile created.    * @throws IOException    */
specifier|private
name|StoreFile
name|commitFile
parameter_list|(
specifier|final
name|Path
name|path
parameter_list|,
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
name|TimeRangeTracker
name|snapshotTimeRangeTracker
parameter_list|,
name|AtomicLong
name|flushedSize
parameter_list|,
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Write-out finished successfully, move into the right spot
name|String
name|fileName
init|=
name|path
operator|.
name|getName
argument_list|()
decl_stmt|;
name|Path
name|dstPath
init|=
operator|new
name|Path
argument_list|(
name|homedir
argument_list|,
name|fileName
argument_list|)
decl_stmt|;
name|validateStoreFile
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|String
name|msg
init|=
literal|"Renaming flushed file at "
operator|+
name|path
operator|+
literal|" to "
operator|+
name|dstPath
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|msg
argument_list|)
expr_stmt|;
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": "
operator|+
name|msg
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|path
argument_list|,
name|dstPath
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to rename "
operator|+
name|path
operator|+
literal|" to "
operator|+
name|dstPath
argument_list|)
expr_stmt|;
block|}
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": reopening flushed file"
argument_list|)
expr_stmt|;
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|dstPath
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|dataBlockEncoder
argument_list|)
decl_stmt|;
name|passSchemaMetricsTo
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|sf
operator|.
name|createReader
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|+=
name|r
operator|.
name|getTotalUncompressedBytes
argument_list|()
expr_stmt|;
comment|// This increments the metrics associated with total flushed bytes for this
comment|// family. The overall flush count is stored in the static metrics and
comment|// retrieved from HRegion.recentFlushes, which is set within
comment|// HRegion.internalFlushcache, which indirectly calls this to actually do
comment|// the flushing through the StoreFlusherImpl class
name|getSchemaMetrics
argument_list|()
operator|.
name|updatePersistentStoreMetric
argument_list|(
name|SchemaMetrics
operator|.
name|StoreMetricType
operator|.
name|FLUSH_SIZE
argument_list|,
name|flushedSize
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Added "
operator|+
name|sf
operator|+
literal|", entries="
operator|+
name|r
operator|.
name|getEntries
argument_list|()
operator|+
literal|", sequenceid="
operator|+
name|logCacheFlushId
operator|+
literal|", filesize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|r
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|sf
return|;
block|}
comment|/*    * @param maxKeyCount    * @return Writer for a new StoreFile in the tmp dir.    */
specifier|private
name|StoreFile
operator|.
name|Writer
name|createWriterInTmp
parameter_list|(
name|int
name|maxKeyCount
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|createWriterInTmp
argument_list|(
name|maxKeyCount
argument_list|,
name|this
operator|.
name|family
operator|.
name|getCompression
argument_list|()
argument_list|,
literal|false
argument_list|)
return|;
block|}
comment|/*    * @param maxKeyCount    * @param compression Compression algorithm to use    * @param isCompaction whether we are creating a new file in a compaction    * @return Writer for a new StoreFile in the tmp dir.    */
name|StoreFile
operator|.
name|Writer
name|createWriterInTmp
parameter_list|(
name|int
name|maxKeyCount
parameter_list|,
name|Compression
operator|.
name|Algorithm
name|compression
parameter_list|,
name|boolean
name|isCompaction
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|CacheConfig
name|writerCacheConf
decl_stmt|;
if|if
condition|(
name|isCompaction
condition|)
block|{
comment|// Don't cache data on write on compactions.
name|writerCacheConf
operator|=
operator|new
name|CacheConfig
argument_list|(
name|cacheConf
argument_list|)
expr_stmt|;
name|writerCacheConf
operator|.
name|setCacheDataOnWrite
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|writerCacheConf
operator|=
name|cacheConf
expr_stmt|;
block|}
name|StoreFile
operator|.
name|Writer
name|w
init|=
operator|new
name|StoreFile
operator|.
name|WriterBuilder
argument_list|(
name|conf
argument_list|,
name|writerCacheConf
argument_list|,
name|fs
argument_list|,
name|blocksize
argument_list|)
operator|.
name|withOutputDir
argument_list|(
name|region
operator|.
name|getTmpDir
argument_list|()
argument_list|)
operator|.
name|withDataBlockEncoder
argument_list|(
name|dataBlockEncoder
argument_list|)
operator|.
name|withComparator
argument_list|(
name|comparator
argument_list|)
operator|.
name|withBloomType
argument_list|(
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|)
operator|.
name|withMaxKeyCount
argument_list|(
name|maxKeyCount
argument_list|)
operator|.
name|withChecksumType
argument_list|(
name|checksumType
argument_list|)
operator|.
name|withBytesPerChecksum
argument_list|(
name|bytesPerChecksum
argument_list|)
operator|.
name|withCompression
argument_list|(
name|compression
argument_list|)
operator|.
name|build
argument_list|()
decl_stmt|;
comment|// The store file writer's path does not include the CF name, so we need
comment|// to configure the HFile writer directly.
name|SchemaConfigured
name|sc
init|=
operator|(
name|SchemaConfigured
operator|)
name|w
operator|.
name|writer
decl_stmt|;
name|SchemaConfigured
operator|.
name|resetSchemaMetricsConf
argument_list|(
name|sc
argument_list|)
expr_stmt|;
name|passSchemaMetricsTo
argument_list|(
name|sc
argument_list|)
expr_stmt|;
return|return
name|w
return|;
block|}
comment|/*    * Change storefiles adding into place the Reader produced by this new flush.    * @param sf    * @param set That was used to make the passed file<code>p</code>.    * @throws IOException    * @return Whether compaction is required.    */
specifier|private
name|boolean
name|updateStorefiles
parameter_list|(
specifier|final
name|StoreFile
name|sf
parameter_list|,
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|newList
argument_list|)
expr_stmt|;
name|this
operator|.
name|memstore
operator|.
name|clearSnapshot
argument_list|(
name|set
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// We need the lock, as long as we are updating the storefiles
comment|// or changing the memstore. Let us release it before calling
comment|// notifyChangeReadersObservers. See HBASE-4485 for a possible
comment|// deadlock scenario that could have happened if continue to hold
comment|// the lock.
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
comment|// Tell listeners of the change in readers.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
return|return
name|needsCompaction
argument_list|()
return|;
block|}
comment|/*    * Notify all observers that set of Readers has changed.    * @throws IOException    */
specifier|private
name|void
name|notifyChangedReadersObservers
parameter_list|()
throws|throws
name|IOException
block|{
for|for
control|(
name|ChangedReadersObserver
name|o
range|:
name|this
operator|.
name|changedReaderObservers
control|)
block|{
name|o
operator|.
name|updateReaders
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Get all scanners with no filtering based on TTL (that happens further down    * the line).    * @return all scanners for this store    */
specifier|protected
name|List
argument_list|<
name|KeyValueScanner
argument_list|>
name|getScanners
parameter_list|(
name|boolean
name|cacheBlocks
parameter_list|,
name|boolean
name|isGet
parameter_list|,
name|boolean
name|isCompaction
parameter_list|,
name|ScanQueryMatcher
name|matcher
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|StoreFile
argument_list|>
name|storeFiles
decl_stmt|;
name|List
argument_list|<
name|KeyValueScanner
argument_list|>
name|memStoreScanners
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|storeFiles
operator|=
name|this
operator|.
name|getStorefiles
argument_list|()
expr_stmt|;
name|memStoreScanners
operator|=
name|this
operator|.
name|memstore
operator|.
name|getScanners
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
comment|// First the store file scanners
comment|// TODO this used to get the store files in descending order,
comment|// but now we get them in ascending order, which I think is
comment|// actually more correct, since memstore get put at the end.
name|List
argument_list|<
name|StoreFileScanner
argument_list|>
name|sfScanners
init|=
name|StoreFileScanner
operator|.
name|getScannersForStoreFiles
argument_list|(
name|storeFiles
argument_list|,
name|cacheBlocks
argument_list|,
name|isGet
argument_list|,
name|isCompaction
argument_list|,
name|matcher
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|KeyValueScanner
argument_list|>
name|scanners
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValueScanner
argument_list|>
argument_list|(
name|sfScanners
operator|.
name|size
argument_list|()
operator|+
literal|1
argument_list|)
decl_stmt|;
name|scanners
operator|.
name|addAll
argument_list|(
name|sfScanners
argument_list|)
expr_stmt|;
comment|// Then the memstore scanners
name|scanners
operator|.
name|addAll
argument_list|(
name|memStoreScanners
argument_list|)
expr_stmt|;
return|return
name|scanners
return|;
block|}
comment|/*    * @param o Observer who wants to know about changes in set of Readers    */
name|void
name|addChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
name|this
operator|.
name|changedReaderObservers
operator|.
name|add
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|/*    * @param o Observer no longer interested in changes in set of Readers.    */
name|void
name|deleteChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
comment|// We don't check if observer present; it may not be (legitimately)
name|this
operator|.
name|changedReaderObservers
operator|.
name|remove
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the StoreFiles.  This method may take some time, so the calling    * thread must be able to block for long periods.    *    *<p>During this time, the Store can work as usual, getting values from    * StoreFiles and writing new StoreFiles from the memstore.    *    * Existing StoreFiles are not destroyed until the new compacted StoreFile is    * completely written-out to disk.    *    *<p>The compactLock prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *    *<p>We don't want to hold the structureLock for the whole time, as a compact()    * can be lengthy and we want to allow cache-flushes during this period.    *    * @param cr    *          compaction details obtained from requestCompaction()    * @throws IOException    * @return Storefile we compacted into or null if we failed or opted out early.    */
name|StoreFile
name|compact
parameter_list|(
name|CompactionRequest
name|cr
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|cr
operator|==
literal|null
operator|||
name|cr
operator|.
name|getFiles
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
return|return
literal|null
return|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|cr
operator|.
name|getStore
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|equals
argument_list|(
name|this
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
name|cr
operator|.
name|getFiles
argument_list|()
decl_stmt|;
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
comment|// sanity check: we're compacting files that this store knows about
comment|// TODO: change this to LOG.error() after more debugging
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|filesCompacting
operator|.
name|containsAll
argument_list|(
name|filesToCompact
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// Max-sequenceID is the last key in the files we're compacting
name|long
name|maxId
init|=
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|filesToCompact
argument_list|)
decl_stmt|;
comment|// Ready to go. Have list of files to compact.
name|LOG
operator|.
name|info
argument_list|(
literal|"Starting compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in "
operator|+
name|this
operator|+
literal|" of "
operator|+
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|getRegionNameAsString
argument_list|()
operator|+
literal|" into tmpdir="
operator|+
name|region
operator|.
name|getTmpDir
argument_list|()
operator|+
literal|", seqid="
operator|+
name|maxId
operator|+
literal|", totalSize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|cr
operator|.
name|getSize
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|StoreFile
name|sf
init|=
literal|null
decl_stmt|;
try|try
block|{
name|StoreFile
operator|.
name|Writer
name|writer
init|=
name|this
operator|.
name|compactor
operator|.
name|compact
argument_list|(
name|this
argument_list|,
name|filesToCompact
argument_list|,
name|cr
operator|.
name|isMajor
argument_list|()
argument_list|,
name|maxId
argument_list|)
decl_stmt|;
comment|// Move the compaction into place.
if|if
condition|(
name|this
operator|.
name|conf
operator|.
name|getBoolean
argument_list|(
literal|"hbase.hstore.compaction.complete"
argument_list|,
literal|true
argument_list|)
condition|)
block|{
name|sf
operator|=
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|writer
argument_list|)
expr_stmt|;
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|postCompact
argument_list|(
name|this
argument_list|,
name|sf
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// Create storefile around what we wrote with a reader on it.
name|sf
operator|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|writer
operator|.
name|getPath
argument_list|()
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|dataBlockEncoder
argument_list|)
expr_stmt|;
name|sf
operator|.
name|createReader
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Completed"
operator|+
operator|(
name|cr
operator|.
name|isMajor
argument_list|()
condition|?
literal|" major "
else|:
literal|" "
operator|)
operator|+
literal|"compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in "
operator|+
name|this
operator|+
literal|" of "
operator|+
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|getRegionNameAsString
argument_list|()
operator|+
literal|" into "
operator|+
operator|(
name|sf
operator|==
literal|null
condition|?
literal|"none"
else|:
name|sf
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|)
operator|+
literal|", size="
operator|+
operator|(
name|sf
operator|==
literal|null
condition|?
literal|"none"
else|:
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
argument_list|)
operator|)
operator|+
literal|"; total size for store is "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|storeSize
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|sf
return|;
block|}
comment|/**    * Compact the most recent N files. Used in testing.    */
specifier|public
name|void
name|compactRecentForTesting
parameter_list|(
name|int
name|N
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
decl_stmt|;
name|long
name|maxId
decl_stmt|;
name|boolean
name|isMajor
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesToCompact
operator|=
name|Lists
operator|.
name|newArrayList
argument_list|(
name|storefiles
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|filesCompacting
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// exclude all files older than the newest file we're currently
comment|// compacting. this allows us to preserve contiguity (HBASE-2856)
name|StoreFile
name|last
init|=
name|filesCompacting
operator|.
name|get
argument_list|(
name|filesCompacting
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|idx
init|=
name|filesToCompact
operator|.
name|indexOf
argument_list|(
name|last
argument_list|)
decl_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|idx
operator|!=
operator|-
literal|1
argument_list|)
expr_stmt|;
name|filesToCompact
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|idx
operator|+
literal|1
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
name|int
name|count
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
decl_stmt|;
if|if
condition|(
name|N
operator|>
name|count
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Not enough files"
argument_list|)
throw|;
block|}
name|filesToCompact
operator|=
name|filesToCompact
operator|.
name|subList
argument_list|(
name|count
operator|-
name|N
argument_list|,
name|count
argument_list|)
expr_stmt|;
name|maxId
operator|=
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
name|isMajor
operator|=
operator|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
name|storefiles
operator|.
name|size
argument_list|()
operator|)
expr_stmt|;
name|filesCompacting
operator|.
name|addAll
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
name|Collections
operator|.
name|sort
argument_list|(
name|filesCompacting
argument_list|,
name|StoreFile
operator|.
name|Comparators
operator|.
name|FLUSH_TIME
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
try|try
block|{
comment|// Ready to go. Have list of files to compact.
name|StoreFile
operator|.
name|Writer
name|writer
init|=
name|this
operator|.
name|compactor
operator|.
name|compact
argument_list|(
name|this
argument_list|,
name|filesToCompact
argument_list|,
name|isMajor
argument_list|,
name|maxId
argument_list|)
decl_stmt|;
comment|// Move the compaction into place.
name|StoreFile
name|sf
init|=
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|writer
argument_list|)
decl_stmt|;
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|postCompact
argument_list|(
name|this
argument_list|,
name|sf
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|boolean
name|hasReferences
parameter_list|()
block|{
return|return
name|hasReferences
argument_list|(
name|this
operator|.
name|storefiles
argument_list|)
return|;
block|}
comment|/*    * @param files    * @return True if any of the files in<code>files</code> are References.    */
specifier|private
name|boolean
name|hasReferences
parameter_list|(
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|StoreFile
name|hsf
range|:
name|files
control|)
block|{
if|if
condition|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/*    * Gets lowest timestamp from candidate StoreFiles    *    * @param fs    * @param dir    * @throws IOException    */
specifier|public
specifier|static
name|long
name|getLowestTimestamp
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|minTs
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|;
for|for
control|(
name|StoreFile
name|storeFile
range|:
name|candidates
control|)
block|{
name|minTs
operator|=
name|Math
operator|.
name|min
argument_list|(
name|minTs
argument_list|,
name|storeFile
operator|.
name|getModificationTimeStamp
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|minTs
return|;
block|}
comment|/** getter for CompactionProgress object    * @return CompactionProgress object; can be null    */
specifier|public
name|CompactionProgress
name|getCompactionProgress
parameter_list|()
block|{
return|return
name|this
operator|.
name|compactor
operator|.
name|getProgress
argument_list|()
return|;
block|}
comment|/*    * @return True if we should run a major compaction.    */
name|boolean
name|isMajorCompaction
parameter_list|()
throws|throws
name|IOException
block|{
for|for
control|(
name|StoreFile
name|sf
range|:
name|this
operator|.
name|storefiles
control|)
block|{
if|if
condition|(
name|sf
operator|.
name|getReader
argument_list|()
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"StoreFile "
operator|+
name|sf
operator|+
literal|" has null Reader"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
block|}
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
argument_list|)
decl_stmt|;
comment|// exclude files above the max compaction threshold
comment|// except: save all references. we MUST compact them
name|int
name|pos
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|<
name|candidates
operator|.
name|size
argument_list|()
operator|&&
name|candidates
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
operator|>
name|this
operator|.
name|maxCompactSize
operator|&&
operator|!
name|candidates
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|isReference
argument_list|()
condition|)
operator|++
name|pos
expr_stmt|;
name|candidates
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|pos
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
return|return
name|isMajorCompaction
argument_list|(
name|candidates
argument_list|)
return|;
block|}
comment|/*    * @param filesToCompact Files to compact. Can be null.    * @return True if we should run a major compaction.    */
specifier|private
name|boolean
name|isMajorCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
name|long
name|mcTime
init|=
name|getNextMajorCompactTime
argument_list|()
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|==
literal|null
operator|||
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
operator|||
name|mcTime
operator|==
literal|0
condition|)
block|{
return|return
name|result
return|;
block|}
comment|// TODO: Use better method for determining stamp of last major (HBASE-2990)
name|long
name|lowTimestamp
init|=
name|getLowestTimestamp
argument_list|(
name|filesToCompact
argument_list|)
decl_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
if|if
condition|(
name|lowTimestamp
operator|>
literal|0l
operator|&&
name|lowTimestamp
operator|<
operator|(
name|now
operator|-
name|mcTime
operator|)
condition|)
block|{
comment|// Major compaction time has elapsed.
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
comment|// Single file
name|StoreFile
name|sf
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|long
name|oldest
init|=
operator|(
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|timeRangeTracker
operator|==
literal|null
operator|)
condition|?
name|Long
operator|.
name|MIN_VALUE
else|:
name|now
operator|-
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|timeRangeTracker
operator|.
name|minimumTimestamp
decl_stmt|;
if|if
condition|(
name|sf
operator|.
name|isMajorCompaction
argument_list|()
operator|&&
operator|(
name|this
operator|.
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|oldest
operator|<
name|this
operator|.
name|ttl
operator|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping major compaction of "
operator|+
name|this
operator|+
literal|" because one (major) compacted file only and oldestTime "
operator|+
name|oldest
operator|+
literal|"ms is< ttl="
operator|+
name|this
operator|.
name|ttl
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|this
operator|.
name|ttl
operator|!=
name|HConstants
operator|.
name|FOREVER
operator|&&
name|oldest
operator|>
name|this
operator|.
name|ttl
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Major compaction triggered on store "
operator|+
name|this
operator|+
literal|", because keyvalues outdated; time since last major compaction "
operator|+
operator|(
name|now
operator|-
name|lowTimestamp
operator|)
operator|+
literal|"ms"
argument_list|)
expr_stmt|;
name|result
operator|=
literal|true
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Major compaction triggered on store "
operator|+
name|this
operator|+
literal|"; time since last major compaction "
operator|+
operator|(
name|now
operator|-
name|lowTimestamp
operator|)
operator|+
literal|"ms"
argument_list|)
expr_stmt|;
block|}
name|result
operator|=
literal|true
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
name|long
name|getNextMajorCompactTime
parameter_list|()
block|{
comment|// default = 24hrs
name|long
name|ret
init|=
name|conf
operator|.
name|getLong
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|,
literal|1000
operator|*
literal|60
operator|*
literal|60
operator|*
literal|24
argument_list|)
decl_stmt|;
if|if
condition|(
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|String
name|strCompactionTime
init|=
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
decl_stmt|;
name|ret
operator|=
operator|(
operator|new
name|Long
argument_list|(
name|strCompactionTime
argument_list|)
operator|)
operator|.
name|longValue
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|ret
operator|>
literal|0
condition|)
block|{
comment|// default = 20% = +/- 4.8 hrs
name|double
name|jitterPct
init|=
name|conf
operator|.
name|getFloat
argument_list|(
literal|"hbase.hregion.majorcompaction.jitter"
argument_list|,
literal|0.20F
argument_list|)
decl_stmt|;
if|if
condition|(
name|jitterPct
operator|>
literal|0
condition|)
block|{
name|long
name|jitter
init|=
name|Math
operator|.
name|round
argument_list|(
name|ret
operator|*
name|jitterPct
argument_list|)
decl_stmt|;
comment|// deterministic jitter avoids a major compaction storm on restart
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|snapshot
init|=
name|storefiles
decl_stmt|;
if|if
condition|(
name|snapshot
operator|!=
literal|null
operator|&&
operator|!
name|snapshot
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|String
name|seed
init|=
name|snapshot
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
name|double
name|curRand
init|=
operator|new
name|Random
argument_list|(
name|seed
operator|.
name|hashCode
argument_list|()
argument_list|)
operator|.
name|nextDouble
argument_list|()
decl_stmt|;
name|ret
operator|+=
name|jitter
operator|-
name|Math
operator|.
name|round
argument_list|(
literal|2L
operator|*
name|jitter
operator|*
name|curRand
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|ret
operator|=
literal|0
expr_stmt|;
comment|// no storefiles == no major compaction
block|}
block|}
block|}
return|return
name|ret
return|;
block|}
specifier|public
name|CompactionRequest
name|requestCompaction
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|requestCompaction
argument_list|(
name|NO_PRIORITY
argument_list|)
return|;
block|}
specifier|public
name|CompactionRequest
name|requestCompaction
parameter_list|(
name|int
name|priority
parameter_list|)
throws|throws
name|IOException
block|{
comment|// don't even select for compaction if writes are disabled
if|if
condition|(
operator|!
name|this
operator|.
name|region
operator|.
name|areWritesEnabled
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
name|CompactionRequest
name|ret
init|=
literal|null
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
comment|// candidates = all storefiles not already in compaction queue
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
init|=
name|Lists
operator|.
name|newArrayList
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|filesCompacting
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// exclude all files older than the newest file we're currently
comment|// compacting. this allows us to preserve contiguity (HBASE-2856)
name|StoreFile
name|last
init|=
name|filesCompacting
operator|.
name|get
argument_list|(
name|filesCompacting
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|idx
init|=
name|candidates
operator|.
name|indexOf
argument_list|(
name|last
argument_list|)
decl_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|idx
operator|!=
operator|-
literal|1
argument_list|)
expr_stmt|;
name|candidates
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|idx
operator|+
literal|1
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
name|boolean
name|override
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|override
operator|=
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|preCompactSelection
argument_list|(
name|this
argument_list|,
name|candidates
argument_list|)
expr_stmt|;
block|}
name|CompactSelection
name|filesToCompact
decl_stmt|;
if|if
condition|(
name|override
condition|)
block|{
comment|// coprocessor is overriding normal file selection
name|filesToCompact
operator|=
operator|new
name|CompactSelection
argument_list|(
name|conf
argument_list|,
name|candidates
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|filesToCompact
operator|=
name|compactSelection
argument_list|(
name|candidates
argument_list|,
name|priority
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|postCompactSelection
argument_list|(
name|this
argument_list|,
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|filesToCompact
operator|.
name|getFilesToCompact
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// no files to compact
if|if
condition|(
name|filesToCompact
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
comment|// basic sanity check: do not try to compact the same StoreFile twice.
if|if
condition|(
operator|!
name|Collections
operator|.
name|disjoint
argument_list|(
name|filesCompacting
argument_list|,
name|filesToCompact
operator|.
name|getFilesToCompact
argument_list|()
argument_list|)
condition|)
block|{
comment|// TODO: change this from an IAE to LOG.error after sufficient testing
name|Preconditions
operator|.
name|checkArgument
argument_list|(
literal|false
argument_list|,
literal|"%s overlaps with %s"
argument_list|,
name|filesToCompact
argument_list|,
name|filesCompacting
argument_list|)
expr_stmt|;
block|}
name|filesCompacting
operator|.
name|addAll
argument_list|(
name|filesToCompact
operator|.
name|getFilesToCompact
argument_list|()
argument_list|)
expr_stmt|;
name|Collections
operator|.
name|sort
argument_list|(
name|filesCompacting
argument_list|,
name|StoreFile
operator|.
name|Comparators
operator|.
name|FLUSH_TIME
argument_list|)
expr_stmt|;
comment|// major compaction iff all StoreFiles are included
name|boolean
name|isMajor
init|=
operator|(
name|filesToCompact
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|==
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|)
decl_stmt|;
if|if
condition|(
name|isMajor
condition|)
block|{
comment|// since we're enqueuing a major, update the compaction wait interval
name|this
operator|.
name|forceMajor
operator|=
literal|false
expr_stmt|;
block|}
comment|// everything went better than expected. create a compaction request
name|int
name|pri
init|=
name|getCompactPriority
argument_list|(
name|priority
argument_list|)
decl_stmt|;
name|ret
operator|=
operator|new
name|CompactionRequest
argument_list|(
name|region
argument_list|,
name|this
argument_list|,
name|filesToCompact
argument_list|,
name|isMajor
argument_list|,
name|pri
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|ret
operator|!=
literal|null
condition|)
block|{
name|CompactionRequest
operator|.
name|preRequest
argument_list|(
name|ret
argument_list|)
expr_stmt|;
block|}
return|return
name|ret
return|;
block|}
specifier|public
name|void
name|finishRequest
parameter_list|(
name|CompactionRequest
name|cr
parameter_list|)
block|{
name|CompactionRequest
operator|.
name|postRequest
argument_list|(
name|cr
argument_list|)
expr_stmt|;
name|cr
operator|.
name|finishRequest
argument_list|()
expr_stmt|;
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|cr
operator|.
name|getFiles
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Algorithm to choose which files to compact, see {@link #compactSelection(java.util.List, int)}    * @param candidates    * @return    * @throws IOException    */
name|CompactSelection
name|compactSelection
parameter_list|(
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|compactSelection
argument_list|(
name|candidates
argument_list|,
name|NO_PRIORITY
argument_list|)
return|;
block|}
comment|/**    * Algorithm to choose which files to compact    *    * Configuration knobs:    *  "hbase.hstore.compaction.ratio"    *    normal case: minor compact when file<= sum(smaller_files) * ratio    *  "hbase.hstore.compaction.min.size"    *    unconditionally compact individual files below this size    *  "hbase.hstore.compaction.max.size"    *    never compact individual files above this size (unless splitting)    *  "hbase.hstore.compaction.min"    *    min files needed to minor compact    *  "hbase.hstore.compaction.max"    *    max files to compact at once (avoids OOM)    *    * @param candidates candidate files, ordered from oldest to newest    * @return subset copy of candidate list that meets compaction criteria    * @throws IOException    */
name|CompactSelection
name|compactSelection
parameter_list|(
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
parameter_list|,
name|int
name|priority
parameter_list|)
throws|throws
name|IOException
block|{
comment|// ASSUMPTION!!! filesCompacting is locked when calling this function
comment|/* normal skew:      *      *         older ----> newer      *     _      *    | |   _      *    | |  | |   _      *  --|-|- |-|- |-|---_-------_-------  minCompactSize      *    | |  | |  | |  | |  _  | |      *    | |  | |  | |  | | | | | |      *    | |  | |  | |  | | | | | |      */
name|CompactSelection
name|compactSelection
init|=
operator|new
name|CompactSelection
argument_list|(
name|conf
argument_list|,
name|candidates
argument_list|)
decl_stmt|;
name|boolean
name|forcemajor
init|=
name|this
operator|.
name|forceMajor
operator|&&
name|filesCompacting
operator|.
name|isEmpty
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|forcemajor
condition|)
block|{
comment|// Delete the expired store files before the compaction selection.
if|if
condition|(
name|conf
operator|.
name|getBoolean
argument_list|(
literal|"hbase.store.delete.expired.storefile"
argument_list|,
literal|true
argument_list|)
operator|&&
operator|(
name|ttl
operator|!=
name|Long
operator|.
name|MAX_VALUE
operator|)
operator|&&
operator|(
name|this
operator|.
name|scanInfo
operator|.
name|minVersions
operator|==
literal|0
operator|)
condition|)
block|{
name|CompactSelection
name|expiredSelection
init|=
name|compactSelection
operator|.
name|selectExpiredStoreFilesToCompact
argument_list|(
name|EnvironmentEdgeManager
operator|.
name|currentTimeMillis
argument_list|()
operator|-
name|this
operator|.
name|ttl
argument_list|)
decl_stmt|;
comment|// If there is any expired store files, delete them  by compaction.
if|if
condition|(
name|expiredSelection
operator|!=
literal|null
condition|)
block|{
return|return
name|expiredSelection
return|;
block|}
block|}
comment|// do not compact old files above a configurable threshold
comment|// save all references. we MUST compact them
name|int
name|pos
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|<
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|&&
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
operator|>
name|maxCompactSize
operator|&&
operator|!
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|isReference
argument_list|()
condition|)
operator|++
name|pos
expr_stmt|;
if|if
condition|(
name|pos
operator|!=
literal|0
condition|)
name|compactSelection
operator|.
name|clearSubList
argument_list|(
literal|0
argument_list|,
name|pos
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|this
operator|.
name|getHRegionInfo
argument_list|()
operator|.
name|getEncodedName
argument_list|()
operator|+
literal|" - "
operator|+
name|this
operator|+
literal|": no store files to compact"
argument_list|)
expr_stmt|;
name|compactSelection
operator|.
name|emptyFileList
argument_list|()
expr_stmt|;
return|return
name|compactSelection
return|;
block|}
comment|// Force a major compaction if this is a user-requested major compaction,
comment|// or if we do not have too many files to compact and this was requested
comment|// as a major compaction
name|boolean
name|majorcompaction
init|=
operator|(
name|forcemajor
operator|&&
name|priority
operator|==
name|PRIORITY_USER
operator|)
operator|||
operator|(
name|forcemajor
operator|||
name|isMajorCompaction
argument_list|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
argument_list|)
operator|)
operator|&&
operator|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|<
name|this
operator|.
name|maxFilesToCompact
operator|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|this
operator|.
name|getHRegionInfo
argument_list|()
operator|.
name|getEncodedName
argument_list|()
operator|+
literal|" - "
operator|+
name|this
operator|.
name|getColumnFamilyName
argument_list|()
operator|+
literal|": Initiating "
operator|+
operator|(
name|majorcompaction
condition|?
literal|"major"
else|:
literal|"minor"
operator|)
operator|+
literal|"compaction"
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|hasReferences
argument_list|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
argument_list|)
condition|)
block|{
comment|// we're doing a minor compaction, let's see what files are applicable
name|int
name|start
init|=
literal|0
decl_stmt|;
name|double
name|r
init|=
name|compactSelection
operator|.
name|getCompactSelectionRatio
argument_list|()
decl_stmt|;
comment|// skip selection algorithm if we don't have enough files
if|if
condition|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|<
name|this
operator|.
name|minFilesToCompact
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Not compacting files because we only have "
operator|+
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|+
literal|" files ready for compaction.  Need "
operator|+
name|this
operator|.
name|minFilesToCompact
operator|+
literal|" to initiate."
argument_list|)
expr_stmt|;
block|}
name|compactSelection
operator|.
name|emptyFileList
argument_list|()
expr_stmt|;
return|return
name|compactSelection
return|;
block|}
comment|// remove bulk import files that request to be excluded from minors
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|removeAll
argument_list|(
name|Collections2
operator|.
name|filter
argument_list|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
argument_list|,
operator|new
name|Predicate
argument_list|<
name|StoreFile
argument_list|>
argument_list|()
block|{
specifier|public
name|boolean
name|apply
parameter_list|(
name|StoreFile
name|input
parameter_list|)
block|{
return|return
name|input
operator|.
name|excludeFromMinorCompaction
argument_list|()
return|;
block|}
block|}
argument_list|)
argument_list|)
expr_stmt|;
comment|/* TODO: add sorting + unit test back in when HBASE-2856 is fixed       // Sort files by size to correct when normal skew is altered by bulk load.       Collections.sort(filesToCompact, StoreFile.Comparators.FILE_SIZE);        */
comment|// get store file sizes for incremental compacting selection.
name|int
name|countOfFiles
init|=
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
index|[]
name|fileSizes
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
name|long
index|[]
name|sumSize
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|countOfFiles
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
operator|--
name|i
control|)
block|{
name|StoreFile
name|file
init|=
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|fileSizes
index|[
name|i
index|]
operator|=
name|file
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
expr_stmt|;
comment|// calculate the sum of fileSizes[i,i+maxFilesToCompact-1) for algo
name|int
name|tooFar
init|=
name|i
operator|+
name|this
operator|.
name|maxFilesToCompact
operator|-
literal|1
decl_stmt|;
name|sumSize
index|[
name|i
index|]
operator|=
name|fileSizes
index|[
name|i
index|]
operator|+
operator|(
operator|(
name|i
operator|+
literal|1
operator|<
name|countOfFiles
operator|)
condition|?
name|sumSize
index|[
name|i
operator|+
literal|1
index|]
else|:
literal|0
operator|)
operator|-
operator|(
operator|(
name|tooFar
operator|<
name|countOfFiles
operator|)
condition|?
name|fileSizes
index|[
name|tooFar
index|]
else|:
literal|0
operator|)
expr_stmt|;
block|}
comment|/* Start at the oldest file and stop when you find the first file that        * meets compaction criteria:        *   (1) a recently-flushed, small file (i.e.<= minCompactSize)        *      OR        *   (2) within the compactRatio of sum(newer_files)        * Given normal skew, any newer files will also meet this criteria        *        * Additional Note:        * If fileSizes.size()>> maxFilesToCompact, we will recurse on        * compact().  Consider the oldest files first to avoid a        * situation where we always compact [end-threshold,end).  Then, the        * last file becomes an aggregate of the previous compactions.        */
while|while
condition|(
name|countOfFiles
operator|-
name|start
operator|>=
name|this
operator|.
name|minFilesToCompact
operator|&&
name|fileSizes
index|[
name|start
index|]
operator|>
name|Math
operator|.
name|max
argument_list|(
name|minCompactSize
argument_list|,
call|(
name|long
call|)
argument_list|(
name|sumSize
index|[
name|start
operator|+
literal|1
index|]
operator|*
name|r
argument_list|)
argument_list|)
condition|)
block|{
operator|++
name|start
expr_stmt|;
block|}
name|int
name|end
init|=
name|Math
operator|.
name|min
argument_list|(
name|countOfFiles
argument_list|,
name|start
operator|+
name|this
operator|.
name|maxFilesToCompact
argument_list|)
decl_stmt|;
name|long
name|totalSize
init|=
name|fileSizes
index|[
name|start
index|]
operator|+
operator|(
operator|(
name|start
operator|+
literal|1
operator|<
name|countOfFiles
operator|)
condition|?
name|sumSize
index|[
name|start
operator|+
literal|1
index|]
else|:
literal|0
operator|)
decl_stmt|;
name|compactSelection
operator|=
name|compactSelection
operator|.
name|getSubList
argument_list|(
name|start
argument_list|,
name|end
argument_list|)
expr_stmt|;
comment|// if we don't have enough files to compact, just wait
if|if
condition|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|<
name|this
operator|.
name|minFilesToCompact
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipped compaction of "
operator|+
name|this
operator|+
literal|".  Only "
operator|+
operator|(
name|end
operator|-
name|start
operator|)
operator|+
literal|" file(s) of size "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|" have met compaction criteria."
argument_list|)
expr_stmt|;
block|}
name|compactSelection
operator|.
name|emptyFileList
argument_list|()
expr_stmt|;
return|return
name|compactSelection
return|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|majorcompaction
condition|)
block|{
if|if
condition|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|>
name|this
operator|.
name|maxFilesToCompact
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Warning, compacting more than "
operator|+
name|this
operator|.
name|maxFilesToCompact
operator|+
literal|" files, probably because of a user-requested major compaction"
argument_list|)
expr_stmt|;
if|if
condition|(
name|priority
operator|!=
name|PRIORITY_USER
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Compacting more than max files on a non user-requested compaction"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|>
name|this
operator|.
name|maxFilesToCompact
condition|)
block|{
comment|// all files included in this compaction, up to max
name|int
name|pastMax
init|=
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|size
argument_list|()
operator|-
name|this
operator|.
name|maxFilesToCompact
decl_stmt|;
name|compactSelection
operator|.
name|getFilesToCompact
argument_list|()
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|pastMax
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|compactSelection
return|;
block|}
comment|/**    * Validates a store file by opening and closing it. In HFileV2 this should    * not be an expensive operation.    *    * @param path the path to the store file    */
specifier|private
name|void
name|validateStoreFile
parameter_list|(
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|StoreFile
name|storeFile
init|=
literal|null
decl_stmt|;
try|try
block|{
name|storeFile
operator|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|path
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|NoOpDataBlockEncoder
operator|.
name|INSTANCE
argument_list|)
expr_stmt|;
name|passSchemaMetricsTo
argument_list|(
name|storeFile
argument_list|)
expr_stmt|;
name|storeFile
operator|.
name|createReader
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to open store file : "
operator|+
name|path
operator|+
literal|", keeping it in tmp location"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
finally|finally
block|{
if|if
condition|(
name|storeFile
operator|!=
literal|null
condition|)
block|{
name|storeFile
operator|.
name|closeReader
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/*    *<p>It works by processing a compaction that's been written to disk.    *    *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *    *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Moving the new compacted StoreFile into place    * 2) Unload all replaced StoreFile, close and collect list to delete.    * 3) Loading the new TreeMap.    * 4) Compute new store size    *</pre>    *    * @param compactedFiles list of files that were compacted    * @param compactedFile StoreFile that is the result of the compaction    * @return StoreFile created. May be null.    * @throws IOException    */
name|StoreFile
name|completeCompaction
parameter_list|(
specifier|final
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|compactedFiles
parameter_list|,
specifier|final
name|StoreFile
operator|.
name|Writer
name|compactedFile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// 1. Moving the new files into place -- if there is a new file (may not
comment|// be if all cells were expired or deleted).
name|StoreFile
name|result
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|compactedFile
operator|!=
literal|null
condition|)
block|{
name|validateStoreFile
argument_list|(
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
comment|// Move the file into the right spot
name|Path
name|origPath
init|=
name|compactedFile
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|destPath
init|=
operator|new
name|Path
argument_list|(
name|homedir
argument_list|,
name|origPath
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Renaming compacted file at "
operator|+
name|origPath
operator|+
literal|" to "
operator|+
name|destPath
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|origPath
argument_list|,
name|destPath
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|origPath
operator|+
literal|" to "
operator|+
name|destPath
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|origPath
operator|+
literal|" to "
operator|+
name|destPath
argument_list|)
throw|;
block|}
name|result
operator|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|destPath
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|dataBlockEncoder
argument_list|)
expr_stmt|;
name|passSchemaMetricsTo
argument_list|(
name|result
argument_list|)
expr_stmt|;
name|result
operator|.
name|createReader
argument_list|()
expr_stmt|;
block|}
try|try
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Change this.storefiles so it reflects new state but do not
comment|// delete old store files until we have sent out notification of
comment|// change in case old files are still being accessed by outstanding
comment|// scanners.
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newStoreFiles
init|=
name|Lists
operator|.
name|newArrayList
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newStoreFiles
operator|.
name|removeAll
argument_list|(
name|compactedFiles
argument_list|)
expr_stmt|;
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|compactedFiles
argument_list|)
expr_stmt|;
comment|// safe bc: lock.writeLock()
comment|// If a StoreFile result, move it into place.  May be null.
if|if
condition|(
name|result
operator|!=
literal|null
condition|)
block|{
name|newStoreFiles
operator|.
name|add
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|newStoreFiles
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// We need the lock, as long as we are updating the storefiles
comment|// or changing the memstore. Let us release it before calling
comment|// notifyChangeReadersObservers. See HBASE-4485 for a possible
comment|// deadlock scenario that could have happened if continue to hold
comment|// the lock.
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
comment|// Tell observers that list of StoreFiles has changed.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
comment|// Finally, delete old store files.
for|for
control|(
name|StoreFile
name|hsf
range|:
name|compactedFiles
control|)
block|{
name|hsf
operator|.
name|deleteReader
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|=
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files in "
operator|+
name|this
operator|+
literal|". Compacted file is "
operator|+
operator|(
name|result
operator|==
literal|null
condition|?
literal|"none"
else|:
name|result
operator|.
name|toString
argument_list|()
operator|)
operator|+
literal|".  Files replaced "
operator|+
name|compactedFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// 4. Compute new store size
name|this
operator|.
name|storeSize
operator|=
literal|0L
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|=
literal|0L
expr_stmt|;
for|for
control|(
name|StoreFile
name|hsf
range|:
name|this
operator|.
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|hsf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|hsf
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|+=
name|r
operator|.
name|getTotalUncompressedBytes
argument_list|()
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
specifier|public
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|sortAndClone
parameter_list|(
name|List
argument_list|<
name|StoreFile
argument_list|>
name|storeFiles
parameter_list|)
block|{
name|Collections
operator|.
name|sort
argument_list|(
name|storeFiles
argument_list|,
name|StoreFile
operator|.
name|Comparators
operator|.
name|FLUSH_TIME
argument_list|)
expr_stmt|;
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|newList
init|=
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|storeFiles
argument_list|)
decl_stmt|;
return|return
name|newList
return|;
block|}
comment|// ////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * @return the number of files in this store    */
specifier|public
name|int
name|getNumberOfStoreFiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/*    * @param wantedVersions How many versions were asked for.    * @return wantedVersions or this families' {@link HConstants#VERSIONS}.    */
name|int
name|versionsToReturn
parameter_list|(
specifier|final
name|int
name|wantedVersions
parameter_list|)
block|{
if|if
condition|(
name|wantedVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
comment|// Make sure we do not return more than maximum versions for this store.
name|int
name|maxVersions
init|=
name|this
operator|.
name|family
operator|.
name|getMaxVersions
argument_list|()
decl_stmt|;
return|return
name|wantedVersions
operator|>
name|maxVersions
condition|?
name|maxVersions
else|:
name|wantedVersions
return|;
block|}
specifier|static
name|boolean
name|isExpired
parameter_list|(
specifier|final
name|KeyValue
name|key
parameter_list|,
specifier|final
name|long
name|oldestTimestamp
parameter_list|)
block|{
return|return
name|key
operator|.
name|getTimestamp
argument_list|()
operator|<
name|oldestTimestamp
return|;
block|}
comment|/**    * Find the key that matches<i>row</i> exactly, or the one that immediately    * precedes it. WARNING: Only use this method on a table where writes occur    * with strictly increasing timestamps. This method assumes this pattern of    * writes in order to make it reasonably performant.  Also our search is    * dependent on the axiom that deletes are for cells that are in the container    * that follows whether a memstore snapshot or a storefile, not for the    * current container: i.e. we'll see deletes before we come across cells we    * are to delete. Presumption is that the memstore#kvset is processed before    * memstore#snapshot and so on.    * @param row The row key of the targeted row.    * @return Found keyvalue or null if none found.    * @throws IOException    */
name|KeyValue
name|getRowKeyAtOrBefore
parameter_list|(
specifier|final
name|byte
index|[]
name|row
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If minVersions is set, we will not ignore expired KVs.
comment|// As we're only looking for the latest matches, that should be OK.
comment|// With minVersions> 0 we guarantee that any KV that has any version
comment|// at all (expired or not) has at least one version that will not expire.
comment|// Note that this method used to take a KeyValue as arguments. KeyValue
comment|// can be back-dated, a row key cannot.
name|long
name|ttlToUse
init|=
name|scanInfo
operator|.
name|getMinVersions
argument_list|()
operator|>
literal|0
condition|?
name|Long
operator|.
name|MAX_VALUE
else|:
name|this
operator|.
name|ttl
decl_stmt|;
name|KeyValue
name|kv
init|=
operator|new
name|KeyValue
argument_list|(
name|row
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
decl_stmt|;
name|GetClosestRowBeforeTracker
name|state
init|=
operator|new
name|GetClosestRowBeforeTracker
argument_list|(
name|this
operator|.
name|comparator
argument_list|,
name|kv
argument_list|,
name|ttlToUse
argument_list|,
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// First go to the memstore.  Pick up deletes and candidates.
name|this
operator|.
name|memstore
operator|.
name|getRowKeyAtOrBefore
argument_list|(
name|state
argument_list|)
expr_stmt|;
comment|// Check if match, if we got a candidate on the asked for 'kv' row.
comment|// Process each store file. Run through from newest to oldest.
for|for
control|(
name|StoreFile
name|sf
range|:
name|Lists
operator|.
name|reverse
argument_list|(
name|storefiles
argument_list|)
control|)
block|{
comment|// Update the candidate keys from the current map file
name|rowAtOrBeforeFromStoreFile
argument_list|(
name|sf
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
return|return
name|state
operator|.
name|getCandidate
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Check an individual MapFile for the row at or before a given row.    * @param f    * @param state    * @throws IOException    */
specifier|private
name|void
name|rowAtOrBeforeFromStoreFile
parameter_list|(
specifier|final
name|StoreFile
name|f
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|f
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|f
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|r
operator|.
name|getEntries
argument_list|()
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|f
operator|+
literal|" is a empty store file"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// TODO: Cache these keys rather than make each time?
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
name|KeyValue
name|firstOnRow
init|=
name|state
operator|.
name|getTargetKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|lastKV
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
block|{
comment|// If last key in file is not of the target table, no candidates in this
comment|// file.  Return.
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|lastKV
argument_list|)
condition|)
return|return;
comment|// If the row we're looking for is past the end of file, set search key to
comment|// last key. TODO: Cache last and first key rather than make each time.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|lastKV
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
block|}
comment|// Get a scanner that caches blocks and that uses pread.
name|HFileScanner
name|scanner
init|=
name|r
operator|.
name|getHFileReader
argument_list|()
operator|.
name|getScanner
argument_list|(
literal|true
argument_list|,
literal|true
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|// Seek scanner.  If can't seek it, return.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
return|return;
comment|// If we found candidate on firstOnRow, just return. THIS WILL NEVER HAPPEN!
comment|// Unlikely that there'll be an instance of actual first row in table.
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
return|return;
comment|// If here, need to start backing up.
while|while
condition|(
name|scanner
operator|.
name|seekBefore
argument_list|(
name|firstOnRow
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyLength
argument_list|()
argument_list|)
condition|)
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
if|if
condition|(
operator|!
name|state
operator|.
name|isBetterCandidate
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
comment|// Make new first on row.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|kv
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
comment|// Seek scanner.  If can't seek it, break.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
break|break;
comment|// If we find something, break;
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
break|break;
block|}
block|}
comment|/*    * Seek the file scanner to firstOnRow or first entry in file.    * @param scanner    * @param firstOnRow    * @param firstKV    * @return True if we successfully seeked scanner.    * @throws IOException    */
specifier|private
name|boolean
name|seekToScanner
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|KeyValue
name|firstKV
parameter_list|)
throws|throws
name|IOException
block|{
name|KeyValue
name|kv
init|=
name|firstOnRow
decl_stmt|;
comment|// If firstOnRow< firstKV, set to firstKV
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|firstKV
argument_list|,
name|firstOnRow
argument_list|)
operator|==
literal|0
condition|)
name|kv
operator|=
name|firstKV
expr_stmt|;
name|int
name|result
init|=
name|scanner
operator|.
name|seekTo
argument_list|(
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|result
operator|>=
literal|0
return|;
block|}
comment|/*    * When we come in here, we are probably at the kv just before we break into    * the row that firstOnRow is on.  Usually need to increment one time to get    * on to the row we are interested in.    * @param scanner    * @param firstOnRow    * @param state    * @return True we found a candidate.    * @throws IOException    */
specifier|private
name|boolean
name|walkForwardInSingleRow
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|foundCandidate
init|=
literal|false
decl_stmt|;
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
comment|// If we are not in the row, skip.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
continue|continue;
comment|// Did we go beyond the target row? If so break.
if|if
condition|(
name|state
operator|.
name|isTooFar
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
condition|)
break|break;
if|if
condition|(
name|state
operator|.
name|isExpired
argument_list|(
name|kv
argument_list|)
condition|)
block|{
continue|continue;
block|}
comment|// If we added something, this row is a contender. break.
if|if
condition|(
name|state
operator|.
name|handle
argument_list|(
name|kv
argument_list|)
condition|)
block|{
name|foundCandidate
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
return|return
name|foundCandidate
return|;
block|}
specifier|public
name|boolean
name|canSplit
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Not splitable if we find a reference store file present in the store.
for|for
control|(
name|StoreFile
name|sf
range|:
name|storefiles
control|)
block|{
if|if
condition|(
name|sf
operator|.
name|isReference
argument_list|()
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|sf
operator|+
literal|" is not splittable"
argument_list|)
expr_stmt|;
block|}
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Determines if Store should be split    * @return byte[] if store should be split, null otherwise.    */
specifier|public
name|byte
index|[]
name|getSplitPoint
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// sanity checks
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
comment|// Should already be enforced by the split policy!
assert|assert
operator|!
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
assert|;
comment|// Not splitable if we find a reference store file present in the store.
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|StoreFile
name|largestSf
init|=
literal|null
decl_stmt|;
for|for
control|(
name|StoreFile
name|sf
range|:
name|storefiles
control|)
block|{
if|if
condition|(
name|sf
operator|.
name|isReference
argument_list|()
condition|)
block|{
comment|// Should already be enforced since we return false in this case
assert|assert
literal|false
operator|:
literal|"getSplitPoint() called on a region that can't split!"
assert|;
return|return
literal|null
return|;
block|}
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|sf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|size
init|=
name|r
operator|.
name|length
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|largestSf
operator|=
name|sf
expr_stmt|;
block|}
block|}
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|largestSf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|largestSf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// Get first, last, and mid keys.  Midkey is the key that starts block
comment|// in middle of hfile.  Has column and timestamp.  Need to return just
comment|// the row we want to split on as midkey.
name|byte
index|[]
name|midkey
init|=
name|r
operator|.
name|midkey
argument_list|()
decl_stmt|;
if|if
condition|(
name|midkey
operator|!=
literal|null
condition|)
block|{
name|KeyValue
name|mk
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|midkey
argument_list|,
literal|0
argument_list|,
name|midkey
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
comment|// if the midkey is the same as the first and last keys, then we cannot
comment|// (ever) split this region.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|firstKey
argument_list|)
operator|==
literal|0
operator|&&
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|lastKey
argument_list|)
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"cannot split because midkey is the same as first or "
operator|+
literal|"last row"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
return|return
name|mk
operator|.
name|getRow
argument_list|()
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed getting store size for "
operator|+
name|this
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** @return aggregate size of all HStores used in the last compaction */
specifier|public
name|long
name|getLastCompactSize
parameter_list|()
block|{
return|return
name|this
operator|.
name|lastCompactSize
return|;
block|}
comment|/** @return aggregate size of HStore */
specifier|public
name|long
name|getSize
parameter_list|()
block|{
return|return
name|storeSize
return|;
block|}
specifier|public
name|void
name|triggerMajorCompaction
parameter_list|()
block|{
name|this
operator|.
name|forceMajor
operator|=
literal|true
expr_stmt|;
block|}
name|boolean
name|getForceMajorCompaction
parameter_list|()
block|{
return|return
name|this
operator|.
name|forceMajor
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return a scanner for both the memstore and the HStore files. Assumes we    * are not in a compaction.    * @throws IOException    */
specifier|public
name|StoreScanner
name|getScanner
parameter_list|(
name|Scan
name|scan
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|targetCols
parameter_list|)
throws|throws
name|IOException
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|targetCols
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|getColumnFamilyName
argument_list|()
return|;
block|}
comment|/**    * @return Count of store files    */
name|int
name|getStorefilesCount
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/**    * @return The size of the store files, in bytes, uncompressed.    */
name|long
name|getStoreSizeUncompressed
parameter_list|()
block|{
return|return
name|this
operator|.
name|totalUncompressedBytes
return|;
block|}
comment|/**    * @return The size of the store files, in bytes.    */
name|long
name|getStorefilesSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * @return The size of the store file indexes, in bytes.    */
name|long
name|getStorefilesIndexSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|indexSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * Returns the total size of all index blocks in the data block indexes,    * including the root level, intermediate levels, and the leaf level for    * multi-level indexes, or just the root level for single-level indexes.    *    * @return the total size of block indexes in the store    */
name|long
name|getTotalStaticIndexSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|size
operator|+=
name|s
operator|.
name|getReader
argument_list|()
operator|.
name|getUncompressedDataIndexSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * Returns the total byte size of all Bloom filter bit arrays. For compound    * Bloom filters even the Bloom blocks currently not loaded into the block    * cache are counted.    *    * @return the total size of all Bloom filters in the store    */
name|long
name|getTotalStaticBloomSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
name|size
operator|+=
name|r
operator|.
name|getTotalBloomSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * @return The size of this store's memstore, in bytes    */
name|long
name|getMemStoreSize
parameter_list|()
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|heapSize
argument_list|()
return|;
block|}
specifier|public
name|int
name|getCompactPriority
parameter_list|()
block|{
return|return
name|getCompactPriority
argument_list|(
name|NO_PRIORITY
argument_list|)
return|;
block|}
comment|/**    * @return The priority that this store should have in the compaction queue    * @param priority    */
specifier|public
name|int
name|getCompactPriority
parameter_list|(
name|int
name|priority
parameter_list|)
block|{
comment|// If this is a user-requested compaction, leave this at the highest priority
if|if
condition|(
name|priority
operator|==
name|PRIORITY_USER
condition|)
block|{
return|return
name|PRIORITY_USER
return|;
block|}
else|else
block|{
return|return
name|this
operator|.
name|blockingStoreFileCount
operator|-
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
block|}
name|boolean
name|throttleCompaction
parameter_list|(
name|long
name|compactionSize
parameter_list|)
block|{
comment|// see HBASE-5867 for discussion on the default
name|long
name|throttlePoint
init|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.regionserver.thread.compaction.throttle"
argument_list|,
literal|2
operator|*
name|this
operator|.
name|minFilesToCompact
operator|*
name|this
operator|.
name|region
operator|.
name|memstoreFlushSize
argument_list|)
decl_stmt|;
return|return
name|compactionSize
operator|>
name|throttlePoint
return|;
block|}
name|HRegion
name|getHRegion
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
return|;
block|}
name|HRegionInfo
name|getHRegionInfo
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
operator|.
name|regionInfo
return|;
block|}
comment|/**    * Increments the value for the given row/family/qualifier.    *    * This function will always be seen as atomic by other readers    * because it only puts a single KV to memstore. Thus no    * read/write control necessary.    *    * @param row    * @param f    * @param qualifier    * @param newValue the new value to set into memstore    * @return memstore size delta    * @throws IOException    */
specifier|public
name|long
name|updateColumnValue
parameter_list|(
name|byte
index|[]
name|row
parameter_list|,
name|byte
index|[]
name|f
parameter_list|,
name|byte
index|[]
name|qualifier
parameter_list|,
name|long
name|newValue
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|long
name|now
init|=
name|EnvironmentEdgeManager
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
return|return
name|this
operator|.
name|memstore
operator|.
name|updateColumnValue
argument_list|(
name|row
argument_list|,
name|f
argument_list|,
name|qualifier
argument_list|,
name|newValue
argument_list|,
name|now
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Adds or replaces the specified KeyValues.    *<p>    * For each KeyValue specified, if a cell with the same row, family, and    * qualifier exists in MemStore, it will be replaced.  Otherwise, it will just    * be inserted to MemStore.    *<p>    * This operation is atomic on each KeyValue (row/family/qualifier) but not    * necessarily atomic across all of them.    * @param kvs    * @return memstore size delta    * @throws IOException    */
specifier|public
name|long
name|upsert
parameter_list|(
name|List
argument_list|<
name|KeyValue
argument_list|>
name|kvs
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// TODO: Make this operation atomic w/ MVCC
return|return
name|this
operator|.
name|memstore
operator|.
name|upsert
argument_list|(
name|kvs
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|public
name|StoreFlusher
name|getStoreFlusher
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
return|return
operator|new
name|StoreFlusherImpl
argument_list|(
name|cacheFlushId
argument_list|)
return|;
block|}
specifier|private
class|class
name|StoreFlusherImpl
implements|implements
name|StoreFlusher
block|{
specifier|private
name|long
name|cacheFlushId
decl_stmt|;
specifier|private
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
decl_stmt|;
specifier|private
name|StoreFile
name|storeFile
decl_stmt|;
specifier|private
name|Path
name|storeFilePath
decl_stmt|;
specifier|private
name|TimeRangeTracker
name|snapshotTimeRangeTracker
decl_stmt|;
specifier|private
name|AtomicLong
name|flushedSize
decl_stmt|;
specifier|private
name|StoreFlusherImpl
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
name|this
operator|.
name|cacheFlushId
operator|=
name|cacheFlushId
expr_stmt|;
name|this
operator|.
name|flushedSize
operator|=
operator|new
name|AtomicLong
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|prepare
parameter_list|()
block|{
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
name|this
operator|.
name|snapshot
operator|=
name|memstore
operator|.
name|getSnapshot
argument_list|()
expr_stmt|;
name|this
operator|.
name|snapshotTimeRangeTracker
operator|=
name|memstore
operator|.
name|getSnapshotTimeRangeTracker
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|flushCache
parameter_list|(
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
name|storeFilePath
operator|=
name|Store
operator|.
name|this
operator|.
name|flushCache
argument_list|(
name|cacheFlushId
argument_list|,
name|snapshot
argument_list|,
name|snapshotTimeRangeTracker
argument_list|,
name|flushedSize
argument_list|,
name|status
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|commit
parameter_list|(
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|storeFilePath
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
name|storeFile
operator|=
name|Store
operator|.
name|this
operator|.
name|commitFile
argument_list|(
name|storeFilePath
argument_list|,
name|cacheFlushId
argument_list|,
name|snapshotTimeRangeTracker
argument_list|,
name|flushedSize
argument_list|,
name|status
argument_list|)
expr_stmt|;
comment|// Add new file to store files.  Clear snapshot too while we have
comment|// the Store write lock.
return|return
name|Store
operator|.
name|this
operator|.
name|updateStorefiles
argument_list|(
name|storeFile
argument_list|,
name|snapshot
argument_list|)
return|;
block|}
block|}
comment|/**    * See if there's too much store files in this store    * @return true if number of store files is greater than    *  the number defined in minFilesToCompact    */
specifier|public
name|boolean
name|needsCompaction
parameter_list|()
block|{
return|return
operator|(
name|storefiles
operator|.
name|size
argument_list|()
operator|-
name|filesCompacting
operator|.
name|size
argument_list|()
operator|)
operator|>
name|minFilesToCompact
return|;
block|}
comment|/**    * Used for tests. Get the cache configuration for this Store.    */
specifier|public
name|CacheConfig
name|getCacheConfig
parameter_list|()
block|{
return|return
name|this
operator|.
name|cacheConf
return|;
block|}
specifier|public
specifier|static
specifier|final
name|long
name|FIXED_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|SchemaConfigured
operator|.
name|SCHEMA_CONFIGURED_UNALIGNED_HEAP_SIZE
operator|+
operator|+
operator|(
literal|17
operator|*
name|ClassSize
operator|.
name|REFERENCE
operator|)
operator|+
operator|(
literal|6
operator|*
name|Bytes
operator|.
name|SIZEOF_LONG
operator|)
operator|+
operator|(
literal|5
operator|*
name|Bytes
operator|.
name|SIZEOF_INT
operator|)
operator|+
name|Bytes
operator|.
name|SIZEOF_BOOLEAN
argument_list|)
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|long
name|DEEP_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|FIXED_OVERHEAD
operator|+
name|ClassSize
operator|.
name|OBJECT
operator|+
name|ClassSize
operator|.
name|REENTRANT_LOCK
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP_ENTRY
operator|+
name|ClassSize
operator|.
name|OBJECT
operator|+
name|ScanInfo
operator|.
name|FIXED_OVERHEAD
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|long
name|heapSize
parameter_list|()
block|{
return|return
name|DEEP_OVERHEAD
operator|+
name|this
operator|.
name|memstore
operator|.
name|heapSize
argument_list|()
return|;
block|}
specifier|public
name|KeyValue
operator|.
name|KVComparator
name|getComparator
parameter_list|()
block|{
return|return
name|comparator
return|;
block|}
comment|/**    * Immutable information for scans over a store.    */
specifier|public
specifier|static
class|class
name|ScanInfo
block|{
specifier|private
name|byte
index|[]
name|family
decl_stmt|;
specifier|private
name|int
name|minVersions
decl_stmt|;
specifier|private
name|int
name|maxVersions
decl_stmt|;
specifier|private
name|long
name|ttl
decl_stmt|;
specifier|private
name|boolean
name|keepDeletedCells
decl_stmt|;
specifier|private
name|long
name|timeToPurgeDeletes
decl_stmt|;
specifier|private
name|KVComparator
name|comparator
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|long
name|FIXED_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|ClassSize
operator|.
name|OBJECT
operator|+
operator|(
literal|2
operator|*
name|ClassSize
operator|.
name|REFERENCE
operator|)
operator|+
operator|(
literal|2
operator|*
name|Bytes
operator|.
name|SIZEOF_INT
operator|)
operator|+
name|Bytes
operator|.
name|SIZEOF_LONG
operator|+
name|Bytes
operator|.
name|SIZEOF_BOOLEAN
argument_list|)
decl_stmt|;
comment|/**      * @param family Name of this store's column family      * @param minVersions Store's MIN_VERSIONS setting      * @param maxVersions Store's VERSIONS setting      * @param ttl Store's TTL (in ms)      * @param timeToPurgeDeletes duration in ms after which a delete marker can      *        be purged during a major compaction.      * @param keepDeletedCells Store's keepDeletedCells setting      * @param comparator The store's comparator      */
specifier|public
name|ScanInfo
parameter_list|(
name|byte
index|[]
name|family
parameter_list|,
name|int
name|minVersions
parameter_list|,
name|int
name|maxVersions
parameter_list|,
name|long
name|ttl
parameter_list|,
name|boolean
name|keepDeletedCells
parameter_list|,
name|long
name|timeToPurgeDeletes
parameter_list|,
name|KVComparator
name|comparator
parameter_list|)
block|{
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|minVersions
operator|=
name|minVersions
expr_stmt|;
name|this
operator|.
name|maxVersions
operator|=
name|maxVersions
expr_stmt|;
name|this
operator|.
name|ttl
operator|=
name|ttl
expr_stmt|;
name|this
operator|.
name|keepDeletedCells
operator|=
name|keepDeletedCells
expr_stmt|;
name|this
operator|.
name|timeToPurgeDeletes
operator|=
name|timeToPurgeDeletes
expr_stmt|;
name|this
operator|.
name|comparator
operator|=
name|comparator
expr_stmt|;
block|}
specifier|public
name|byte
index|[]
name|getFamily
parameter_list|()
block|{
return|return
name|family
return|;
block|}
specifier|public
name|int
name|getMinVersions
parameter_list|()
block|{
return|return
name|minVersions
return|;
block|}
specifier|public
name|int
name|getMaxVersions
parameter_list|()
block|{
return|return
name|maxVersions
return|;
block|}
specifier|public
name|long
name|getTtl
parameter_list|()
block|{
return|return
name|ttl
return|;
block|}
specifier|public
name|boolean
name|getKeepDeletedCells
parameter_list|()
block|{
return|return
name|keepDeletedCells
return|;
block|}
specifier|public
name|long
name|getTimeToPurgeDeletes
parameter_list|()
block|{
return|return
name|timeToPurgeDeletes
return|;
block|}
specifier|public
name|KVComparator
name|getComparator
parameter_list|()
block|{
return|return
name|comparator
return|;
block|}
block|}
block|}
end_class

end_unit

