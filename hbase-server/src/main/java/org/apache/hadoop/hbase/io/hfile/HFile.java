begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Closeable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|InetSocketAddress
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Optional
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|LongAdder
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|io
operator|.
name|IOUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|Cell
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|CellComparator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|FSDataInputStreamWrapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|MetricsIO
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|MetricsIOWrapperImpl
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|compress
operator|.
name|Compression
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|encoding
operator|.
name|DataBlockEncoding
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|ReaderContext
operator|.
name|ReaderType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|CellSink
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|ShipperListener
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|BloomFilterWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|FSUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|yetus
operator|.
name|audience
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hbase
operator|.
name|thirdparty
operator|.
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hbase
operator|.
name|thirdparty
operator|.
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_comment
comment|/**  * File format for hbase.  * A file of sorted key/value pairs. Both keys and values are byte arrays.  *<p>  * The memory footprint of a HFile includes the following (below is taken from the  *<a  * href=https://issues.apache.org/jira/browse/HADOOP-3315>TFile</a> documentation  * but applies also to HFile):  *<ul>  *<li>Some constant overhead of reading or writing a compressed block.  *<ul>  *<li>Each compressed block requires one compression/decompression codec for  * I/O.  *<li>Temporary space to buffer the key.  *<li>Temporary space to buffer the value.  *</ul>  *<li>HFile index, which is proportional to the total number of Data Blocks.  * The total amount of memory needed to hold the index can be estimated as  * (56+AvgKeySize)*NumBlocks.  *</ul>  * Suggestions on performance optimization.  *<ul>  *<li>Minimum block size. We recommend a setting of minimum block size between  * 8KB to 1MB for general usage. Larger block size is preferred if files are  * primarily for sequential access. However, it would lead to inefficient random  * access (because there are more data to decompress). Smaller blocks are good  * for random access, but require more memory to hold the block index, and may  * be slower to create (because we must flush the compressor stream at the  * conclusion of each data block, which leads to an FS I/O flush). Further, due  * to the internal caching in Compression codec, the smallest possible block  * size would be around 20KB-30KB.  *<li>The current implementation does not offer true multi-threading for  * reading. The implementation uses FSDataInputStream seek()+read(), which is  * shown to be much faster than positioned-read call in single thread mode.  * However, it also means that if multiple threads attempt to access the same  * HFile (using multiple scanners) simultaneously, the actual I/O is carried out  * sequentially even if they access different DFS blocks (Reexamine! pread seems  * to be 10% faster than seek+read in my testing -- stack).  *<li>Compression codec. Use "none" if the data is not very compressable (by  * compressable, I mean a compression ratio at least 2:1). Generally, use "lzo"  * as the starting point for experimenting. "gz" overs slightly better  * compression ratio over "lzo" but requires 4x CPU to compress and 2x CPU to  * decompress, comparing to "lzo".  *</ul>  *  * For more on the background behind HFile, see<a  * href=https://issues.apache.org/jira/browse/HBASE-61>HBASE-61</a>.  *<p>  * File is made of data blocks followed by meta data blocks (if any), a fileinfo  * block, data block index, meta data block index, and a fixed size trailer  * which records the offsets at which file changes content type.  *<pre>&lt;data blocks&gt;&lt;meta blocks&gt;&lt;fileinfo&gt;&lt;  * data index&gt;&lt;meta index&gt;&lt;trailer&gt;</pre>  * Each block has a bit of magic at its start.  Block are comprised of  * key/values.  In data blocks, they are both byte arrays.  Metadata blocks are  * a String key and a byte array value.  An empty file looks like this:  *<pre>&lt;fileinfo&gt;&lt;trailer&gt;</pre>.  That is, there are not data nor meta  * blocks present.  *<p>  * TODO: Do scanners need to be able to take a start and end row?  * TODO: Should BlockIndex know the name of its file?  Should it have a Path  * that points at its file say for the case where an index lives apart from  * an HFile instance?  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Private
specifier|public
specifier|final
class|class
name|HFile
block|{
comment|// LOG is being used in HFileBlock and CheckSumUtil
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|HFile
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/**    * Maximum length of key in HFile.    */
specifier|public
specifier|final
specifier|static
name|int
name|MAXIMUM_KEY_LENGTH
init|=
name|Integer
operator|.
name|MAX_VALUE
decl_stmt|;
comment|/**    * Default compression: none.    */
specifier|public
specifier|final
specifier|static
name|Compression
operator|.
name|Algorithm
name|DEFAULT_COMPRESSION_ALGORITHM
init|=
name|Compression
operator|.
name|Algorithm
operator|.
name|NONE
decl_stmt|;
comment|/** Minimum supported HFile format version */
specifier|public
specifier|static
specifier|final
name|int
name|MIN_FORMAT_VERSION
init|=
literal|2
decl_stmt|;
comment|/** Maximum supported HFile format version    */
specifier|public
specifier|static
specifier|final
name|int
name|MAX_FORMAT_VERSION
init|=
literal|3
decl_stmt|;
comment|/**    * Minimum HFile format version with support for persisting cell tags    */
specifier|public
specifier|static
specifier|final
name|int
name|MIN_FORMAT_VERSION_WITH_TAGS
init|=
literal|3
decl_stmt|;
comment|/** Default compression name: none. */
specifier|public
specifier|final
specifier|static
name|String
name|DEFAULT_COMPRESSION
init|=
name|DEFAULT_COMPRESSION_ALGORITHM
operator|.
name|getName
argument_list|()
decl_stmt|;
comment|/** Meta data block name for bloom filter bits. */
specifier|public
specifier|static
specifier|final
name|String
name|BLOOM_FILTER_DATA_KEY
init|=
literal|"BLOOM_FILTER_DATA"
decl_stmt|;
comment|/**    * We assume that HFile path ends with    * ROOT_DIR/TABLE_NAME/REGION_NAME/CF_NAME/HFILE, so it has at least this    * many levels of nesting. This is needed for identifying table and CF name    * from an HFile path.    */
specifier|public
specifier|final
specifier|static
name|int
name|MIN_NUM_HFILE_PATH_LEVELS
init|=
literal|5
decl_stmt|;
comment|/**    * The number of bytes per checksum.    */
specifier|public
specifier|static
specifier|final
name|int
name|DEFAULT_BYTES_PER_CHECKSUM
init|=
literal|16
operator|*
literal|1024
decl_stmt|;
comment|// For measuring number of checksum failures
specifier|static
specifier|final
name|LongAdder
name|CHECKSUM_FAILURES
init|=
operator|new
name|LongAdder
argument_list|()
decl_stmt|;
comment|// For tests. Gets incremented when we read a block whether from HDFS or from Cache.
specifier|public
specifier|static
specifier|final
name|LongAdder
name|DATABLOCK_READ_COUNT
init|=
operator|new
name|LongAdder
argument_list|()
decl_stmt|;
comment|/** Static instance for the metrics so that HFileReaders access the same instance */
specifier|static
specifier|final
name|MetricsIO
name|metrics
init|=
operator|new
name|MetricsIO
argument_list|(
operator|new
name|MetricsIOWrapperImpl
argument_list|()
argument_list|)
decl_stmt|;
comment|/**    * Shutdown constructor.    */
specifier|private
name|HFile
parameter_list|()
block|{}
comment|/**    * Number of checksum verification failures. It also    * clears the counter.    */
specifier|public
specifier|static
specifier|final
name|long
name|getAndResetChecksumFailuresCount
parameter_list|()
block|{
return|return
name|CHECKSUM_FAILURES
operator|.
name|sumThenReset
argument_list|()
return|;
block|}
comment|/**    * Number of checksum verification failures. It also    * clears the counter.    */
specifier|public
specifier|static
specifier|final
name|long
name|getChecksumFailuresCount
parameter_list|()
block|{
return|return
name|CHECKSUM_FAILURES
operator|.
name|sum
argument_list|()
return|;
block|}
specifier|public
specifier|static
specifier|final
name|void
name|updateReadLatency
parameter_list|(
name|long
name|latencyMillis
parameter_list|,
name|boolean
name|pread
parameter_list|)
block|{
if|if
condition|(
name|pread
condition|)
block|{
name|metrics
operator|.
name|updateFsPreadTime
argument_list|(
name|latencyMillis
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|metrics
operator|.
name|updateFsReadTime
argument_list|(
name|latencyMillis
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
specifier|final
name|void
name|updateWriteLatency
parameter_list|(
name|long
name|latencyMillis
parameter_list|)
block|{
name|metrics
operator|.
name|updateFsWriteTime
argument_list|(
name|latencyMillis
argument_list|)
expr_stmt|;
block|}
comment|/** API required to write an {@link HFile} */
specifier|public
interface|interface
name|Writer
extends|extends
name|Closeable
extends|,
name|CellSink
extends|,
name|ShipperListener
block|{
comment|/** Max memstore (mvcc) timestamp in FileInfo */
specifier|public
specifier|static
specifier|final
name|byte
index|[]
name|MAX_MEMSTORE_TS_KEY
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
literal|"MAX_MEMSTORE_TS_KEY"
argument_list|)
decl_stmt|;
comment|/** Add an element to the file info map. */
name|void
name|appendFileInfo
parameter_list|(
name|byte
index|[]
name|key
parameter_list|,
name|byte
index|[]
name|value
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/** @return the path to this {@link HFile} */
name|Path
name|getPath
parameter_list|()
function_decl|;
comment|/**      * Adds an inline block writer such as a multi-level block index writer or      * a compound Bloom filter writer.      */
name|void
name|addInlineBlockWriter
parameter_list|(
name|InlineBlockWriter
name|bloomWriter
parameter_list|)
function_decl|;
comment|// The below three methods take Writables.  We'd like to undo Writables but undoing the below
comment|// would be pretty painful.  Could take a byte [] or a Message but we want to be backward
comment|// compatible around hfiles so would need to map between Message and Writable or byte [] and
comment|// current Writable serialization.  This would be a bit of work to little gain.  Thats my
comment|// thinking at moment.  St.Ack 20121129
name|void
name|appendMetaBlock
parameter_list|(
name|String
name|bloomFilterMetaKey
parameter_list|,
name|Writable
name|metaWriter
parameter_list|)
function_decl|;
comment|/**      * Store general Bloom filter in the file. This does not deal with Bloom filter      * internals but is necessary, since Bloom filters are stored differently      * in HFile version 1 and version 2.      */
name|void
name|addGeneralBloomFilter
parameter_list|(
name|BloomFilterWriter
name|bfw
parameter_list|)
function_decl|;
comment|/**      * Store delete family Bloom filter in the file, which is only supported in      * HFile V2.      */
name|void
name|addDeleteFamilyBloomFilter
parameter_list|(
name|BloomFilterWriter
name|bfw
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**      * Return the file context for the HFile this writer belongs to      */
name|HFileContext
name|getFileContext
parameter_list|()
function_decl|;
block|}
comment|/**    * This variety of ways to construct writers is used throughout the code, and    * we want to be able to swap writer implementations.    */
specifier|public
specifier|static
class|class
name|WriterFactory
block|{
specifier|protected
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|protected
specifier|final
name|CacheConfig
name|cacheConf
decl_stmt|;
specifier|protected
name|FileSystem
name|fs
decl_stmt|;
specifier|protected
name|Path
name|path
decl_stmt|;
specifier|protected
name|FSDataOutputStream
name|ostream
decl_stmt|;
specifier|protected
name|InetSocketAddress
index|[]
name|favoredNodes
decl_stmt|;
specifier|private
name|HFileContext
name|fileContext
decl_stmt|;
specifier|protected
name|boolean
name|shouldDropBehind
init|=
literal|false
decl_stmt|;
name|WriterFactory
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|)
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|cacheConf
operator|=
name|cacheConf
expr_stmt|;
block|}
specifier|public
name|WriterFactory
name|withPath
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|fs
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|path
operator|=
name|path
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withOutputStream
parameter_list|(
name|FSDataOutputStream
name|ostream
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|ostream
argument_list|)
expr_stmt|;
name|this
operator|.
name|ostream
operator|=
name|ostream
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withFavoredNodes
parameter_list|(
name|InetSocketAddress
index|[]
name|favoredNodes
parameter_list|)
block|{
comment|// Deliberately not checking for null here.
name|this
operator|.
name|favoredNodes
operator|=
name|favoredNodes
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withFileContext
parameter_list|(
name|HFileContext
name|fileContext
parameter_list|)
block|{
name|this
operator|.
name|fileContext
operator|=
name|fileContext
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withShouldDropCacheBehind
parameter_list|(
name|boolean
name|shouldDropBehind
parameter_list|)
block|{
name|this
operator|.
name|shouldDropBehind
operator|=
name|shouldDropBehind
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|Writer
name|create
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
operator|(
name|path
operator|!=
literal|null
condition|?
literal|1
else|:
literal|0
operator|)
operator|+
operator|(
name|ostream
operator|!=
literal|null
condition|?
literal|1
else|:
literal|0
operator|)
operator|!=
literal|1
condition|)
block|{
throw|throw
operator|new
name|AssertionError
argument_list|(
literal|"Please specify exactly one of "
operator|+
literal|"filesystem/path or path"
argument_list|)
throw|;
block|}
if|if
condition|(
name|path
operator|!=
literal|null
condition|)
block|{
name|ostream
operator|=
name|HFileWriterImpl
operator|.
name|createOutputStream
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|path
argument_list|,
name|favoredNodes
argument_list|)
expr_stmt|;
try|try
block|{
name|ostream
operator|.
name|setDropBehind
argument_list|(
name|shouldDropBehind
operator|&&
name|cacheConf
operator|.
name|shouldDropBehindCompaction
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedOperationException
name|uoe
parameter_list|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
literal|"Unable to set drop behind on {}"
argument_list|,
name|path
argument_list|,
name|uoe
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Unable to set drop behind on {}"
argument_list|,
name|path
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|HFileWriterImpl
argument_list|(
name|conf
argument_list|,
name|cacheConf
argument_list|,
name|path
argument_list|,
name|ostream
argument_list|,
name|fileContext
argument_list|)
return|;
block|}
block|}
comment|/** The configuration key for HFile version to use for new files */
specifier|public
specifier|static
specifier|final
name|String
name|FORMAT_VERSION_KEY
init|=
literal|"hfile.format.version"
decl_stmt|;
specifier|public
specifier|static
name|int
name|getFormatVersion
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|int
name|version
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|FORMAT_VERSION_KEY
argument_list|,
name|MAX_FORMAT_VERSION
argument_list|)
decl_stmt|;
name|checkFormatVersion
argument_list|(
name|version
argument_list|)
expr_stmt|;
return|return
name|version
return|;
block|}
comment|/**    * Returns the factory to be used to create {@link HFile} writers.    * Disables block cache access for all writers created through the    * returned factory.    */
specifier|public
specifier|static
specifier|final
name|WriterFactory
name|getWriterFactoryNoCache
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|HFile
operator|.
name|getWriterFactory
argument_list|(
name|conf
argument_list|,
name|CacheConfig
operator|.
name|DISABLED
argument_list|)
return|;
block|}
comment|/**    * Returns the factory to be used to create {@link HFile} writers    */
specifier|public
specifier|static
specifier|final
name|WriterFactory
name|getWriterFactory
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|)
block|{
name|int
name|version
init|=
name|getFormatVersion
argument_list|(
name|conf
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|version
condition|)
block|{
case|case
literal|2
case|:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"This should never happen. "
operator|+
literal|"Did you change hfile.format.version to read v2? This version of the software writes v3"
operator|+
literal|" hfiles only (but it can read v2 files without having to update hfile.format.version "
operator|+
literal|"in hbase-site.xml)"
argument_list|)
throw|;
case|case
literal|3
case|:
return|return
operator|new
name|HFile
operator|.
name|WriterFactory
argument_list|(
name|conf
argument_list|,
name|cacheConf
argument_list|)
return|;
default|default:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Cannot create writer for HFile "
operator|+
literal|"format version "
operator|+
name|version
argument_list|)
throw|;
block|}
block|}
comment|/**    * An abstraction used by the block index.    * Implementations will check cache for any asked-for block and return cached block if found.    * Otherwise, after reading from fs, will try and put block into cache before returning.    */
specifier|public
interface|interface
name|CachingBlockReader
block|{
comment|/**      * Read in a file block.      * @param offset offset to read.      * @param onDiskBlockSize size of the block      * @param isCompaction is this block being read as part of a compaction      * @param expectedBlockType the block type we are expecting to read with this read operation,      *   or null to read whatever block type is available and avoid checking (that might reduce      *   caching efficiency of encoded data blocks)      * @param expectedDataBlockEncoding the data block encoding the caller is expecting data blocks      *   to be in, or null to not perform this check and return the block irrespective of the      *   encoding. This check only applies to data blocks and can be set to null when the caller is      *   expecting to read a non-data block and has set expectedBlockType accordingly.      * @return Block wrapped in a ByteBuffer.      */
name|HFileBlock
name|readBlock
parameter_list|(
name|long
name|offset
parameter_list|,
name|long
name|onDiskBlockSize
parameter_list|,
name|boolean
name|cacheBlock
parameter_list|,
specifier|final
name|boolean
name|pread
parameter_list|,
specifier|final
name|boolean
name|isCompaction
parameter_list|,
specifier|final
name|boolean
name|updateCacheMetrics
parameter_list|,
name|BlockType
name|expectedBlockType
parameter_list|,
name|DataBlockEncoding
name|expectedDataBlockEncoding
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
comment|/** An interface used by clients to open and iterate an {@link HFile}. */
specifier|public
interface|interface
name|Reader
extends|extends
name|Closeable
extends|,
name|CachingBlockReader
block|{
comment|/**      * Returns this reader's "name". Usually the last component of the path.      * Needs to be constant as the file is being moved to support caching on      * write.      */
name|String
name|getName
parameter_list|()
function_decl|;
name|CellComparator
name|getComparator
parameter_list|()
function_decl|;
name|HFileScanner
name|getScanner
parameter_list|(
name|boolean
name|cacheBlocks
parameter_list|,
specifier|final
name|boolean
name|pread
parameter_list|,
specifier|final
name|boolean
name|isCompaction
parameter_list|)
function_decl|;
name|HFileBlock
name|getMetaBlock
parameter_list|(
name|String
name|metaBlockName
parameter_list|,
name|boolean
name|cacheBlock
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|Optional
argument_list|<
name|Cell
argument_list|>
name|getLastKey
parameter_list|()
function_decl|;
name|Optional
argument_list|<
name|Cell
argument_list|>
name|midKey
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|long
name|length
parameter_list|()
function_decl|;
name|long
name|getEntries
parameter_list|()
function_decl|;
name|Optional
argument_list|<
name|Cell
argument_list|>
name|getFirstKey
parameter_list|()
function_decl|;
name|long
name|indexSize
parameter_list|()
function_decl|;
name|Optional
argument_list|<
name|byte
index|[]
argument_list|>
name|getFirstRowKey
parameter_list|()
function_decl|;
name|Optional
argument_list|<
name|byte
index|[]
argument_list|>
name|getLastRowKey
parameter_list|()
function_decl|;
name|FixedFileTrailer
name|getTrailer
parameter_list|()
function_decl|;
name|void
name|setDataBlockIndexReader
parameter_list|(
name|HFileBlockIndex
operator|.
name|CellBasedKeyBlockIndexReader
name|reader
parameter_list|)
function_decl|;
name|HFileBlockIndex
operator|.
name|CellBasedKeyBlockIndexReader
name|getDataBlockIndexReader
parameter_list|()
function_decl|;
name|void
name|setMetaBlockIndexReader
parameter_list|(
name|HFileBlockIndex
operator|.
name|ByteArrayKeyBlockIndexReader
name|reader
parameter_list|)
function_decl|;
name|HFileBlockIndex
operator|.
name|ByteArrayKeyBlockIndexReader
name|getMetaBlockIndexReader
parameter_list|()
function_decl|;
name|HFileScanner
name|getScanner
parameter_list|(
name|boolean
name|cacheBlocks
parameter_list|,
name|boolean
name|pread
parameter_list|)
function_decl|;
comment|/**      * Retrieves general Bloom filter metadata as appropriate for each      * {@link HFile} version.      * Knows nothing about how that metadata is structured.      */
name|DataInput
name|getGeneralBloomFilterMetadata
parameter_list|()
throws|throws
name|IOException
function_decl|;
comment|/**      * Retrieves delete family Bloom filter metadata as appropriate for each      * {@link HFile}  version.      * Knows nothing about how that metadata is structured.      */
name|DataInput
name|getDeleteBloomFilterMetadata
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|Path
name|getPath
parameter_list|()
function_decl|;
comment|/** Close method with optional evictOnClose */
name|void
name|close
parameter_list|(
name|boolean
name|evictOnClose
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|DataBlockEncoding
name|getDataBlockEncoding
parameter_list|()
function_decl|;
name|boolean
name|hasMVCCInfo
parameter_list|()
function_decl|;
comment|/**      * Return the file context of the HFile this reader belongs to      */
name|HFileContext
name|getFileContext
parameter_list|()
function_decl|;
name|boolean
name|isPrimaryReplicaReader
parameter_list|()
function_decl|;
name|DataBlockEncoding
name|getEffectiveEncodingInCache
parameter_list|(
name|boolean
name|isCompaction
parameter_list|)
function_decl|;
annotation|@
name|VisibleForTesting
name|HFileBlock
operator|.
name|FSReader
name|getUncachedBlockReader
parameter_list|()
function_decl|;
annotation|@
name|VisibleForTesting
name|boolean
name|prefetchComplete
parameter_list|()
function_decl|;
comment|/**      * To close the stream's socket. Note: This can be concurrently called from multiple threads and      * implementation should take care of thread safety.      */
name|void
name|unbufferStream
parameter_list|()
function_decl|;
name|ReaderContext
name|getContext
parameter_list|()
function_decl|;
name|HFileInfo
name|getHFileInfo
parameter_list|()
function_decl|;
name|void
name|setDataBlockEncoder
parameter_list|(
name|HFileDataBlockEncoder
name|dataBlockEncoder
parameter_list|)
function_decl|;
block|}
comment|/**    * Method returns the reader given the specified arguments.    * TODO This is a bad abstraction.  See HBASE-6635.    *    * @param context Reader context info    * @param fileInfo HFile info    * @param cacheConf Cache configuation values, cannot be null.    * @param conf Configuration    * @return an appropriate instance of HFileReader    * @throws IOException If file is invalid, will throw CorruptHFileException flavored IOException    */
annotation|@
name|edu
operator|.
name|umd
operator|.
name|cs
operator|.
name|findbugs
operator|.
name|annotations
operator|.
name|SuppressWarnings
argument_list|(
name|value
operator|=
literal|"SF_SWITCH_FALLTHROUGH"
argument_list|,
name|justification
operator|=
literal|"Intentional"
argument_list|)
specifier|public
specifier|static
name|Reader
name|createReader
parameter_list|(
name|ReaderContext
name|context
parameter_list|,
name|HFileInfo
name|fileInfo
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
if|if
condition|(
name|context
operator|.
name|getReaderType
argument_list|()
operator|==
name|ReaderType
operator|.
name|STREAM
condition|)
block|{
comment|// stream reader will share trailer with pread reader, see HFileStreamReader#copyFields
return|return
operator|new
name|HFileStreamReader
argument_list|(
name|context
argument_list|,
name|fileInfo
argument_list|,
name|cacheConf
argument_list|,
name|conf
argument_list|)
return|;
block|}
name|FixedFileTrailer
name|trailer
init|=
name|fileInfo
operator|.
name|getTrailer
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|trailer
operator|.
name|getMajorVersion
argument_list|()
condition|)
block|{
case|case
literal|2
case|:
name|LOG
operator|.
name|debug
argument_list|(
literal|"Opening HFile v2 with v3 reader"
argument_list|)
expr_stmt|;
comment|// Fall through. FindBugs: SF_SWITCH_FALLTHROUGH
case|case
literal|3
case|:
return|return
operator|new
name|HFilePreadReader
argument_list|(
name|context
argument_list|,
name|fileInfo
argument_list|,
name|cacheConf
argument_list|,
name|conf
argument_list|)
return|;
default|default:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Invalid HFile version "
operator|+
name|trailer
operator|.
name|getMajorVersion
argument_list|()
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{
name|IOUtils
operator|.
name|closeQuietly
argument_list|(
name|context
operator|.
name|getInputStreamWrapper
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|CorruptHFileException
argument_list|(
literal|"Problem reading HFile Trailer from file "
operator|+
name|context
operator|.
name|getFilePath
argument_list|()
argument_list|,
name|t
argument_list|)
throw|;
block|}
finally|finally
block|{
name|context
operator|.
name|getInputStreamWrapper
argument_list|()
operator|.
name|unbuffer
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Creates reader with cache configuration disabled    * @param fs filesystem    * @param path Path to file to read    * @param conf Configuration    * @return an active Reader instance    * @throws IOException Will throw a CorruptHFileException    *   (DoNotRetryIOException subtype) if hfile is corrupt/invalid.    */
specifier|public
specifier|static
name|Reader
name|createReader
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
comment|// The primaryReplicaReader is mainly used for constructing block cache key, so if we do not use
comment|// block cache then it is OK to set it as any value. We use true here.
return|return
name|createReader
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
name|CacheConfig
operator|.
name|DISABLED
argument_list|,
literal|true
argument_list|,
name|conf
argument_list|)
return|;
block|}
comment|/**    * @param fs filesystem    * @param path Path to file to read    * @param cacheConf This must not be null. @see    *          {@link org.apache.hadoop.hbase.io.hfile.CacheConfig#CacheConfig(Configuration)}    * @param primaryReplicaReader true if this is a reader for primary replica    * @param conf Configuration    * @return an active Reader instance    * @throws IOException Will throw a CorruptHFileException (DoNotRetryIOException subtype) if hfile    *           is corrupt/invalid.    */
specifier|public
specifier|static
name|Reader
name|createReader
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|,
name|boolean
name|primaryReplicaReader
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|cacheConf
argument_list|,
literal|"Cannot create Reader with null CacheConf"
argument_list|)
expr_stmt|;
name|FSDataInputStreamWrapper
name|stream
init|=
operator|new
name|FSDataInputStreamWrapper
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
decl_stmt|;
name|ReaderContext
name|context
init|=
operator|new
name|ReaderContextBuilder
argument_list|()
operator|.
name|withFilePath
argument_list|(
name|path
argument_list|)
operator|.
name|withInputStreamWrapper
argument_list|(
name|stream
argument_list|)
operator|.
name|withFileSize
argument_list|(
name|fs
operator|.
name|getFileStatus
argument_list|(
name|path
argument_list|)
operator|.
name|getLen
argument_list|()
argument_list|)
operator|.
name|withFileSystem
argument_list|(
name|stream
operator|.
name|getHfs
argument_list|()
argument_list|)
operator|.
name|withPrimaryReplicaReader
argument_list|(
name|primaryReplicaReader
argument_list|)
operator|.
name|withReaderType
argument_list|(
name|ReaderType
operator|.
name|PREAD
argument_list|)
operator|.
name|build
argument_list|()
decl_stmt|;
name|HFileInfo
name|fileInfo
init|=
operator|new
name|HFileInfo
argument_list|(
name|context
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|Reader
name|reader
init|=
name|createReader
argument_list|(
name|context
argument_list|,
name|fileInfo
argument_list|,
name|cacheConf
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|fileInfo
operator|.
name|initMetaAndIndex
argument_list|(
name|reader
argument_list|)
expr_stmt|;
return|return
name|reader
return|;
block|}
comment|/**    * Returns true if the specified file has a valid HFile Trailer.    * @param fs filesystem    * @param path Path to file to verify    * @return true if the file has a valid HFile Trailer, otherwise false    * @throws IOException if failed to read from the underlying stream    */
specifier|public
specifier|static
name|boolean
name|isHFileFormat
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|isHFileFormat
argument_list|(
name|fs
argument_list|,
name|fs
operator|.
name|getFileStatus
argument_list|(
name|path
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Returns true if the specified file has a valid HFile Trailer.    * @param fs filesystem    * @param fileStatus the file to verify    * @return true if the file has a valid HFile Trailer, otherwise false    * @throws IOException if failed to read from the underlying stream    */
specifier|public
specifier|static
name|boolean
name|isHFileFormat
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|FileStatus
name|fileStatus
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|Path
name|path
init|=
name|fileStatus
operator|.
name|getPath
argument_list|()
decl_stmt|;
specifier|final
name|long
name|size
init|=
name|fileStatus
operator|.
name|getLen
argument_list|()
decl_stmt|;
try|try
init|(
name|FSDataInputStreamWrapper
name|fsdis
init|=
operator|new
name|FSDataInputStreamWrapper
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
init|)
block|{
name|boolean
name|isHBaseChecksum
init|=
name|fsdis
operator|.
name|shouldUseHBaseChecksum
argument_list|()
decl_stmt|;
assert|assert
operator|!
name|isHBaseChecksum
assert|;
comment|// Initially we must read with FS checksum.
name|FixedFileTrailer
operator|.
name|readFromStream
argument_list|(
name|fsdis
operator|.
name|getStream
argument_list|(
name|isHBaseChecksum
argument_list|)
argument_list|,
name|size
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
catch|catch
parameter_list|(
name|IllegalArgumentException
name|e
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
block|}
comment|/**    * Get names of supported compression algorithms. The names are acceptable by    * HFile.Writer.    *    * @return Array of strings, each represents a supported compression    *         algorithm. Currently, the following compression algorithms are    *         supported.    *<ul>    *<li>"none" - No compression.    *<li>"gz" - GZIP compression.    *</ul>    */
specifier|public
specifier|static
name|String
index|[]
name|getSupportedCompressionAlgorithms
parameter_list|()
block|{
return|return
name|Compression
operator|.
name|getSupportedAlgorithms
argument_list|()
return|;
block|}
comment|// Utility methods.
comment|/*    * @param l Long to convert to an int.    * @return<code>l</code> cast as an int.    */
specifier|static
name|int
name|longToInt
parameter_list|(
specifier|final
name|long
name|l
parameter_list|)
block|{
comment|// Expecting the size() of a block not exceeding 4GB. Assuming the
comment|// size() will wrap to negative integer if it exceeds 2GB (From tfile).
return|return
call|(
name|int
call|)
argument_list|(
name|l
operator|&
literal|0x00000000ffffffffL
argument_list|)
return|;
block|}
comment|/**    * Returns all HFiles belonging to the given region directory. Could return an    * empty list.    *    * @param fs  The file system reference.    * @param regionDir  The region directory to scan.    * @return The list of files found.    * @throws IOException When scanning the files fails.    */
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|getStoreFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|regionDir
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|Path
argument_list|>
name|regionHFiles
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|PathFilter
name|dirFilter
init|=
operator|new
name|FSUtils
operator|.
name|DirFilter
argument_list|(
name|fs
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|familyDirs
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|regionDir
argument_list|,
name|dirFilter
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|dir
range|:
name|familyDirs
control|)
block|{
name|FileStatus
index|[]
name|files
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
if|if
condition|(
operator|!
name|file
operator|.
name|isDirectory
argument_list|()
operator|&&
operator|(
operator|!
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|HConstants
operator|.
name|HREGION_OLDLOGDIR_NAME
argument_list|)
operator|)
operator|&&
operator|(
operator|!
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|HConstants
operator|.
name|RECOVERED_EDITS_DIR
argument_list|)
operator|)
condition|)
block|{
name|regionHFiles
operator|.
name|add
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|regionHFiles
return|;
block|}
comment|/**    * Checks the given {@link HFile} format version, and throws an exception if    * invalid. Note that if the version number comes from an input file and has    * not been verified, the caller needs to re-throw an {@link IOException} to    * indicate that this is not a software error, but corrupted input.    *    * @param version an HFile version    * @throws IllegalArgumentException if the version is invalid    */
specifier|public
specifier|static
name|void
name|checkFormatVersion
parameter_list|(
name|int
name|version
parameter_list|)
throws|throws
name|IllegalArgumentException
block|{
if|if
condition|(
name|version
argument_list|<
name|MIN_FORMAT_VERSION
operator|||
name|version
argument_list|>
name|MAX_FORMAT_VERSION
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Invalid HFile version: "
operator|+
name|version
operator|+
literal|" (expected to be "
operator|+
literal|"between "
operator|+
name|MIN_FORMAT_VERSION
operator|+
literal|" and "
operator|+
name|MAX_FORMAT_VERSION
operator|+
literal|")"
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|void
name|checkHFileVersion
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|)
block|{
name|int
name|version
init|=
name|c
operator|.
name|getInt
argument_list|(
name|FORMAT_VERSION_KEY
argument_list|,
name|MAX_FORMAT_VERSION
argument_list|)
decl_stmt|;
if|if
condition|(
name|version
argument_list|<
name|MAX_FORMAT_VERSION
operator|||
name|version
argument_list|>
name|MAX_FORMAT_VERSION
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"The setting for "
operator|+
name|FORMAT_VERSION_KEY
operator|+
literal|" (in your hbase-*.xml files) is "
operator|+
name|version
operator|+
literal|" which does not match "
operator|+
name|MAX_FORMAT_VERSION
operator|+
literal|"; are you running with a configuration from an older or newer hbase install (an "
operator|+
literal|"incompatible hbase-default.xml or hbase-site.xml on your CLASSPATH)?"
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
comment|// delegate to preserve old behavior
name|HFilePrettyPrinter
operator|.
name|main
argument_list|(
name|args
argument_list|)
expr_stmt|;
block|}
block|}
end_class

end_unit

