begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Closeable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|SequenceInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|InetSocketAddress
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|ByteBuffer
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Comparator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicLong
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ByteStringer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|classification
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataInputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|Cell
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|CellComparator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|fs
operator|.
name|HFileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|FSDataInputStreamWrapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|compress
operator|.
name|Compression
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|encoding
operator|.
name|DataBlockEncoding
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|protobuf
operator|.
name|ProtobufMagic
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|protobuf
operator|.
name|ProtobufUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|protobuf
operator|.
name|generated
operator|.
name|HBaseProtos
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|protobuf
operator|.
name|generated
operator|.
name|HBaseProtos
operator|.
name|BytesBytesPair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|protobuf
operator|.
name|generated
operator|.
name|HFileProtos
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|BloomFilterWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|FSUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_comment
comment|/**  * File format for hbase.  * A file of sorted key/value pairs. Both keys and values are byte arrays.  *<p>  * The memory footprint of a HFile includes the following (below is taken from the  *<a  * href=https://issues.apache.org/jira/browse/HADOOP-3315>TFile</a> documentation  * but applies also to HFile):  *<ul>  *<li>Some constant overhead of reading or writing a compressed block.  *<ul>  *<li>Each compressed block requires one compression/decompression codec for  * I/O.  *<li>Temporary space to buffer the key.  *<li>Temporary space to buffer the value.  *</ul>  *<li>HFile index, which is proportional to the total number of Data Blocks.  * The total amount of memory needed to hold the index can be estimated as  * (56+AvgKeySize)*NumBlocks.  *</ul>  * Suggestions on performance optimization.  *<ul>  *<li>Minimum block size. We recommend a setting of minimum block size between  * 8KB to 1MB for general usage. Larger block size is preferred if files are  * primarily for sequential access. However, it would lead to inefficient random  * access (because there are more data to decompress). Smaller blocks are good  * for random access, but require more memory to hold the block index, and may  * be slower to create (because we must flush the compressor stream at the  * conclusion of each data block, which leads to an FS I/O flush). Further, due  * to the internal caching in Compression codec, the smallest possible block  * size would be around 20KB-30KB.  *<li>The current implementation does not offer true multi-threading for  * reading. The implementation uses FSDataInputStream seek()+read(), which is  * shown to be much faster than positioned-read call in single thread mode.  * However, it also means that if multiple threads attempt to access the same  * HFile (using multiple scanners) simultaneously, the actual I/O is carried out  * sequentially even if they access different DFS blocks (Reexamine! pread seems  * to be 10% faster than seek+read in my testing -- stack).  *<li>Compression codec. Use "none" if the data is not very compressable (by  * compressable, I mean a compression ratio at least 2:1). Generally, use "lzo"  * as the starting point for experimenting. "gz" overs slightly better  * compression ratio over "lzo" but requires 4x CPU to compress and 2x CPU to  * decompress, comparing to "lzo".  *</ul>  *  * For more on the background behind HFile, see<a  * href=https://issues.apache.org/jira/browse/HBASE-61>HBASE-61</a>.  *<p>  * File is made of data blocks followed by meta data blocks (if any), a fileinfo  * block, data block index, meta data block index, and a fixed size trailer  * which records the offsets at which file changes content type.  *<pre>&lt;data blocks&gt;&lt;meta blocks&gt;&lt;fileinfo&gt;&lt;  * data index&gt;&lt;meta index&gt;&lt;trailer&gt;</pre>  * Each block has a bit of magic at its start.  Block are comprised of  * key/values.  In data blocks, they are both byte arrays.  Metadata blocks are  * a String key and a byte array value.  An empty file looks like this:  *<pre>&lt;fileinfo&gt;&lt;trailer&gt;</pre>.  That is, there are not data nor meta  * blocks present.  *<p>  * TODO: Do scanners need to be able to take a start and end row?  * TODO: Should BlockIndex know the name of its file?  Should it have a Path  * that points at its file say for the case where an index lives apart from  * an HFile instance?  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Private
specifier|public
class|class
name|HFile
block|{
comment|// LOG is being used in HFileBlock and CheckSumUtil
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HFile
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/**    * Maximum length of key in HFile.    */
specifier|public
specifier|final
specifier|static
name|int
name|MAXIMUM_KEY_LENGTH
init|=
name|Integer
operator|.
name|MAX_VALUE
decl_stmt|;
comment|/**    * Default compression: none.    */
specifier|public
specifier|final
specifier|static
name|Compression
operator|.
name|Algorithm
name|DEFAULT_COMPRESSION_ALGORITHM
init|=
name|Compression
operator|.
name|Algorithm
operator|.
name|NONE
decl_stmt|;
comment|/** Minimum supported HFile format version */
specifier|public
specifier|static
specifier|final
name|int
name|MIN_FORMAT_VERSION
init|=
literal|2
decl_stmt|;
comment|/** Maximum supported HFile format version    */
specifier|public
specifier|static
specifier|final
name|int
name|MAX_FORMAT_VERSION
init|=
literal|3
decl_stmt|;
comment|/**    * Minimum HFile format version with support for persisting cell tags    */
specifier|public
specifier|static
specifier|final
name|int
name|MIN_FORMAT_VERSION_WITH_TAGS
init|=
literal|3
decl_stmt|;
comment|/** Default compression name: none. */
specifier|public
specifier|final
specifier|static
name|String
name|DEFAULT_COMPRESSION
init|=
name|DEFAULT_COMPRESSION_ALGORITHM
operator|.
name|getName
argument_list|()
decl_stmt|;
comment|/** Meta data block name for bloom filter bits. */
specifier|public
specifier|static
specifier|final
name|String
name|BLOOM_FILTER_DATA_KEY
init|=
literal|"BLOOM_FILTER_DATA"
decl_stmt|;
comment|/**    * We assume that HFile path ends with    * ROOT_DIR/TABLE_NAME/REGION_NAME/CF_NAME/HFILE, so it has at least this    * many levels of nesting. This is needed for identifying table and CF name    * from an HFile path.    */
specifier|public
specifier|final
specifier|static
name|int
name|MIN_NUM_HFILE_PATH_LEVELS
init|=
literal|5
decl_stmt|;
comment|/**    * The number of bytes per checksum.    */
specifier|public
specifier|static
specifier|final
name|int
name|DEFAULT_BYTES_PER_CHECKSUM
init|=
literal|16
operator|*
literal|1024
decl_stmt|;
comment|// For measuring number of checksum failures
specifier|static
specifier|final
name|AtomicLong
name|checksumFailures
init|=
operator|new
name|AtomicLong
argument_list|()
decl_stmt|;
comment|// for test purpose
specifier|public
specifier|static
specifier|final
name|AtomicLong
name|dataBlockReadCnt
init|=
operator|new
name|AtomicLong
argument_list|(
literal|0
argument_list|)
decl_stmt|;
comment|/**    * Number of checksum verification failures. It also    * clears the counter.    */
specifier|public
specifier|static
specifier|final
name|long
name|getChecksumFailuresCount
parameter_list|()
block|{
return|return
name|checksumFailures
operator|.
name|getAndSet
argument_list|(
literal|0
argument_list|)
return|;
block|}
comment|/** API required to write an {@link HFile} */
specifier|public
interface|interface
name|Writer
extends|extends
name|Closeable
block|{
comment|/** Max memstore (mvcc) timestamp in FileInfo */
specifier|public
specifier|static
specifier|final
name|byte
index|[]
name|MAX_MEMSTORE_TS_KEY
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
literal|"MAX_MEMSTORE_TS_KEY"
argument_list|)
decl_stmt|;
comment|/** Add an element to the file info map. */
name|void
name|appendFileInfo
parameter_list|(
name|byte
index|[]
name|key
parameter_list|,
name|byte
index|[]
name|value
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|void
name|append
parameter_list|(
name|Cell
name|cell
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/** @return the path to this {@link HFile} */
name|Path
name|getPath
parameter_list|()
function_decl|;
comment|/**      * Adds an inline block writer such as a multi-level block index writer or      * a compound Bloom filter writer.      */
name|void
name|addInlineBlockWriter
parameter_list|(
name|InlineBlockWriter
name|bloomWriter
parameter_list|)
function_decl|;
comment|// The below three methods take Writables.  We'd like to undo Writables but undoing the below would be pretty
comment|// painful.  Could take a byte [] or a Message but we want to be backward compatible around hfiles so would need
comment|// to map between Message and Writable or byte [] and current Writable serialization.  This would be a bit of work
comment|// to little gain.  Thats my thinking at moment.  St.Ack 20121129
name|void
name|appendMetaBlock
parameter_list|(
name|String
name|bloomFilterMetaKey
parameter_list|,
name|Writable
name|metaWriter
parameter_list|)
function_decl|;
comment|/**      * Store general Bloom filter in the file. This does not deal with Bloom filter      * internals but is necessary, since Bloom filters are stored differently      * in HFile version 1 and version 2.      */
name|void
name|addGeneralBloomFilter
parameter_list|(
name|BloomFilterWriter
name|bfw
parameter_list|)
function_decl|;
comment|/**      * Store delete family Bloom filter in the file, which is only supported in      * HFile V2.      */
name|void
name|addDeleteFamilyBloomFilter
parameter_list|(
name|BloomFilterWriter
name|bfw
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**      * Return the file context for the HFile this writer belongs to      */
name|HFileContext
name|getFileContext
parameter_list|()
function_decl|;
block|}
comment|/**    * This variety of ways to construct writers is used throughout the code, and    * we want to be able to swap writer implementations.    */
specifier|public
specifier|static
specifier|abstract
class|class
name|WriterFactory
block|{
specifier|protected
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|protected
specifier|final
name|CacheConfig
name|cacheConf
decl_stmt|;
specifier|protected
name|FileSystem
name|fs
decl_stmt|;
specifier|protected
name|Path
name|path
decl_stmt|;
specifier|protected
name|FSDataOutputStream
name|ostream
decl_stmt|;
specifier|protected
name|CellComparator
name|comparator
init|=
name|CellComparator
operator|.
name|COMPARATOR
decl_stmt|;
specifier|protected
name|InetSocketAddress
index|[]
name|favoredNodes
decl_stmt|;
specifier|private
name|HFileContext
name|fileContext
decl_stmt|;
name|WriterFactory
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|)
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|cacheConf
operator|=
name|cacheConf
expr_stmt|;
block|}
specifier|public
name|WriterFactory
name|withPath
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|fs
argument_list|)
expr_stmt|;
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|path
operator|=
name|path
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withOutputStream
parameter_list|(
name|FSDataOutputStream
name|ostream
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|ostream
argument_list|)
expr_stmt|;
name|this
operator|.
name|ostream
operator|=
name|ostream
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withComparator
parameter_list|(
name|CellComparator
name|comparator
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|comparator
argument_list|)
expr_stmt|;
name|this
operator|.
name|comparator
operator|=
name|comparator
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withFavoredNodes
parameter_list|(
name|InetSocketAddress
index|[]
name|favoredNodes
parameter_list|)
block|{
comment|// Deliberately not checking for null here.
name|this
operator|.
name|favoredNodes
operator|=
name|favoredNodes
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|WriterFactory
name|withFileContext
parameter_list|(
name|HFileContext
name|fileContext
parameter_list|)
block|{
name|this
operator|.
name|fileContext
operator|=
name|fileContext
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|Writer
name|create
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
operator|(
name|path
operator|!=
literal|null
condition|?
literal|1
else|:
literal|0
operator|)
operator|+
operator|(
name|ostream
operator|!=
literal|null
condition|?
literal|1
else|:
literal|0
operator|)
operator|!=
literal|1
condition|)
block|{
throw|throw
operator|new
name|AssertionError
argument_list|(
literal|"Please specify exactly one of "
operator|+
literal|"filesystem/path or path"
argument_list|)
throw|;
block|}
if|if
condition|(
name|path
operator|!=
literal|null
condition|)
block|{
name|ostream
operator|=
name|HFileWriterImpl
operator|.
name|createOutputStream
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|path
argument_list|,
name|favoredNodes
argument_list|)
expr_stmt|;
block|}
return|return
name|createWriter
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
name|ostream
argument_list|,
name|comparator
argument_list|,
name|fileContext
argument_list|)
return|;
block|}
specifier|protected
specifier|abstract
name|Writer
name|createWriter
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|FSDataOutputStream
name|ostream
parameter_list|,
name|CellComparator
name|comparator
parameter_list|,
name|HFileContext
name|fileContext
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
comment|/** The configuration key for HFile version to use for new files */
specifier|public
specifier|static
specifier|final
name|String
name|FORMAT_VERSION_KEY
init|=
literal|"hfile.format.version"
decl_stmt|;
specifier|public
specifier|static
name|int
name|getFormatVersion
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|int
name|version
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|FORMAT_VERSION_KEY
argument_list|,
name|MAX_FORMAT_VERSION
argument_list|)
decl_stmt|;
name|checkFormatVersion
argument_list|(
name|version
argument_list|)
expr_stmt|;
return|return
name|version
return|;
block|}
comment|/**    * Returns the factory to be used to create {@link HFile} writers.    * Disables block cache access for all writers created through the    * returned factory.    */
specifier|public
specifier|static
specifier|final
name|WriterFactory
name|getWriterFactoryNoCache
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|Configuration
name|tempConf
init|=
operator|new
name|Configuration
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|tempConf
operator|.
name|setFloat
argument_list|(
name|HConstants
operator|.
name|HFILE_BLOCK_CACHE_SIZE_KEY
argument_list|,
literal|0.0f
argument_list|)
expr_stmt|;
return|return
name|HFile
operator|.
name|getWriterFactory
argument_list|(
name|conf
argument_list|,
operator|new
name|CacheConfig
argument_list|(
name|tempConf
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Returns the factory to be used to create {@link HFile} writers    */
specifier|public
specifier|static
specifier|final
name|WriterFactory
name|getWriterFactory
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|)
block|{
name|int
name|version
init|=
name|getFormatVersion
argument_list|(
name|conf
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|version
condition|)
block|{
case|case
literal|2
case|:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"This should never happen. "
operator|+
literal|"Did you change hfile.format.version to read v2? This version of the software writes v3"
operator|+
literal|" hfiles only (but it can read v2 files without having to update hfile.format.version "
operator|+
literal|"in hbase-site.xml)"
argument_list|)
throw|;
case|case
literal|3
case|:
return|return
operator|new
name|HFileWriterFactory
argument_list|(
name|conf
argument_list|,
name|cacheConf
argument_list|)
return|;
default|default:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Cannot create writer for HFile "
operator|+
literal|"format version "
operator|+
name|version
argument_list|)
throw|;
block|}
block|}
comment|/**    * An abstraction used by the block index.    * Implementations will check cache for any asked-for block and return cached block if found.    * Otherwise, after reading from fs, will try and put block into cache before returning.    */
specifier|public
interface|interface
name|CachingBlockReader
block|{
comment|/**      * Read in a file block.      * @param offset offset to read.      * @param onDiskBlockSize size of the block      * @param cacheBlock      * @param pread      * @param isCompaction is this block being read as part of a compaction      * @param expectedBlockType the block type we are expecting to read with this read operation,      *  or null to read whatever block type is available and avoid checking (that might reduce      *  caching efficiency of encoded data blocks)      * @param expectedDataBlockEncoding the data block encoding the caller is expecting data blocks      *  to be in, or null to not perform this check and return the block irrespective of the      *  encoding. This check only applies to data blocks and can be set to null when the caller is      *  expecting to read a non-data block and has set expectedBlockType accordingly.      * @return Block wrapped in a ByteBuffer.      * @throws IOException      */
name|HFileBlock
name|readBlock
parameter_list|(
name|long
name|offset
parameter_list|,
name|long
name|onDiskBlockSize
parameter_list|,
name|boolean
name|cacheBlock
parameter_list|,
specifier|final
name|boolean
name|pread
parameter_list|,
specifier|final
name|boolean
name|isCompaction
parameter_list|,
specifier|final
name|boolean
name|updateCacheMetrics
parameter_list|,
name|BlockType
name|expectedBlockType
parameter_list|,
name|DataBlockEncoding
name|expectedDataBlockEncoding
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
comment|/** An interface used by clients to open and iterate an {@link HFile}. */
specifier|public
interface|interface
name|Reader
extends|extends
name|Closeable
extends|,
name|CachingBlockReader
block|{
comment|/**      * Returns this reader's "name". Usually the last component of the path.      * Needs to be constant as the file is being moved to support caching on      * write.      */
name|String
name|getName
parameter_list|()
function_decl|;
name|CellComparator
name|getComparator
parameter_list|()
function_decl|;
name|HFileScanner
name|getScanner
parameter_list|(
name|boolean
name|cacheBlocks
parameter_list|,
specifier|final
name|boolean
name|pread
parameter_list|,
specifier|final
name|boolean
name|isCompaction
parameter_list|)
function_decl|;
name|ByteBuffer
name|getMetaBlock
parameter_list|(
name|String
name|metaBlockName
parameter_list|,
name|boolean
name|cacheBlock
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|Map
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|loadFileInfo
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|Cell
name|getLastKey
parameter_list|()
function_decl|;
name|Cell
name|midkey
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|long
name|length
parameter_list|()
function_decl|;
name|long
name|getEntries
parameter_list|()
function_decl|;
name|Cell
name|getFirstKey
parameter_list|()
function_decl|;
name|long
name|indexSize
parameter_list|()
function_decl|;
name|byte
index|[]
name|getFirstRowKey
parameter_list|()
function_decl|;
name|byte
index|[]
name|getLastRowKey
parameter_list|()
function_decl|;
name|FixedFileTrailer
name|getTrailer
parameter_list|()
function_decl|;
name|HFileBlockIndex
operator|.
name|BlockIndexReader
name|getDataBlockIndexReader
parameter_list|()
function_decl|;
name|HFileScanner
name|getScanner
parameter_list|(
name|boolean
name|cacheBlocks
parameter_list|,
name|boolean
name|pread
parameter_list|)
function_decl|;
name|Compression
operator|.
name|Algorithm
name|getCompressionAlgorithm
parameter_list|()
function_decl|;
comment|/**      * Retrieves general Bloom filter metadata as appropriate for each      * {@link HFile} version.      * Knows nothing about how that metadata is structured.      */
name|DataInput
name|getGeneralBloomFilterMetadata
parameter_list|()
throws|throws
name|IOException
function_decl|;
comment|/**      * Retrieves delete family Bloom filter metadata as appropriate for each      * {@link HFile}  version.      * Knows nothing about how that metadata is structured.      */
name|DataInput
name|getDeleteBloomFilterMetadata
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|Path
name|getPath
parameter_list|()
function_decl|;
comment|/** Close method with optional evictOnClose */
name|void
name|close
parameter_list|(
name|boolean
name|evictOnClose
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|DataBlockEncoding
name|getDataBlockEncoding
parameter_list|()
function_decl|;
name|boolean
name|hasMVCCInfo
parameter_list|()
function_decl|;
comment|/**      * Return the file context of the HFile this reader belongs to      */
name|HFileContext
name|getFileContext
parameter_list|()
function_decl|;
name|boolean
name|shouldIncludeMemstoreTS
parameter_list|()
function_decl|;
name|boolean
name|isDecodeMemstoreTS
parameter_list|()
function_decl|;
name|DataBlockEncoding
name|getEffectiveEncodingInCache
parameter_list|(
name|boolean
name|isCompaction
parameter_list|)
function_decl|;
annotation|@
name|VisibleForTesting
name|HFileBlock
operator|.
name|FSReader
name|getUncachedBlockReader
parameter_list|()
function_decl|;
annotation|@
name|VisibleForTesting
name|boolean
name|prefetchComplete
parameter_list|()
function_decl|;
block|}
comment|/**    * Method returns the reader given the specified arguments.    * TODO This is a bad abstraction.  See HBASE-6635.    *    * @param path hfile's path    * @param fsdis stream of path's file    * @param size max size of the trailer.    * @param cacheConf Cache configuation values, cannot be null.    * @param hfs    * @return an appropriate instance of HFileReader    * @throws IOException If file is invalid, will throw CorruptHFileException flavored IOException    */
specifier|private
specifier|static
name|Reader
name|pickReaderVersion
parameter_list|(
name|Path
name|path
parameter_list|,
name|FSDataInputStreamWrapper
name|fsdis
parameter_list|,
name|long
name|size
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|,
name|HFileSystem
name|hfs
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|FixedFileTrailer
name|trailer
init|=
literal|null
decl_stmt|;
try|try
block|{
name|boolean
name|isHBaseChecksum
init|=
name|fsdis
operator|.
name|shouldUseHBaseChecksum
argument_list|()
decl_stmt|;
assert|assert
operator|!
name|isHBaseChecksum
assert|;
comment|// Initially we must read with FS checksum.
name|trailer
operator|=
name|FixedFileTrailer
operator|.
name|readFromStream
argument_list|(
name|fsdis
operator|.
name|getStream
argument_list|(
name|isHBaseChecksum
argument_list|)
argument_list|,
name|size
argument_list|)
expr_stmt|;
switch|switch
condition|(
name|trailer
operator|.
name|getMajorVersion
argument_list|()
condition|)
block|{
case|case
literal|2
case|:
name|LOG
operator|.
name|debug
argument_list|(
literal|"Opening HFile v2 with v3 reader"
argument_list|)
expr_stmt|;
comment|// Fall through.
case|case
literal|3
case|:
return|return
operator|new
name|HFileReaderImpl
argument_list|(
name|path
argument_list|,
name|trailer
argument_list|,
name|fsdis
argument_list|,
name|size
argument_list|,
name|cacheConf
argument_list|,
name|hfs
argument_list|,
name|conf
argument_list|)
return|;
default|default:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Invalid HFile version "
operator|+
name|trailer
operator|.
name|getMajorVersion
argument_list|()
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{
try|try
block|{
name|fsdis
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t2
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error closing fsdis FSDataInputStreamWrapper"
argument_list|,
name|t2
argument_list|)
expr_stmt|;
block|}
throw|throw
operator|new
name|CorruptHFileException
argument_list|(
literal|"Problem reading HFile Trailer from file "
operator|+
name|path
argument_list|,
name|t
argument_list|)
throw|;
block|}
block|}
comment|/**    * @param fs A file system    * @param path Path to HFile    * @param fsdis a stream of path's file    * @param size max size of the trailer.    * @param cacheConf Cache configuration for hfile's contents    * @param conf Configuration    * @return A version specific Hfile Reader    * @throws IOException If file is invalid, will throw CorruptHFileException flavored IOException    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"resource"
argument_list|)
specifier|public
specifier|static
name|Reader
name|createReader
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|FSDataInputStreamWrapper
name|fsdis
parameter_list|,
name|long
name|size
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|HFileSystem
name|hfs
init|=
literal|null
decl_stmt|;
comment|// If the fs is not an instance of HFileSystem, then create an
comment|// instance of HFileSystem that wraps over the specified fs.
comment|// In this case, we will not be able to avoid checksumming inside
comment|// the filesystem.
if|if
condition|(
operator|!
operator|(
name|fs
operator|instanceof
name|HFileSystem
operator|)
condition|)
block|{
name|hfs
operator|=
operator|new
name|HFileSystem
argument_list|(
name|fs
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|hfs
operator|=
operator|(
name|HFileSystem
operator|)
name|fs
expr_stmt|;
block|}
return|return
name|pickReaderVersion
argument_list|(
name|path
argument_list|,
name|fsdis
argument_list|,
name|size
argument_list|,
name|cacheConf
argument_list|,
name|hfs
argument_list|,
name|conf
argument_list|)
return|;
block|}
comment|/**    *    * @param fs filesystem    * @param path Path to file to read    * @param cacheConf This must not be null.  @see {@link org.apache.hadoop.hbase.io.hfile.CacheConfig#CacheConfig(Configuration)}    * @return an active Reader instance    * @throws IOException Will throw a CorruptHFileException (DoNotRetryIOException subtype) if hfile is corrupt/invalid.    */
specifier|public
specifier|static
name|Reader
name|createReader
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|cacheConf
argument_list|,
literal|"Cannot create Reader with null CacheConf"
argument_list|)
expr_stmt|;
name|FSDataInputStreamWrapper
name|stream
init|=
operator|new
name|FSDataInputStreamWrapper
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
decl_stmt|;
return|return
name|pickReaderVersion
argument_list|(
name|path
argument_list|,
name|stream
argument_list|,
name|fs
operator|.
name|getFileStatus
argument_list|(
name|path
argument_list|)
operator|.
name|getLen
argument_list|()
argument_list|,
name|cacheConf
argument_list|,
name|stream
operator|.
name|getHfs
argument_list|()
argument_list|,
name|conf
argument_list|)
return|;
block|}
comment|/**    * This factory method is used only by unit tests    */
specifier|static
name|Reader
name|createReaderFromStream
parameter_list|(
name|Path
name|path
parameter_list|,
name|FSDataInputStream
name|fsdis
parameter_list|,
name|long
name|size
parameter_list|,
name|CacheConfig
name|cacheConf
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|FSDataInputStreamWrapper
name|wrapper
init|=
operator|new
name|FSDataInputStreamWrapper
argument_list|(
name|fsdis
argument_list|)
decl_stmt|;
return|return
name|pickReaderVersion
argument_list|(
name|path
argument_list|,
name|wrapper
argument_list|,
name|size
argument_list|,
name|cacheConf
argument_list|,
literal|null
argument_list|,
name|conf
argument_list|)
return|;
block|}
comment|/**    * Returns true if the specified file has a valid HFile Trailer.    * @param fs filesystem    * @param path Path to file to verify    * @return true if the file has a valid HFile Trailer, otherwise false    * @throws IOException if failed to read from the underlying stream    */
specifier|public
specifier|static
name|boolean
name|isHFileFormat
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|isHFileFormat
argument_list|(
name|fs
argument_list|,
name|fs
operator|.
name|getFileStatus
argument_list|(
name|path
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Returns true if the specified file has a valid HFile Trailer.    * @param fs filesystem    * @param fileStatus the file to verify    * @return true if the file has a valid HFile Trailer, otherwise false    * @throws IOException if failed to read from the underlying stream    */
specifier|public
specifier|static
name|boolean
name|isHFileFormat
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|FileStatus
name|fileStatus
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|Path
name|path
init|=
name|fileStatus
operator|.
name|getPath
argument_list|()
decl_stmt|;
specifier|final
name|long
name|size
init|=
name|fileStatus
operator|.
name|getLen
argument_list|()
decl_stmt|;
name|FSDataInputStreamWrapper
name|fsdis
init|=
operator|new
name|FSDataInputStreamWrapper
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
decl_stmt|;
try|try
block|{
name|boolean
name|isHBaseChecksum
init|=
name|fsdis
operator|.
name|shouldUseHBaseChecksum
argument_list|()
decl_stmt|;
assert|assert
operator|!
name|isHBaseChecksum
assert|;
comment|// Initially we must read with FS checksum.
name|FixedFileTrailer
operator|.
name|readFromStream
argument_list|(
name|fsdis
operator|.
name|getStream
argument_list|(
name|isHBaseChecksum
argument_list|)
argument_list|,
name|size
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
catch|catch
parameter_list|(
name|IllegalArgumentException
name|e
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
name|e
throw|;
block|}
finally|finally
block|{
try|try
block|{
name|fsdis
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error closing fsdis FSDataInputStreamWrapper: "
operator|+
name|path
argument_list|,
name|t
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Metadata for this file. Conjured by the writer. Read in by the reader.    */
specifier|public
specifier|static
class|class
name|FileInfo
implements|implements
name|SortedMap
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
block|{
specifier|static
specifier|final
name|String
name|RESERVED_PREFIX
init|=
literal|"hfile."
decl_stmt|;
specifier|static
specifier|final
name|byte
index|[]
name|RESERVED_PREFIX_BYTES
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|byte
index|[]
name|LASTKEY
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
operator|+
literal|"LASTKEY"
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|byte
index|[]
name|AVG_KEY_LEN
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
operator|+
literal|"AVG_KEY_LEN"
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|byte
index|[]
name|AVG_VALUE_LEN
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
operator|+
literal|"AVG_VALUE_LEN"
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|byte
index|[]
name|CREATE_TIME_TS
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
operator|+
literal|"CREATE_TIME_TS"
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|byte
index|[]
name|COMPARATOR
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
operator|+
literal|"COMPARATOR"
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|byte
index|[]
name|TAGS_COMPRESSED
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
operator|+
literal|"TAGS_COMPRESSED"
argument_list|)
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|byte
index|[]
name|MAX_TAGS_LEN
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RESERVED_PREFIX
operator|+
literal|"MAX_TAGS_LEN"
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|SortedMap
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|map
init|=
operator|new
name|TreeMap
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|(
name|Bytes
operator|.
name|BYTES_COMPARATOR
argument_list|)
decl_stmt|;
specifier|public
name|FileInfo
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
block|}
comment|/**      * Append the given key/value pair to the file info, optionally checking the      * key prefix.      *      * @param k key to add      * @param v value to add      * @param checkPrefix whether to check that the provided key does not start      *          with the reserved prefix      * @return this file info object      * @throws IOException if the key or value is invalid      */
specifier|public
name|FileInfo
name|append
parameter_list|(
specifier|final
name|byte
index|[]
name|k
parameter_list|,
specifier|final
name|byte
index|[]
name|v
parameter_list|,
specifier|final
name|boolean
name|checkPrefix
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|k
operator|==
literal|null
operator|||
name|v
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|NullPointerException
argument_list|(
literal|"Key nor value may be null"
argument_list|)
throw|;
block|}
if|if
condition|(
name|checkPrefix
operator|&&
name|isReservedFileInfoKey
argument_list|(
name|k
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Keys with a "
operator|+
name|FileInfo
operator|.
name|RESERVED_PREFIX
operator|+
literal|" are reserved"
argument_list|)
throw|;
block|}
name|put
argument_list|(
name|k
argument_list|,
name|v
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
specifier|public
name|void
name|clear
parameter_list|()
block|{
name|this
operator|.
name|map
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
specifier|public
name|Comparator
operator|<
condition|?
name|super
name|byte
index|[]
operator|>
name|comparator
argument_list|()
block|{
return|return
name|map
operator|.
name|comparator
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|containsKey
parameter_list|(
name|Object
name|key
parameter_list|)
block|{
return|return
name|map
operator|.
name|containsKey
argument_list|(
name|key
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|containsValue
parameter_list|(
name|Object
name|value
parameter_list|)
block|{
return|return
name|map
operator|.
name|containsValue
argument_list|(
name|value
argument_list|)
return|;
block|}
specifier|public
name|Set
argument_list|<
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|>
name|entrySet
parameter_list|()
block|{
return|return
name|map
operator|.
name|entrySet
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|equals
parameter_list|(
name|Object
name|o
parameter_list|)
block|{
return|return
name|map
operator|.
name|equals
argument_list|(
name|o
argument_list|)
return|;
block|}
specifier|public
name|byte
index|[]
name|firstKey
parameter_list|()
block|{
return|return
name|map
operator|.
name|firstKey
argument_list|()
return|;
block|}
specifier|public
name|byte
index|[]
name|get
parameter_list|(
name|Object
name|key
parameter_list|)
block|{
return|return
name|map
operator|.
name|get
argument_list|(
name|key
argument_list|)
return|;
block|}
specifier|public
name|int
name|hashCode
parameter_list|()
block|{
return|return
name|map
operator|.
name|hashCode
argument_list|()
return|;
block|}
specifier|public
name|SortedMap
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|headMap
parameter_list|(
name|byte
index|[]
name|toKey
parameter_list|)
block|{
return|return
name|this
operator|.
name|map
operator|.
name|headMap
argument_list|(
name|toKey
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|isEmpty
parameter_list|()
block|{
return|return
name|map
operator|.
name|isEmpty
argument_list|()
return|;
block|}
specifier|public
name|Set
argument_list|<
name|byte
index|[]
argument_list|>
name|keySet
parameter_list|()
block|{
return|return
name|map
operator|.
name|keySet
argument_list|()
return|;
block|}
specifier|public
name|byte
index|[]
name|lastKey
parameter_list|()
block|{
return|return
name|map
operator|.
name|lastKey
argument_list|()
return|;
block|}
specifier|public
name|byte
index|[]
name|put
parameter_list|(
name|byte
index|[]
name|key
parameter_list|,
name|byte
index|[]
name|value
parameter_list|)
block|{
return|return
name|this
operator|.
name|map
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
return|;
block|}
specifier|public
name|void
name|putAll
argument_list|(
name|Map
operator|<
condition|?
then|extends
name|byte
index|[]
argument_list|,
operator|?
expr|extends
name|byte
index|[]
operator|>
name|m
argument_list|)
block|{
name|this
operator|.
name|map
operator|.
name|putAll
argument_list|(
name|m
argument_list|)
expr_stmt|;
block|}
specifier|public
name|byte
index|[]
name|remove
parameter_list|(
name|Object
name|key
parameter_list|)
block|{
return|return
name|this
operator|.
name|map
operator|.
name|remove
argument_list|(
name|key
argument_list|)
return|;
block|}
specifier|public
name|int
name|size
parameter_list|()
block|{
return|return
name|map
operator|.
name|size
argument_list|()
return|;
block|}
specifier|public
name|SortedMap
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|subMap
parameter_list|(
name|byte
index|[]
name|fromKey
parameter_list|,
name|byte
index|[]
name|toKey
parameter_list|)
block|{
return|return
name|this
operator|.
name|map
operator|.
name|subMap
argument_list|(
name|fromKey
argument_list|,
name|toKey
argument_list|)
return|;
block|}
specifier|public
name|SortedMap
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|tailMap
parameter_list|(
name|byte
index|[]
name|fromKey
parameter_list|)
block|{
return|return
name|this
operator|.
name|map
operator|.
name|tailMap
argument_list|(
name|fromKey
argument_list|)
return|;
block|}
specifier|public
name|Collection
argument_list|<
name|byte
index|[]
argument_list|>
name|values
parameter_list|()
block|{
return|return
name|map
operator|.
name|values
argument_list|()
return|;
block|}
comment|/**      * Write out this instance on the passed in<code>out</code> stream.      * We write it as a protobuf.      * @param out      * @throws IOException      * @see #read(DataInputStream)      */
name|void
name|write
parameter_list|(
specifier|final
name|DataOutputStream
name|out
parameter_list|)
throws|throws
name|IOException
block|{
name|HFileProtos
operator|.
name|FileInfoProto
operator|.
name|Builder
name|builder
init|=
name|HFileProtos
operator|.
name|FileInfoProto
operator|.
name|newBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|e
range|:
name|this
operator|.
name|map
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HBaseProtos
operator|.
name|BytesBytesPair
operator|.
name|Builder
name|bbpBuilder
init|=
name|HBaseProtos
operator|.
name|BytesBytesPair
operator|.
name|newBuilder
argument_list|()
decl_stmt|;
name|bbpBuilder
operator|.
name|setFirst
argument_list|(
name|ByteStringer
operator|.
name|wrap
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|bbpBuilder
operator|.
name|setSecond
argument_list|(
name|ByteStringer
operator|.
name|wrap
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|builder
operator|.
name|addMapEntry
argument_list|(
name|bbpBuilder
operator|.
name|build
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|out
operator|.
name|write
argument_list|(
name|ProtobufMagic
operator|.
name|PB_MAGIC
argument_list|)
expr_stmt|;
name|builder
operator|.
name|build
argument_list|()
operator|.
name|writeDelimitedTo
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
comment|/**      * Populate this instance with what we find on the passed in<code>in</code> stream.      * Can deserialize protobuf of old Writables format.      * @param in      * @throws IOException      * @see #write(DataOutputStream)      */
name|void
name|read
parameter_list|(
specifier|final
name|DataInputStream
name|in
parameter_list|)
throws|throws
name|IOException
block|{
comment|// This code is tested over in TestHFileReaderV1 where we read an old hfile w/ this new code.
name|int
name|pblen
init|=
name|ProtobufUtil
operator|.
name|lengthOfPBMagic
argument_list|()
decl_stmt|;
name|byte
index|[]
name|pbuf
init|=
operator|new
name|byte
index|[
name|pblen
index|]
decl_stmt|;
if|if
condition|(
name|in
operator|.
name|markSupported
argument_list|()
condition|)
name|in
operator|.
name|mark
argument_list|(
name|pblen
argument_list|)
expr_stmt|;
name|int
name|read
init|=
name|in
operator|.
name|read
argument_list|(
name|pbuf
argument_list|)
decl_stmt|;
if|if
condition|(
name|read
operator|!=
name|pblen
condition|)
throw|throw
operator|new
name|IOException
argument_list|(
literal|"read="
operator|+
name|read
operator|+
literal|", wanted="
operator|+
name|pblen
argument_list|)
throw|;
if|if
condition|(
name|ProtobufUtil
operator|.
name|isPBMagicPrefix
argument_list|(
name|pbuf
argument_list|)
condition|)
block|{
name|parsePB
argument_list|(
name|HFileProtos
operator|.
name|FileInfoProto
operator|.
name|parseDelimitedFrom
argument_list|(
name|in
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|in
operator|.
name|markSupported
argument_list|()
condition|)
block|{
name|in
operator|.
name|reset
argument_list|()
expr_stmt|;
name|parseWritable
argument_list|(
name|in
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// We cannot use BufferedInputStream, it consumes more than we read from the underlying IS
name|ByteArrayInputStream
name|bais
init|=
operator|new
name|ByteArrayInputStream
argument_list|(
name|pbuf
argument_list|)
decl_stmt|;
name|SequenceInputStream
name|sis
init|=
operator|new
name|SequenceInputStream
argument_list|(
name|bais
argument_list|,
name|in
argument_list|)
decl_stmt|;
comment|// Concatenate input streams
comment|// TODO: Am I leaking anything here wrapping the passed in stream?  We are not calling close on the wrapped
comment|// streams but they should be let go after we leave this context?  I see that we keep a reference to the
comment|// passed in inputstream but since we no longer have a reference to this after we leave, we should be ok.
name|parseWritable
argument_list|(
operator|new
name|DataInputStream
argument_list|(
name|sis
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/** Now parse the old Writable format.  It was a list of Map entries.  Each map entry was a key and a value of      * a byte [].  The old map format had a byte before each entry that held a code which was short for the key or      * value type.  We know it was a byte [] so in below we just read and dump it.      * @throws IOException      */
name|void
name|parseWritable
parameter_list|(
specifier|final
name|DataInputStream
name|in
parameter_list|)
throws|throws
name|IOException
block|{
comment|// First clear the map.  Otherwise we will just accumulate entries every time this method is called.
name|this
operator|.
name|map
operator|.
name|clear
argument_list|()
expr_stmt|;
comment|// Read the number of entries in the map
name|int
name|entries
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
comment|// Then read each key/value pair
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|entries
condition|;
name|i
operator|++
control|)
block|{
name|byte
index|[]
name|key
init|=
name|Bytes
operator|.
name|readByteArray
argument_list|(
name|in
argument_list|)
decl_stmt|;
comment|// We used to read a byte that encoded the class type.  Read and ignore it because it is always byte [] in hfile
name|in
operator|.
name|readByte
argument_list|()
expr_stmt|;
name|byte
index|[]
name|value
init|=
name|Bytes
operator|.
name|readByteArray
argument_list|(
name|in
argument_list|)
decl_stmt|;
name|this
operator|.
name|map
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**      * Fill our map with content of the pb we read off disk      * @param fip protobuf message to read      */
name|void
name|parsePB
parameter_list|(
specifier|final
name|HFileProtos
operator|.
name|FileInfoProto
name|fip
parameter_list|)
block|{
name|this
operator|.
name|map
operator|.
name|clear
argument_list|()
expr_stmt|;
for|for
control|(
name|BytesBytesPair
name|pair
range|:
name|fip
operator|.
name|getMapEntryList
argument_list|()
control|)
block|{
name|this
operator|.
name|map
operator|.
name|put
argument_list|(
name|pair
operator|.
name|getFirst
argument_list|()
operator|.
name|toByteArray
argument_list|()
argument_list|,
name|pair
operator|.
name|getSecond
argument_list|()
operator|.
name|toByteArray
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/** Return true if the given file info key is reserved for internal use. */
specifier|public
specifier|static
name|boolean
name|isReservedFileInfoKey
parameter_list|(
name|byte
index|[]
name|key
parameter_list|)
block|{
return|return
name|Bytes
operator|.
name|startsWith
argument_list|(
name|key
argument_list|,
name|FileInfo
operator|.
name|RESERVED_PREFIX_BYTES
argument_list|)
return|;
block|}
comment|/**    * Get names of supported compression algorithms. The names are acceptable by    * HFile.Writer.    *    * @return Array of strings, each represents a supported compression    *         algorithm. Currently, the following compression algorithms are    *         supported.    *<ul>    *<li>"none" - No compression.    *<li>"gz" - GZIP compression.    *</ul>    */
specifier|public
specifier|static
name|String
index|[]
name|getSupportedCompressionAlgorithms
parameter_list|()
block|{
return|return
name|Compression
operator|.
name|getSupportedAlgorithms
argument_list|()
return|;
block|}
comment|// Utility methods.
comment|/*    * @param l Long to convert to an int.    * @return<code>l</code> cast as an int.    */
specifier|static
name|int
name|longToInt
parameter_list|(
specifier|final
name|long
name|l
parameter_list|)
block|{
comment|// Expecting the size() of a block not exceeding 4GB. Assuming the
comment|// size() will wrap to negative integer if it exceeds 2GB (From tfile).
return|return
call|(
name|int
call|)
argument_list|(
name|l
operator|&
literal|0x00000000ffffffffL
argument_list|)
return|;
block|}
comment|/**    * Returns all HFiles belonging to the given region directory. Could return an    * empty list.    *    * @param fs  The file system reference.    * @param regionDir  The region directory to scan.    * @return The list of files found.    * @throws IOException When scanning the files fails.    */
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|getStoreFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|regionDir
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|Path
argument_list|>
name|regionHFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|PathFilter
name|dirFilter
init|=
operator|new
name|FSUtils
operator|.
name|DirFilter
argument_list|(
name|fs
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|familyDirs
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|regionDir
argument_list|,
name|dirFilter
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|dir
range|:
name|familyDirs
control|)
block|{
name|FileStatus
index|[]
name|files
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
if|if
condition|(
operator|!
name|file
operator|.
name|isDirectory
argument_list|()
operator|&&
operator|(
operator|!
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|HConstants
operator|.
name|HREGION_OLDLOGDIR_NAME
argument_list|)
operator|)
operator|&&
operator|(
operator|!
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|HConstants
operator|.
name|RECOVERED_EDITS_DIR
argument_list|)
operator|)
condition|)
block|{
name|regionHFiles
operator|.
name|add
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|regionHFiles
return|;
block|}
comment|/**    * Checks the given {@link HFile} format version, and throws an exception if    * invalid. Note that if the version number comes from an input file and has    * not been verified, the caller needs to re-throw an {@link IOException} to    * indicate that this is not a software error, but corrupted input.    *    * @param version an HFile version    * @throws IllegalArgumentException if the version is invalid    */
specifier|public
specifier|static
name|void
name|checkFormatVersion
parameter_list|(
name|int
name|version
parameter_list|)
throws|throws
name|IllegalArgumentException
block|{
if|if
condition|(
name|version
argument_list|<
name|MIN_FORMAT_VERSION
operator|||
name|version
argument_list|>
name|MAX_FORMAT_VERSION
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Invalid HFile version: "
operator|+
name|version
operator|+
literal|" (expected to be "
operator|+
literal|"between "
operator|+
name|MIN_FORMAT_VERSION
operator|+
literal|" and "
operator|+
name|MAX_FORMAT_VERSION
operator|+
literal|")"
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|void
name|checkHFileVersion
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|)
block|{
name|int
name|version
init|=
name|c
operator|.
name|getInt
argument_list|(
name|FORMAT_VERSION_KEY
argument_list|,
name|MAX_FORMAT_VERSION
argument_list|)
decl_stmt|;
if|if
condition|(
name|version
argument_list|<
name|MAX_FORMAT_VERSION
operator|||
name|version
argument_list|>
name|MAX_FORMAT_VERSION
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"The setting for "
operator|+
name|FORMAT_VERSION_KEY
operator|+
literal|" (in your hbase-*.xml files) is "
operator|+
name|version
operator|+
literal|" which does not match "
operator|+
name|MAX_FORMAT_VERSION
operator|+
literal|"; are you running with a configuration from an older or newer hbase install (an "
operator|+
literal|"incompatible hbase-default.xml or hbase-site.xml on your CLASSPATH)?"
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
comment|// delegate to preserve old behavior
name|HFilePrettyPrinter
operator|.
name|main
argument_list|(
name|args
argument_list|)
expr_stmt|;
block|}
block|}
end_class

end_unit

