begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|mapreduce
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|UUID
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|classification
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Admin
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|ConnectionFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|RegionLocator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configurable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|Cell
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|CellUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HTableDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|TableName
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|ImmutableBytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Pair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IOUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
operator|.
name|Reader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
operator|.
name|Writer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Job
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|OutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Partitioner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|lib
operator|.
name|output
operator|.
name|FileOutputCommitter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|lib
operator|.
name|output
operator|.
name|FileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ReflectionUtils
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_comment
comment|/**  * Create 3 level tree directory, first level is using table name as parent directory and then use  * family name as child directory, and all related HFiles for one family are under child directory  * -tableName1  *   -columnFamilyName1  *     -HFile (region1)  *   -columnFamilyName2  *     -HFile1 (region1)  *     -HFile2 (region2)  *     -HFile3 (region3)  * -tableName2  *   -columnFamilyName1  *     -HFile (region1)  * family directory and its hfiles match the output of HFileOutputFormat2  * @see org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Public
annotation|@
name|VisibleForTesting
specifier|public
class|class
name|MultiTableHFileOutputFormat
extends|extends
name|FileOutputFormat
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|Cell
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|MultiTableHFileOutputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|RecordWriter
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|Cell
argument_list|>
name|getRecordWriter
parameter_list|(
specifier|final
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
return|return
name|createMultiHFileRecordWriter
argument_list|(
name|context
argument_list|)
return|;
block|}
specifier|static
parameter_list|<
name|V
extends|extends
name|Cell
parameter_list|>
name|RecordWriter
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|V
argument_list|>
name|createMultiHFileRecordWriter
parameter_list|(
specifier|final
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Get the path of the output directory
specifier|final
name|Path
name|outputPath
init|=
name|FileOutputFormat
operator|.
name|getOutputPath
argument_list|(
name|context
argument_list|)
decl_stmt|;
specifier|final
name|Path
name|outputDir
init|=
operator|new
name|FileOutputCommitter
argument_list|(
name|outputPath
argument_list|,
name|context
argument_list|)
operator|.
name|getWorkPath
argument_list|()
decl_stmt|;
specifier|final
name|Configuration
name|conf
init|=
name|context
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
specifier|final
name|FileSystem
name|fs
init|=
name|outputDir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Connection
name|conn
init|=
name|ConnectionFactory
operator|.
name|createConnection
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Admin
name|admin
init|=
name|conn
operator|.
name|getAdmin
argument_list|()
decl_stmt|;
comment|// Map of existing tables, avoid calling getTable() everytime
specifier|final
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|Table
argument_list|>
name|tables
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
comment|// Map of tables to writers
specifier|final
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|RecordWriter
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|V
argument_list|>
argument_list|>
name|tableWriters
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
return|return
operator|new
name|RecordWriter
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|V
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|ImmutableBytesWritable
name|tableName
parameter_list|,
name|V
name|cell
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
name|RecordWriter
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|V
argument_list|>
name|tableWriter
init|=
name|tableWriters
operator|.
name|get
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
comment|// if there is new table, verify that table directory exists
if|if
condition|(
name|tableWriter
operator|==
literal|null
condition|)
block|{
comment|// using table name as directory name
specifier|final
name|Path
name|tableOutputDir
init|=
operator|new
name|Path
argument_list|(
name|outputDir
argument_list|,
name|Bytes
operator|.
name|toString
argument_list|(
name|tableName
operator|.
name|copyBytes
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|tableOutputDir
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Writing Table '"
operator|+
name|tableName
operator|.
name|toString
argument_list|()
operator|+
literal|"' data into following directory"
operator|+
name|tableOutputDir
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// Configure for tableWriter, if table exist, write configuration of table into conf
name|Table
name|table
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|tables
operator|.
name|containsKey
argument_list|(
name|tableName
argument_list|)
condition|)
block|{
name|table
operator|=
name|tables
operator|.
name|get
argument_list|(
name|tableName
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|table
operator|=
name|getTable
argument_list|(
name|tableName
operator|.
name|copyBytes
argument_list|()
argument_list|,
name|conn
argument_list|,
name|admin
argument_list|)
expr_stmt|;
name|tables
operator|.
name|put
argument_list|(
name|tableName
argument_list|,
name|table
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|table
operator|!=
literal|null
condition|)
block|{
name|configureForOneTable
argument_list|(
name|conf
argument_list|,
name|table
operator|.
name|getTableDescriptor
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// Create writer for one specific table
name|tableWriter
operator|=
operator|new
name|HFileOutputFormat2
operator|.
name|HFileRecordWriter
argument_list|<>
argument_list|(
name|context
argument_list|,
name|tableOutputDir
argument_list|)
expr_stmt|;
comment|// Put table into map
name|tableWriters
operator|.
name|put
argument_list|(
name|tableName
argument_list|,
name|tableWriter
argument_list|)
expr_stmt|;
block|}
comment|// Write<Row, Cell> into tableWriter
comment|// in the original code, it does not use Row
name|tableWriter
operator|.
name|write
argument_list|(
literal|null
argument_list|,
name|cell
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|(
name|TaskAttemptContext
name|c
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
for|for
control|(
name|RecordWriter
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|V
argument_list|>
name|writer
range|:
name|tableWriters
operator|.
name|values
argument_list|()
control|)
block|{
name|writer
operator|.
name|close
argument_list|(
name|c
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|conn
operator|!=
literal|null
condition|)
block|{
name|conn
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|admin
operator|!=
literal|null
condition|)
block|{
name|admin
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
return|;
block|}
comment|/**    * Configure for one table, should be used before creating a new HFileRecordWriter,    * Set compression algorithms and related configuration based on column families    */
specifier|private
specifier|static
name|void
name|configureForOneTable
parameter_list|(
name|Configuration
name|conf
parameter_list|,
specifier|final
name|HTableDescriptor
name|tableDescriptor
parameter_list|)
throws|throws
name|UnsupportedEncodingException
block|{
name|HFileOutputFormat2
operator|.
name|configureCompression
argument_list|(
name|conf
argument_list|,
name|tableDescriptor
argument_list|)
expr_stmt|;
name|HFileOutputFormat2
operator|.
name|configureBlockSize
argument_list|(
name|tableDescriptor
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|HFileOutputFormat2
operator|.
name|configureBloomType
argument_list|(
name|tableDescriptor
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|HFileOutputFormat2
operator|.
name|configureDataBlockEncoding
argument_list|(
name|tableDescriptor
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
comment|/**    * Configure a MapReduce Job to output HFiles for performing an incremental load into    * the multiple tables.    *<ul>    *<li>Inspects the tables to configure a partitioner based on their region boundaries</li>    *<li>Writes the partitions file and configures the partitioner</li>    *<li>Sets the number of reduce tasks to match the total number of all tables' regions</li>    *<li>Sets the reducer up to perform the appropriate sorting (KeyValueSortReducer)</li>    *</ul>    *    * ConfigureIncrementalLoad has set up partitioner and reducer for mapreduce job.    * Caller needs to setup input path, output path and mapper    *    * @param job    * @param tables A list of tables to inspects    * @throws IOException    */
specifier|public
specifier|static
name|void
name|configureIncrementalLoad
parameter_list|(
name|Job
name|job
parameter_list|,
name|List
argument_list|<
name|TableName
argument_list|>
name|tables
parameter_list|)
throws|throws
name|IOException
block|{
name|configureIncrementalLoad
argument_list|(
name|job
argument_list|,
name|tables
argument_list|,
name|MultiTableHFileOutputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|configureIncrementalLoad
parameter_list|(
name|Job
name|job
parameter_list|,
name|List
argument_list|<
name|TableName
argument_list|>
name|tables
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|OutputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
argument_list|>
name|cls
parameter_list|)
throws|throws
name|IOException
block|{
name|Configuration
name|conf
init|=
name|job
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|tableSplitKeys
init|=
name|MultiHFilePartitioner
operator|.
name|getTablesRegionStartKeys
argument_list|(
name|conf
argument_list|,
name|tables
argument_list|)
decl_stmt|;
name|configureIncrementalLoad
argument_list|(
name|job
argument_list|,
name|tableSplitKeys
argument_list|,
name|cls
argument_list|)
expr_stmt|;
block|}
comment|/**    * Same purpose as configureIncrementalLoad(Job job, List<TableName> tables)    * Used when region startKeys of each table is available, input as<TableName, List<RegionStartKey>>    *    * Caller needs to transfer TableName and byte[] to ImmutableBytesWritable    */
specifier|public
specifier|static
name|void
name|configureIncrementalLoad
parameter_list|(
name|Job
name|job
parameter_list|,
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|tableSplitKeys
parameter_list|)
throws|throws
name|IOException
block|{
name|configureIncrementalLoad
argument_list|(
name|job
argument_list|,
name|tableSplitKeys
argument_list|,
name|MultiTableHFileOutputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|configureIncrementalLoad
parameter_list|(
name|Job
name|job
parameter_list|,
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|tableSplitKeys
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|OutputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
argument_list|>
name|cls
parameter_list|)
throws|throws
name|IOException
block|{
name|Configuration
name|conf
init|=
name|job
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
comment|// file path to store<table, splitKey>
name|String
name|hbaseTmpFsDir
init|=
name|conf
operator|.
name|get
argument_list|(
name|HConstants
operator|.
name|TEMPORARY_FS_DIRECTORY_KEY
argument_list|,
name|HConstants
operator|.
name|DEFAULT_TEMPORARY_HDFS_DIRECTORY
argument_list|)
decl_stmt|;
specifier|final
name|Path
name|partitionsPath
init|=
operator|new
name|Path
argument_list|(
name|hbaseTmpFsDir
argument_list|,
literal|"partitions_"
operator|+
name|UUID
operator|.
name|randomUUID
argument_list|()
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Writing partition info into dir: "
operator|+
name|partitionsPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setPartitionerClass
argument_list|(
name|MultiHFilePartitioner
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// get split keys for all the tables, and write them into partition file
name|MultiHFilePartitioner
operator|.
name|writeTableSplitKeys
argument_list|(
name|conf
argument_list|,
name|partitionsPath
argument_list|,
name|tableSplitKeys
argument_list|)
expr_stmt|;
name|MultiHFilePartitioner
operator|.
name|setPartitionFile
argument_list|(
name|conf
argument_list|,
name|partitionsPath
argument_list|)
expr_stmt|;
name|partitionsPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
operator|.
name|makeQualified
argument_list|(
name|partitionsPath
argument_list|)
expr_stmt|;
name|partitionsPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
operator|.
name|deleteOnExit
argument_list|(
name|partitionsPath
argument_list|)
expr_stmt|;
comment|// now only support Mapper output<ImmutableBytesWritable, KeyValue>
comment|// we can use KeyValueSortReducer directly to sort Mapper output
if|if
condition|(
name|KeyValue
operator|.
name|class
operator|.
name|equals
argument_list|(
name|job
operator|.
name|getMapOutputValueClass
argument_list|()
argument_list|)
condition|)
block|{
name|job
operator|.
name|setReducerClass
argument_list|(
name|KeyValueSortReducer
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unknown map output value type:"
operator|+
name|job
operator|.
name|getMapOutputValueClass
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|int
name|reducerNum
init|=
name|getReducerNumber
argument_list|(
name|tableSplitKeys
argument_list|)
decl_stmt|;
name|job
operator|.
name|setNumReduceTasks
argument_list|(
name|reducerNum
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Configuring "
operator|+
name|reducerNum
operator|+
literal|" reduce partitions "
operator|+
literal|"to match current region count"
argument_list|)
expr_stmt|;
comment|// setup output format
name|job
operator|.
name|setOutputFormatClass
argument_list|(
name|cls
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputKeyClass
argument_list|(
name|ImmutableBytesWritable
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputValueClass
argument_list|(
name|KeyValue
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|setStrings
argument_list|(
literal|"io.serializations"
argument_list|,
name|conf
operator|.
name|get
argument_list|(
literal|"io.serializations"
argument_list|)
argument_list|,
name|MutationSerialization
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|,
name|ResultSerialization
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|,
name|KeyValueSerialization
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|TableMapReduceUtil
operator|.
name|addDependencyJars
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|TableMapReduceUtil
operator|.
name|initCredentials
argument_list|(
name|job
argument_list|)
expr_stmt|;
block|}
comment|/**    * Check if table exist, should not dependent on HBase instance    * @return instance of table, if it exist    */
specifier|private
specifier|static
name|Table
name|getTable
parameter_list|(
specifier|final
name|byte
index|[]
name|tableName
parameter_list|,
name|Connection
name|conn
parameter_list|,
name|Admin
name|admin
parameter_list|)
block|{
if|if
condition|(
name|conn
operator|==
literal|null
operator|||
name|admin
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"can not get Connection or Admin"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
try|try
block|{
name|TableName
name|table
init|=
name|TableName
operator|.
name|valueOf
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
if|if
condition|(
name|admin
operator|.
name|tableExists
argument_list|(
name|table
argument_list|)
condition|)
block|{
return|return
name|conn
operator|.
name|getTable
argument_list|(
name|table
argument_list|)
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Exception found in getTable()"
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|LOG
operator|.
name|warn
argument_list|(
literal|"Table: '"
operator|+
name|TableName
operator|.
name|valueOf
argument_list|(
name|tableName
argument_list|)
operator|+
literal|"' does not exist"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|/**    * Get the number of reducers by tables' split keys    */
specifier|private
specifier|static
name|int
name|getReducerNumber
parameter_list|(
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|tableSplitKeys
parameter_list|)
block|{
name|int
name|reducerNum
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|entry
range|:
name|tableSplitKeys
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|reducerNum
operator|+=
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|size
argument_list|()
expr_stmt|;
block|}
return|return
name|reducerNum
return|;
block|}
comment|/**    * MultiTableHFileOutputFormat writes files based on partitions created by MultiHFilePartitioner    * The input is partitioned based on table's name and its region boundaries with the table.    * Two records are in the same partition if they have same table name and the their cells are    * in the same region    */
specifier|static
class|class
name|MultiHFilePartitioner
extends|extends
name|Partitioner
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|Cell
argument_list|>
implements|implements
name|Configurable
block|{
specifier|public
specifier|static
specifier|final
name|String
name|DEFAULT_PATH
init|=
literal|"_partition_multihfile.lst"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|PARTITIONER_PATH
init|=
literal|"mapreduce.multihfile.partitioner.path"
decl_stmt|;
specifier|private
name|Configuration
name|conf
decl_stmt|;
comment|// map to receive<table, splitKeys> from file
specifier|private
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|table_SplitKeys
decl_stmt|;
comment|// each<table,splitKey> pair is map to one unique integer
specifier|private
name|TreeMap
argument_list|<
name|TableSplitKeyPair
argument_list|,
name|Integer
argument_list|>
name|partitionMap
decl_stmt|;
annotation|@
name|Override
specifier|public
name|void
name|setConf
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
try|try
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|partitionMap
operator|=
operator|new
name|TreeMap
argument_list|<>
argument_list|()
expr_stmt|;
name|table_SplitKeys
operator|=
name|readTableSplitKeys
argument_list|(
name|conf
argument_list|)
expr_stmt|;
comment|// initiate partitionMap by table_SplitKeys map
name|int
name|splitNum
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|entry
range|:
name|table_SplitKeys
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|ImmutableBytesWritable
name|table
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
name|list
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
for|for
control|(
name|ImmutableBytesWritable
name|splitKey
range|:
name|list
control|)
block|{
name|partitionMap
operator|.
name|put
argument_list|(
operator|new
name|TableSplitKeyPair
argument_list|(
name|table
argument_list|,
name|splitKey
argument_list|)
argument_list|,
name|splitNum
operator|++
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Can't read partitions file"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|Configuration
name|getConf
parameter_list|()
block|{
return|return
name|conf
return|;
block|}
comment|/**      * Set the path to the SequenceFile storing the sorted<table, splitkey>. It must be the case      * that for<tt>R</tt> reduces, there are<tt>R-1</tt> keys in the SequenceFile.      */
specifier|public
specifier|static
name|void
name|setPartitionFile
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|Path
name|p
parameter_list|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|PARTITIONER_PATH
argument_list|,
name|p
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**      * Get the path to the SequenceFile storing the sorted<table, splitkey>.      * @see #setPartitionFile(Configuration, Path)      */
specifier|public
specifier|static
name|String
name|getPartitionFile
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|conf
operator|.
name|get
argument_list|(
name|PARTITIONER_PATH
argument_list|,
name|DEFAULT_PATH
argument_list|)
return|;
block|}
comment|/**      * Return map of<tableName, the start keys of all of the regions in this table>      */
specifier|public
specifier|static
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|getTablesRegionStartKeys
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|List
argument_list|<
name|TableName
argument_list|>
name|tables
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|TreeMap
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|ret
init|=
operator|new
name|TreeMap
argument_list|<>
argument_list|()
decl_stmt|;
try|try
init|(
name|Connection
name|conn
init|=
name|ConnectionFactory
operator|.
name|createConnection
argument_list|(
name|conf
argument_list|)
init|;
name|Admin
name|admin
operator|=
name|conn
operator|.
name|getAdmin
argument_list|()
init|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Looking up current regions for tables"
argument_list|)
expr_stmt|;
for|for
control|(
name|TableName
name|tName
range|:
name|tables
control|)
block|{
name|RegionLocator
name|table
init|=
name|conn
operator|.
name|getRegionLocator
argument_list|(
name|tName
argument_list|)
decl_stmt|;
comment|// if table not exist, use default split keys for this table
name|byte
index|[]
index|[]
name|byteKeys
init|=
block|{
name|HConstants
operator|.
name|EMPTY_BYTE_ARRAY
block|}
decl_stmt|;
if|if
condition|(
name|admin
operator|.
name|tableExists
argument_list|(
name|tName
argument_list|)
condition|)
block|{
name|byteKeys
operator|=
name|table
operator|.
name|getStartKeys
argument_list|()
expr_stmt|;
block|}
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
name|tableStartKeys
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|byteKeys
operator|.
name|length
argument_list|)
decl_stmt|;
for|for
control|(
name|byte
index|[]
name|byteKey
range|:
name|byteKeys
control|)
block|{
name|tableStartKeys
operator|.
name|add
argument_list|(
operator|new
name|ImmutableBytesWritable
argument_list|(
name|byteKey
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|ret
operator|.
name|put
argument_list|(
operator|new
name|ImmutableBytesWritable
argument_list|(
name|tName
operator|.
name|toBytes
argument_list|()
argument_list|)
argument_list|,
name|tableStartKeys
argument_list|)
expr_stmt|;
block|}
return|return
name|ret
return|;
block|}
block|}
comment|/**      * write<tableName, start key of each region in table> into sequence file in order,      * and this format can be parsed by MultiHFilePartitioner      */
specifier|public
specifier|static
name|void
name|writeTableSplitKeys
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|Path
name|partitionsPath
parameter_list|,
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|map
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Writing partition information to "
operator|+
name|partitionsPath
argument_list|)
expr_stmt|;
if|if
condition|(
name|map
operator|==
literal|null
operator|||
name|map
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"No regions passed for all tables"
argument_list|)
throw|;
block|}
name|SequenceFile
operator|.
name|Writer
name|writer
init|=
name|SequenceFile
operator|.
name|createWriter
argument_list|(
name|conf
argument_list|,
name|Writer
operator|.
name|file
argument_list|(
name|partitionsPath
argument_list|)
argument_list|,
name|Writer
operator|.
name|keyClass
argument_list|(
name|ImmutableBytesWritable
operator|.
name|class
argument_list|)
argument_list|,
name|Writer
operator|.
name|valueClass
argument_list|(
name|ImmutableBytesWritable
operator|.
name|class
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|entry
range|:
name|map
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|ImmutableBytesWritable
name|table
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
name|list
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|list
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Split keys for a table can not be null"
argument_list|)
throw|;
block|}
name|TreeSet
argument_list|<
name|ImmutableBytesWritable
argument_list|>
name|sorted
init|=
operator|new
name|TreeSet
argument_list|<>
argument_list|(
name|list
argument_list|)
decl_stmt|;
name|ImmutableBytesWritable
name|first
init|=
name|sorted
operator|.
name|first
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|first
operator|.
name|equals
argument_list|(
name|HConstants
operator|.
name|EMPTY_BYTE_ARRAY
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"First region of table should have empty start key. Instead has: "
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|first
operator|.
name|get
argument_list|()
argument_list|)
argument_list|)
throw|;
block|}
for|for
control|(
name|ImmutableBytesWritable
name|startKey
range|:
name|sorted
control|)
block|{
name|writer
operator|.
name|append
argument_list|(
name|table
argument_list|,
name|startKey
argument_list|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**      * read partition file into map<table, splitKeys of this table>      */
specifier|private
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|readTableSplitKeys
parameter_list|(
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|parts
init|=
name|getPartitionFile
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Read partition info from file: "
operator|+
name|parts
argument_list|)
expr_stmt|;
specifier|final
name|Path
name|partFile
init|=
operator|new
name|Path
argument_list|(
name|parts
argument_list|)
decl_stmt|;
name|SequenceFile
operator|.
name|Reader
name|reader
init|=
operator|new
name|SequenceFile
operator|.
name|Reader
argument_list|(
name|conf
argument_list|,
name|Reader
operator|.
name|file
argument_list|(
name|partFile
argument_list|)
argument_list|)
decl_stmt|;
comment|// values are already sorted in file, so use list
specifier|final
name|Map
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
argument_list|>
name|map
init|=
operator|new
name|TreeMap
argument_list|<>
argument_list|()
decl_stmt|;
comment|// key and value have same type
name|ImmutableBytesWritable
name|key
init|=
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|ImmutableBytesWritable
operator|.
name|class
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|ImmutableBytesWritable
name|value
init|=
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|ImmutableBytesWritable
operator|.
name|class
argument_list|,
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
while|while
condition|(
name|reader
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
condition|)
block|{
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
name|list
init|=
name|map
operator|.
name|get
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|list
operator|==
literal|null
condition|)
block|{
name|list
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
expr_stmt|;
block|}
name|list
operator|.
name|add
argument_list|(
name|value
argument_list|)
expr_stmt|;
name|map
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|list
argument_list|)
expr_stmt|;
name|key
operator|=
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|ImmutableBytesWritable
operator|.
name|class
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|value
operator|=
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|ImmutableBytesWritable
operator|.
name|class
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|cleanup
argument_list|(
name|LOG
argument_list|,
name|reader
argument_list|)
expr_stmt|;
block|}
return|return
name|map
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|getPartition
parameter_list|(
name|ImmutableBytesWritable
name|table
parameter_list|,
name|Cell
name|value
parameter_list|,
name|int
name|numPartitions
parameter_list|)
block|{
name|byte
index|[]
name|row
init|=
name|CellUtil
operator|.
name|cloneRow
argument_list|(
name|value
argument_list|)
decl_stmt|;
specifier|final
name|ImmutableBytesWritable
name|rowKey
init|=
operator|new
name|ImmutableBytesWritable
argument_list|(
name|row
argument_list|)
decl_stmt|;
name|ImmutableBytesWritable
name|splitId
init|=
operator|new
name|ImmutableBytesWritable
argument_list|(
name|HConstants
operator|.
name|EMPTY_BYTE_ARRAY
argument_list|)
decl_stmt|;
comment|//find splitKey by input rowKey
if|if
condition|(
name|table_SplitKeys
operator|.
name|containsKey
argument_list|(
name|table
argument_list|)
condition|)
block|{
name|List
argument_list|<
name|ImmutableBytesWritable
argument_list|>
name|list
init|=
name|table_SplitKeys
operator|.
name|get
argument_list|(
name|table
argument_list|)
decl_stmt|;
name|int
name|index
init|=
name|Collections
operator|.
name|binarySearch
argument_list|(
name|list
argument_list|,
name|rowKey
argument_list|,
operator|new
name|ImmutableBytesWritable
operator|.
name|Comparator
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|index
operator|<
literal|0
condition|)
block|{
name|index
operator|=
operator|(
name|index
operator|+
literal|1
operator|)
operator|*
operator|(
operator|-
literal|1
operator|)
operator|-
literal|1
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|index
operator|==
name|list
operator|.
name|size
argument_list|()
condition|)
block|{
name|index
operator|-=
literal|1
expr_stmt|;
block|}
if|if
condition|(
name|index
operator|<
literal|0
condition|)
block|{
name|index
operator|=
literal|0
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"row key can not less than HConstants.EMPTY_BYTE_ARRAY "
argument_list|)
expr_stmt|;
block|}
name|splitId
operator|=
name|list
operator|.
name|get
argument_list|(
name|index
argument_list|)
expr_stmt|;
block|}
comment|// find the id of the reducer for the input
name|Integer
name|id
init|=
name|partitionMap
operator|.
name|get
argument_list|(
operator|new
name|TableSplitKeyPair
argument_list|(
name|table
argument_list|,
name|splitId
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|id
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Can not get reducer id for input record"
argument_list|)
expr_stmt|;
return|return
operator|-
literal|1
return|;
block|}
return|return
name|id
operator|.
name|intValue
argument_list|()
operator|%
name|numPartitions
return|;
block|}
comment|/**      * A class store pair<TableName, SplitKey>, has two main usage      * 1. store tableName and one of its splitKey as a pair      * 2. implement comparable, so that partitioner can find splitKey of its input cell      */
specifier|static
class|class
name|TableSplitKeyPair
extends|extends
name|Pair
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|ImmutableBytesWritable
argument_list|>
implements|implements
name|Comparable
argument_list|<
name|TableSplitKeyPair
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
operator|-
literal|6485999667666325594L
decl_stmt|;
specifier|public
name|TableSplitKeyPair
parameter_list|(
name|ImmutableBytesWritable
name|a
parameter_list|,
name|ImmutableBytesWritable
name|b
parameter_list|)
block|{
name|super
argument_list|(
name|a
argument_list|,
name|b
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|TableSplitKeyPair
name|other
parameter_list|)
block|{
if|if
condition|(
name|this
operator|.
name|getFirst
argument_list|()
operator|.
name|equals
argument_list|(
name|other
operator|.
name|getFirst
argument_list|()
argument_list|)
condition|)
block|{
return|return
name|this
operator|.
name|getSecond
argument_list|()
operator|.
name|compareTo
argument_list|(
name|other
operator|.
name|getSecond
argument_list|()
argument_list|)
return|;
block|}
return|return
name|this
operator|.
name|getFirst
argument_list|()
operator|.
name|compareTo
argument_list|(
name|other
operator|.
name|getFirst
argument_list|()
argument_list|)
return|;
block|}
block|}
block|}
block|}
end_class

end_unit

