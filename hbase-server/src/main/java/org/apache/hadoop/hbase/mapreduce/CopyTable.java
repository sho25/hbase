begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|mapreduce
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configured
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HBaseConfiguration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|TableName
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|classification
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|classification
operator|.
name|InterfaceStability
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Admin
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|ConnectionFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Scan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Job
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Tool
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ToolRunner
import|;
end_import

begin_comment
comment|/**  * Tool used to copy a table to another one which can be on a different setup.  * It is also configurable with a start and time as well as a specification  * of the region server implementation if different from the local cluster.  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Public
annotation|@
name|InterfaceStability
operator|.
name|Stable
specifier|public
class|class
name|CopyTable
extends|extends
name|Configured
implements|implements
name|Tool
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|CopyTable
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|final
specifier|static
name|String
name|NAME
init|=
literal|"copytable"
decl_stmt|;
name|long
name|startTime
init|=
literal|0
decl_stmt|;
name|long
name|endTime
init|=
name|HConstants
operator|.
name|LATEST_TIMESTAMP
decl_stmt|;
name|int
name|batch
init|=
name|Integer
operator|.
name|MAX_VALUE
decl_stmt|;
name|int
name|cacheRow
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|versions
init|=
operator|-
literal|1
decl_stmt|;
name|String
name|tableName
init|=
literal|null
decl_stmt|;
name|String
name|startRow
init|=
literal|null
decl_stmt|;
name|String
name|stopRow
init|=
literal|null
decl_stmt|;
name|String
name|dstTableName
init|=
literal|null
decl_stmt|;
name|String
name|peerAddress
init|=
literal|null
decl_stmt|;
name|String
name|families
init|=
literal|null
decl_stmt|;
name|boolean
name|allCells
init|=
literal|false
decl_stmt|;
specifier|static
name|boolean
name|shuffle
init|=
literal|false
decl_stmt|;
name|boolean
name|bulkload
init|=
literal|false
decl_stmt|;
name|Path
name|bulkloadDir
init|=
literal|null
decl_stmt|;
specifier|private
specifier|final
specifier|static
name|String
name|JOB_NAME_CONF_KEY
init|=
literal|"mapreduce.job.name"
decl_stmt|;
comment|/**    * Sets up the actual job.    *    * @param args  The command line parameters.    * @return The newly created job.    * @throws IOException When setting up the job fails.    */
specifier|public
name|Job
name|createSubmittableJob
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|doCommandLine
argument_list|(
name|args
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Job
name|job
init|=
name|Job
operator|.
name|getInstance
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|getConf
argument_list|()
operator|.
name|get
argument_list|(
name|JOB_NAME_CONF_KEY
argument_list|,
name|NAME
operator|+
literal|"_"
operator|+
name|tableName
argument_list|)
argument_list|)
decl_stmt|;
name|job
operator|.
name|setJarByClass
argument_list|(
name|CopyTable
operator|.
name|class
argument_list|)
expr_stmt|;
name|Scan
name|scan
init|=
operator|new
name|Scan
argument_list|()
decl_stmt|;
name|scan
operator|.
name|setBatch
argument_list|(
name|batch
argument_list|)
expr_stmt|;
name|scan
operator|.
name|setCacheBlocks
argument_list|(
literal|false
argument_list|)
expr_stmt|;
if|if
condition|(
name|cacheRow
operator|>
literal|0
condition|)
block|{
name|scan
operator|.
name|setCaching
argument_list|(
name|cacheRow
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|scan
operator|.
name|setCaching
argument_list|(
name|getConf
argument_list|()
operator|.
name|getInt
argument_list|(
name|HConstants
operator|.
name|HBASE_CLIENT_SCANNER_CACHING
argument_list|,
literal|100
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|scan
operator|.
name|setTimeRange
argument_list|(
name|startTime
argument_list|,
name|endTime
argument_list|)
expr_stmt|;
if|if
condition|(
name|allCells
condition|)
block|{
name|scan
operator|.
name|setRaw
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|shuffle
condition|)
block|{
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|TableInputFormat
operator|.
name|SHUFFLE_MAPS
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|versions
operator|>=
literal|0
condition|)
block|{
name|scan
operator|.
name|setMaxVersions
argument_list|(
name|versions
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|startRow
operator|!=
literal|null
condition|)
block|{
name|scan
operator|.
name|setStartRow
argument_list|(
name|Bytes
operator|.
name|toBytesBinary
argument_list|(
name|startRow
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|stopRow
operator|!=
literal|null
condition|)
block|{
name|scan
operator|.
name|setStopRow
argument_list|(
name|Bytes
operator|.
name|toBytesBinary
argument_list|(
name|stopRow
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|families
operator|!=
literal|null
condition|)
block|{
name|String
index|[]
name|fams
init|=
name|families
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|cfRenameMap
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|fam
range|:
name|fams
control|)
block|{
name|String
name|sourceCf
decl_stmt|;
if|if
condition|(
name|fam
operator|.
name|contains
argument_list|(
literal|":"
argument_list|)
condition|)
block|{
comment|// fam looks like "sourceCfName:destCfName"
name|String
index|[]
name|srcAndDest
init|=
name|fam
operator|.
name|split
argument_list|(
literal|":"
argument_list|,
literal|2
argument_list|)
decl_stmt|;
name|sourceCf
operator|=
name|srcAndDest
index|[
literal|0
index|]
expr_stmt|;
name|String
name|destCf
init|=
name|srcAndDest
index|[
literal|1
index|]
decl_stmt|;
name|cfRenameMap
operator|.
name|put
argument_list|(
name|sourceCf
argument_list|,
name|destCf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// fam is just "sourceCf"
name|sourceCf
operator|=
name|fam
expr_stmt|;
block|}
name|scan
operator|.
name|addFamily
argument_list|(
name|Bytes
operator|.
name|toBytes
argument_list|(
name|sourceCf
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|Import
operator|.
name|configureCfRenaming
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|,
name|cfRenameMap
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|setNumReduceTasks
argument_list|(
literal|0
argument_list|)
expr_stmt|;
if|if
condition|(
name|bulkload
condition|)
block|{
name|TableMapReduceUtil
operator|.
name|initTableMapperJob
argument_list|(
name|tableName
argument_list|,
name|scan
argument_list|,
name|Import
operator|.
name|KeyValueImporter
operator|.
name|class
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
name|job
argument_list|)
expr_stmt|;
comment|// We need to split the inputs by destination tables so that output of Map can be bulk-loaded.
name|TableInputFormat
operator|.
name|configureSplitTable
argument_list|(
name|job
argument_list|,
name|TableName
operator|.
name|valueOf
argument_list|(
name|dstTableName
argument_list|)
argument_list|)
expr_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
name|Random
name|rand
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
name|Path
name|root
init|=
operator|new
name|Path
argument_list|(
name|fs
operator|.
name|getWorkingDirectory
argument_list|()
argument_list|,
literal|"copytable"
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|root
argument_list|)
expr_stmt|;
while|while
condition|(
literal|true
condition|)
block|{
name|bulkloadDir
operator|=
operator|new
name|Path
argument_list|(
name|root
argument_list|,
literal|""
operator|+
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|bulkloadDir
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"HFiles will be stored at "
operator|+
name|this
operator|.
name|bulkloadDir
argument_list|)
expr_stmt|;
name|HFileOutputFormat2
operator|.
name|setOutputPath
argument_list|(
name|job
argument_list|,
name|bulkloadDir
argument_list|)
expr_stmt|;
try|try
init|(
name|Connection
name|conn
init|=
name|ConnectionFactory
operator|.
name|createConnection
argument_list|(
name|getConf
argument_list|()
argument_list|)
init|;           Admin admin = conn.getAdmin()
block|)
block|{
name|HFileOutputFormat2
operator|.
name|configureIncrementalLoadMap
argument_list|(
name|job
argument_list|,
name|admin
operator|.
name|getTableDescriptor
argument_list|(
operator|(
name|TableName
operator|.
name|valueOf
argument_list|(
name|dstTableName
argument_list|)
operator|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|TableMapReduceUtil
operator|.
name|initTableMapperJob
argument_list|(
name|tableName
argument_list|,
name|scan
argument_list|,
name|Import
operator|.
name|Importer
operator|.
name|class
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
name|job
argument_list|)
expr_stmt|;
name|TableMapReduceUtil
operator|.
name|initTableReducerJob
argument_list|(
name|dstTableName
argument_list|,
literal|null
argument_list|,
name|job
argument_list|,
literal|null
argument_list|,
name|peerAddress
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
return|return
name|job
return|;
block|}
end_class

begin_comment
comment|/*    * @param errorMsg Error message.  Can be null.    */
end_comment

begin_function
specifier|private
specifier|static
name|void
name|printUsage
parameter_list|(
specifier|final
name|String
name|errorMsg
parameter_list|)
block|{
if|if
condition|(
name|errorMsg
operator|!=
literal|null
operator|&&
name|errorMsg
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"ERROR: "
operator|+
name|errorMsg
argument_list|)
expr_stmt|;
block|}
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] "
operator|+
literal|"[--new.name=NEW] [--peer.adr=ADR]<tablename>"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|()
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Options:"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" rs.class     hbase.regionserver.class of the peer cluster"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"              specify if different from current cluster"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" rs.impl      hbase.regionserver.impl of the peer cluster"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" startrow     the start row"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" stoprow      the stop row"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" starttime    beginning of the time range (unixtime in millis)"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"              without endtime means from starttime to forever"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" endtime      end of the time range.  Ignored if no starttime specified."
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" versions     number of cell versions to copy"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" new.name     new table's name"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" peer.adr     Address of the peer cluster given in the format"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"              hbase.zookeeper.quorum:hbase.zookeeper.client"
operator|+
literal|".port:zookeeper.znode.parent"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" families     comma-separated list of families to copy"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"              To copy from cf1 to cf2, give sourceCfName:destCfName. "
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"              To keep the same name, just give \"cfName\""
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" all.cells    also copy delete markers and deleted cells"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" bulkload     Write input into HFiles and bulk load to the destination "
operator|+
literal|"table"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|()
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Args:"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" tablename    Name of the table to copy"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|()
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Examples:"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" To copy 'TestTable' to a cluster that uses replication for a 1 hour window:"
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|" $ hbase "
operator|+
literal|"org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 "
operator|+
literal|"--peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable "
argument_list|)
expr_stmt|;
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"For performance consider the following general option:\n"
operator|+
literal|"  It is recommended that you set the following to>=100. A higher value uses more memory but\n"
operator|+
literal|"  decreases the round trip time to the server and may increase performance.\n"
operator|+
literal|"    -Dhbase.client.scanner.caching=100\n"
operator|+
literal|"  The following should always be set to false, to prevent writing data twice, which may produce \n"
operator|+
literal|"  inaccurate results.\n"
operator|+
literal|"    -Dmapreduce.map.speculative=false"
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
specifier|private
name|boolean
name|doCommandLine
parameter_list|(
specifier|final
name|String
index|[]
name|args
parameter_list|)
block|{
comment|// Process command-line args. TODO: Better cmd-line processing
comment|// (but hopefully something not as painful as cli options).
if|if
condition|(
name|args
operator|.
name|length
operator|<
literal|1
condition|)
block|{
name|printUsage
argument_list|(
literal|null
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|args
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|String
name|cmd
init|=
name|args
index|[
name|i
index|]
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|equals
argument_list|(
literal|"-h"
argument_list|)
operator|||
name|cmd
operator|.
name|startsWith
argument_list|(
literal|"--h"
argument_list|)
condition|)
block|{
name|printUsage
argument_list|(
literal|null
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
specifier|final
name|String
name|startRowArgKey
init|=
literal|"--startrow="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|startRowArgKey
argument_list|)
condition|)
block|{
name|startRow
operator|=
name|cmd
operator|.
name|substring
argument_list|(
name|startRowArgKey
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|stopRowArgKey
init|=
literal|"--stoprow="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|stopRowArgKey
argument_list|)
condition|)
block|{
name|stopRow
operator|=
name|cmd
operator|.
name|substring
argument_list|(
name|stopRowArgKey
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|startTimeArgKey
init|=
literal|"--starttime="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|startTimeArgKey
argument_list|)
condition|)
block|{
name|startTime
operator|=
name|Long
operator|.
name|parseLong
argument_list|(
name|cmd
operator|.
name|substring
argument_list|(
name|startTimeArgKey
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|endTimeArgKey
init|=
literal|"--endtime="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|endTimeArgKey
argument_list|)
condition|)
block|{
name|endTime
operator|=
name|Long
operator|.
name|parseLong
argument_list|(
name|cmd
operator|.
name|substring
argument_list|(
name|endTimeArgKey
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|batchArgKey
init|=
literal|"--batch="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|batchArgKey
argument_list|)
condition|)
block|{
name|batch
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|cmd
operator|.
name|substring
argument_list|(
name|batchArgKey
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|cacheRowArgKey
init|=
literal|"--cacheRow="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|cacheRowArgKey
argument_list|)
condition|)
block|{
name|cacheRow
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|cmd
operator|.
name|substring
argument_list|(
name|cacheRowArgKey
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|versionsArgKey
init|=
literal|"--versions="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|versionsArgKey
argument_list|)
condition|)
block|{
name|versions
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|cmd
operator|.
name|substring
argument_list|(
name|versionsArgKey
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|newNameArgKey
init|=
literal|"--new.name="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|newNameArgKey
argument_list|)
condition|)
block|{
name|dstTableName
operator|=
name|cmd
operator|.
name|substring
argument_list|(
name|newNameArgKey
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|peerAdrArgKey
init|=
literal|"--peer.adr="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|peerAdrArgKey
argument_list|)
condition|)
block|{
name|peerAddress
operator|=
name|cmd
operator|.
name|substring
argument_list|(
name|peerAdrArgKey
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
continue|continue;
block|}
specifier|final
name|String
name|familiesArgKey
init|=
literal|"--families="
decl_stmt|;
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
name|familiesArgKey
argument_list|)
condition|)
block|{
name|families
operator|=
name|cmd
operator|.
name|substring
argument_list|(
name|familiesArgKey
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
literal|"--all.cells"
argument_list|)
condition|)
block|{
name|allCells
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
literal|"--bulkload"
argument_list|)
condition|)
block|{
name|bulkload
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|cmd
operator|.
name|startsWith
argument_list|(
literal|"--shuffle"
argument_list|)
condition|)
block|{
name|shuffle
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|i
operator|==
name|args
operator|.
name|length
operator|-
literal|1
condition|)
block|{
name|tableName
operator|=
name|cmd
expr_stmt|;
block|}
else|else
block|{
name|printUsage
argument_list|(
literal|"Invalid argument '"
operator|+
name|cmd
operator|+
literal|"'"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
block|}
if|if
condition|(
name|dstTableName
operator|==
literal|null
operator|&&
name|peerAddress
operator|==
literal|null
condition|)
block|{
name|printUsage
argument_list|(
literal|"At least a new table name or a "
operator|+
literal|"peer address must be specified"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
if|if
condition|(
operator|(
name|endTime
operator|!=
literal|0
operator|)
operator|&&
operator|(
name|startTime
operator|>
name|endTime
operator|)
condition|)
block|{
name|printUsage
argument_list|(
literal|"Invalid time range filter: starttime="
operator|+
name|startTime
operator|+
literal|">  endtime="
operator|+
name|endTime
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
if|if
condition|(
name|bulkload
operator|&&
name|peerAddress
operator|!=
literal|null
condition|)
block|{
name|printUsage
argument_list|(
literal|"Remote bulkload is not supported!"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
comment|// set dstTableName if necessary
if|if
condition|(
name|dstTableName
operator|==
literal|null
condition|)
block|{
name|dstTableName
operator|=
name|tableName
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
name|printUsage
argument_list|(
literal|"Can't start because "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
return|return
literal|true
return|;
block|}
end_function

begin_comment
comment|/**    * Main entry point.    *    * @param args  The command line parameters.    * @throws Exception When running the job fails.    */
end_comment

begin_function
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|int
name|ret
init|=
name|ToolRunner
operator|.
name|run
argument_list|(
name|HBaseConfiguration
operator|.
name|create
argument_list|()
argument_list|,
operator|new
name|CopyTable
argument_list|()
argument_list|,
name|args
argument_list|)
decl_stmt|;
name|System
operator|.
name|exit
argument_list|(
name|ret
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
annotation|@
name|Override
specifier|public
name|int
name|run
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|Job
name|job
init|=
name|createSubmittableJob
argument_list|(
name|args
argument_list|)
decl_stmt|;
if|if
condition|(
name|job
operator|==
literal|null
condition|)
return|return
literal|1
return|;
if|if
condition|(
operator|!
name|job
operator|.
name|waitForCompletion
argument_list|(
literal|true
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Map-reduce job failed!"
argument_list|)
expr_stmt|;
if|if
condition|(
name|bulkload
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Files are not bulkloaded!"
argument_list|)
expr_stmt|;
block|}
return|return
literal|1
return|;
block|}
name|int
name|code
init|=
literal|0
decl_stmt|;
if|if
condition|(
name|bulkload
condition|)
block|{
name|code
operator|=
operator|new
name|LoadIncrementalHFiles
argument_list|(
name|this
operator|.
name|getConf
argument_list|()
argument_list|)
operator|.
name|run
argument_list|(
operator|new
name|String
index|[]
block|{
name|this
operator|.
name|bulkloadDir
operator|.
name|toString
argument_list|()
block|,
name|this
operator|.
name|dstTableName
block|}
argument_list|)
expr_stmt|;
if|if
condition|(
name|code
operator|==
literal|0
condition|)
block|{
comment|// bulkloadDir is deleted only LoadIncrementalHFiles was successful so that one can rerun
comment|// LoadIncrementalHFiles.
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|this
operator|.
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|this
operator|.
name|bulkloadDir
argument_list|,
literal|true
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Deleting folder "
operator|+
name|bulkloadDir
operator|+
literal|" failed!"
argument_list|)
expr_stmt|;
name|code
operator|=
literal|1
expr_stmt|;
block|}
block|}
block|}
return|return
name|code
return|;
block|}
end_function

unit|}
end_unit

