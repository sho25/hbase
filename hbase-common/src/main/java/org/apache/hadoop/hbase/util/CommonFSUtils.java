begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|reflect
operator|.
name|InvocationTargetException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|reflect
operator|.
name|Method
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URISyntaxException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Locale
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentHashMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|HadoopIllegalArgumentException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|LocatedFileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|RemoteIterator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|permission
operator|.
name|FsPermission
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|TableName
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|shaded
operator|.
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|shaded
operator|.
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|ipc
operator|.
name|RemoteException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|yetus
operator|.
name|audience
operator|.
name|InterfaceAudience
import|;
end_import

begin_comment
comment|/**  * Utility methods for interacting with the underlying file system.  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Private
specifier|public
specifier|abstract
class|class
name|CommonFSUtils
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|CommonFSUtils
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/** Parameter name for HBase WAL directory */
specifier|public
specifier|static
specifier|final
name|String
name|HBASE_WAL_DIR
init|=
literal|"hbase.wal.dir"
decl_stmt|;
comment|/** Parameter to disable stream capability enforcement checks */
specifier|public
specifier|static
specifier|final
name|String
name|UNSAFE_STREAM_CAPABILITY_ENFORCE
init|=
literal|"hbase.unsafe.stream.capability.enforce"
decl_stmt|;
comment|/** Full access permissions (starting point for a umask) */
specifier|public
specifier|static
specifier|final
name|String
name|FULL_RWX_PERMISSIONS
init|=
literal|"777"
decl_stmt|;
specifier|protected
name|CommonFSUtils
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
block|}
comment|/**    * Compare of path component. Does not consider schema; i.e. if schemas    * different but<code>path</code> starts with<code>rootPath</code>,    * then the function returns true    * @param rootPath value to check for    * @param path subject to check    * @return True if<code>path</code> starts with<code>rootPath</code>    */
specifier|public
specifier|static
name|boolean
name|isStartingWithPath
parameter_list|(
specifier|final
name|Path
name|rootPath
parameter_list|,
specifier|final
name|String
name|path
parameter_list|)
block|{
name|String
name|uriRootPath
init|=
name|rootPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|tailUriPath
init|=
operator|(
operator|new
name|Path
argument_list|(
name|path
argument_list|)
operator|)
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
return|return
name|tailUriPath
operator|.
name|startsWith
argument_list|(
name|uriRootPath
argument_list|)
return|;
block|}
comment|/**    * Compare path component of the Path URI; e.g. if hdfs://a/b/c and /a/b/c, it will compare the    * '/a/b/c' part. Does not consider schema; i.e. if schemas different but path or subpath matches,    * the two will equate.    * @param pathToSearch Path we will be trying to match against.    * @param pathTail what to match    * @return True if<code>pathTail</code> is tail on the path of<code>pathToSearch</code>    */
specifier|public
specifier|static
name|boolean
name|isMatchingTail
parameter_list|(
specifier|final
name|Path
name|pathToSearch
parameter_list|,
name|String
name|pathTail
parameter_list|)
block|{
return|return
name|isMatchingTail
argument_list|(
name|pathToSearch
argument_list|,
operator|new
name|Path
argument_list|(
name|pathTail
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Compare path component of the Path URI; e.g. if hdfs://a/b/c and /a/b/c, it will compare the    * '/a/b/c' part. If you passed in 'hdfs://a/b/c and b/c, it would return true.  Does not consider    * schema; i.e. if schemas different but path or subpath matches, the two will equate.    * @param pathToSearch Path we will be trying to match agains against    * @param pathTail what to match    * @return True if<code>pathTail</code> is tail on the path of<code>pathToSearch</code>    */
specifier|public
specifier|static
name|boolean
name|isMatchingTail
parameter_list|(
specifier|final
name|Path
name|pathToSearch
parameter_list|,
specifier|final
name|Path
name|pathTail
parameter_list|)
block|{
if|if
condition|(
name|pathToSearch
operator|.
name|depth
argument_list|()
operator|!=
name|pathTail
operator|.
name|depth
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
name|Path
name|tailPath
init|=
name|pathTail
decl_stmt|;
name|String
name|tailName
decl_stmt|;
name|Path
name|toSearch
init|=
name|pathToSearch
decl_stmt|;
name|String
name|toSearchName
decl_stmt|;
name|boolean
name|result
init|=
literal|false
decl_stmt|;
do|do
block|{
name|tailName
operator|=
name|tailPath
operator|.
name|getName
argument_list|()
expr_stmt|;
if|if
condition|(
name|tailName
operator|==
literal|null
operator|||
name|tailName
operator|.
name|length
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|result
operator|=
literal|true
expr_stmt|;
break|break;
block|}
name|toSearchName
operator|=
name|toSearch
operator|.
name|getName
argument_list|()
expr_stmt|;
if|if
condition|(
name|toSearchName
operator|==
literal|null
operator|||
name|toSearchName
operator|.
name|length
argument_list|()
operator|<=
literal|0
condition|)
block|{
break|break;
block|}
comment|// Move up a parent on each path for next go around.  Path doesn't let us go off the end.
name|tailPath
operator|=
name|tailPath
operator|.
name|getParent
argument_list|()
expr_stmt|;
name|toSearch
operator|=
name|toSearch
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
do|while
condition|(
name|tailName
operator|.
name|equals
argument_list|(
name|toSearchName
argument_list|)
condition|)
do|;
return|return
name|result
return|;
block|}
comment|/**    * Delete if exists.    * @param fs filesystem object    * @param dir directory to delete    * @return True if deleted<code>dir</code>    * @throws IOException e    */
specifier|public
specifier|static
name|boolean
name|deleteDirectory
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|fs
operator|.
name|exists
argument_list|(
name|dir
argument_list|)
operator|&&
name|fs
operator|.
name|delete
argument_list|(
name|dir
argument_list|,
literal|true
argument_list|)
return|;
block|}
comment|/**    * Return the number of bytes that large input files should be optimally    * be split into to minimize i/o time.    *    * use reflection to search for getDefaultBlockSize(Path f)    * if the method doesn't exist, fall back to using getDefaultBlockSize()    *    * @param fs filesystem object    * @return the default block size for the path's filesystem    * @throws IOException e    */
specifier|public
specifier|static
name|long
name|getDefaultBlockSize
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|Method
name|m
init|=
literal|null
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|FileSystem
argument_list|>
name|cls
init|=
name|fs
operator|.
name|getClass
argument_list|()
decl_stmt|;
try|try
block|{
name|m
operator|=
name|cls
operator|.
name|getMethod
argument_list|(
literal|"getDefaultBlockSize"
argument_list|,
operator|new
name|Class
argument_list|<
name|?
argument_list|>
index|[]
block|{
name|Path
operator|.
name|class
block|}
block|)
empty_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchMethodException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"FileSystem doesn't support getDefaultBlockSize"
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|SecurityException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Doesn't have access to getDefaultBlockSize on FileSystems"
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|m
operator|=
literal|null
expr_stmt|;
comment|// could happen on setAccessible()
block|}
if|if
condition|(
name|m
operator|==
literal|null
condition|)
block|{
return|return
name|fs
operator|.
name|getDefaultBlockSize
argument_list|(
name|path
argument_list|)
return|;
block|}
else|else
block|{
try|try
block|{
name|Object
name|ret
init|=
name|m
operator|.
name|invoke
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
decl_stmt|;
return|return
operator|(
operator|(
name|Long
operator|)
name|ret
operator|)
operator|.
name|longValue
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
end_class

begin_comment
comment|/*    * Get the default replication.    *    * use reflection to search for getDefaultReplication(Path f)    * if the method doesn't exist, fall back to using getDefaultReplication()    *    * @param fs filesystem object    * @param f path of file    * @return default replication for the path's filesystem    * @throws IOException e    */
end_comment

begin_function
specifier|public
specifier|static
name|short
name|getDefaultReplication
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|Method
name|m
init|=
literal|null
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|FileSystem
argument_list|>
name|cls
init|=
name|fs
operator|.
name|getClass
argument_list|()
decl_stmt|;
try|try
block|{
name|m
operator|=
name|cls
operator|.
name|getMethod
argument_list|(
literal|"getDefaultReplication"
argument_list|,
operator|new
name|Class
argument_list|<
name|?
argument_list|>
index|[]
block|{
name|Path
operator|.
name|class
block|}
block|)
empty_stmt|;
block|}
end_function

begin_catch
catch|catch
parameter_list|(
name|NoSuchMethodException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"FileSystem doesn't support getDefaultReplication"
argument_list|)
expr_stmt|;
block|}
end_catch

begin_catch
catch|catch
parameter_list|(
name|SecurityException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Doesn't have access to getDefaultReplication on FileSystems"
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|m
operator|=
literal|null
expr_stmt|;
comment|// could happen on setAccessible()
block|}
end_catch

begin_if
if|if
condition|(
name|m
operator|==
literal|null
condition|)
block|{
return|return
name|fs
operator|.
name|getDefaultReplication
argument_list|(
name|path
argument_list|)
return|;
block|}
else|else
block|{
try|try
block|{
name|Object
name|ret
init|=
name|m
operator|.
name|invoke
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
decl_stmt|;
return|return
operator|(
operator|(
name|Number
operator|)
name|ret
operator|)
operator|.
name|shortValue
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
end_if

begin_comment
unit|}
comment|/**    * Returns the default buffer size to use during writes.    *    * The size of the buffer should probably be a multiple of hardware    * page size (4096 on Intel x86), and it determines how much data is    * buffered during read and write operations.    *    * @param fs filesystem object    * @return default buffer size to use during writes    */
end_comment

begin_function
unit|public
specifier|static
name|int
name|getDefaultBufferSize
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|)
block|{
return|return
name|fs
operator|.
name|getConf
argument_list|()
operator|.
name|getInt
argument_list|(
literal|"io.file.buffer.size"
argument_list|,
literal|4096
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Create the specified file on the filesystem. By default, this will:    *<ol>    *<li>apply the umask in the configuration (if it is enabled)</li>    *<li>use the fs configured buffer size (or 4096 if not set)</li>    *<li>use the default replication</li>    *<li>use the default block size</li>    *<li>not track progress</li>    *</ol>    *    * @param fs {@link FileSystem} on which to write the file    * @param path {@link Path} to the file to write    * @param perm intial permissions    * @param overwrite Whether or not the created file should be overwritten.    * @return output stream to the created file    * @throws IOException if the file cannot be created    */
end_comment

begin_function
specifier|public
specifier|static
name|FSDataOutputStream
name|create
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|FsPermission
name|perm
parameter_list|,
name|boolean
name|overwrite
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
literal|"Creating file="
operator|+
name|path
operator|+
literal|" with permission="
operator|+
name|perm
operator|+
literal|", overwrite="
operator|+
name|overwrite
argument_list|)
expr_stmt|;
block|}
return|return
name|fs
operator|.
name|create
argument_list|(
name|path
argument_list|,
name|perm
argument_list|,
name|overwrite
argument_list|,
name|getDefaultBufferSize
argument_list|(
name|fs
argument_list|)
argument_list|,
name|getDefaultReplication
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
argument_list|,
name|getDefaultBlockSize
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
argument_list|,
literal|null
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Get the file permissions specified in the configuration, if they are    * enabled.    *    * @param fs filesystem that the file will be created on.    * @param conf configuration to read for determining if permissions are    *          enabled and which to use    * @param permssionConfKey property key in the configuration to use when    *          finding the permission    * @return the permission to use when creating a new file on the fs. If    *         special permissions are not specified in the configuration, then    *         the default permissions on the the fs will be returned.    */
end_comment

begin_function
specifier|public
specifier|static
name|FsPermission
name|getFilePermissions
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Configuration
name|conf
parameter_list|,
specifier|final
name|String
name|permssionConfKey
parameter_list|)
block|{
name|boolean
name|enablePermissions
init|=
name|conf
operator|.
name|getBoolean
argument_list|(
name|HConstants
operator|.
name|ENABLE_DATA_FILE_UMASK
argument_list|,
literal|false
argument_list|)
decl_stmt|;
if|if
condition|(
name|enablePermissions
condition|)
block|{
try|try
block|{
name|FsPermission
name|perm
init|=
operator|new
name|FsPermission
argument_list|(
name|FULL_RWX_PERMISSIONS
argument_list|)
decl_stmt|;
comment|// make sure that we have a mask, if not, go default.
name|String
name|mask
init|=
name|conf
operator|.
name|get
argument_list|(
name|permssionConfKey
argument_list|)
decl_stmt|;
if|if
condition|(
name|mask
operator|==
literal|null
condition|)
block|{
return|return
name|FsPermission
operator|.
name|getFileDefault
argument_list|()
return|;
block|}
comment|// appy the umask
name|FsPermission
name|umask
init|=
operator|new
name|FsPermission
argument_list|(
name|mask
argument_list|)
decl_stmt|;
return|return
name|perm
operator|.
name|applyUMask
argument_list|(
name|umask
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|IllegalArgumentException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Incorrect umask attempted to be created: "
operator|+
name|conf
operator|.
name|get
argument_list|(
name|permssionConfKey
argument_list|)
operator|+
literal|", using default file permissions."
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
name|FsPermission
operator|.
name|getFileDefault
argument_list|()
return|;
block|}
block|}
return|return
name|FsPermission
operator|.
name|getFileDefault
argument_list|()
return|;
block|}
end_function

begin_comment
comment|/**    * Verifies root directory path is a valid URI with a scheme    *    * @param root root directory path    * @return Passed<code>root</code> argument.    * @throws IOException if not a valid URI with a scheme    */
end_comment

begin_function
specifier|public
specifier|static
name|Path
name|validateRootPath
parameter_list|(
name|Path
name|root
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
name|URI
name|rootURI
init|=
operator|new
name|URI
argument_list|(
name|root
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|scheme
init|=
name|rootURI
operator|.
name|getScheme
argument_list|()
decl_stmt|;
if|if
condition|(
name|scheme
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Root directory does not have a scheme"
argument_list|)
throw|;
block|}
return|return
name|root
return|;
block|}
catch|catch
parameter_list|(
name|URISyntaxException
name|e
parameter_list|)
block|{
name|IOException
name|io
init|=
operator|new
name|IOException
argument_list|(
literal|"Root directory path is not a valid "
operator|+
literal|"URI -- check your "
operator|+
name|HConstants
operator|.
name|HBASE_DIR
operator|+
literal|" configuration"
argument_list|)
decl_stmt|;
name|io
operator|.
name|initCause
argument_list|(
name|e
argument_list|)
expr_stmt|;
throw|throw
name|io
throw|;
block|}
block|}
end_function

begin_comment
comment|/**    * Checks for the presence of the WAL log root path (using the provided conf object) in the given    * path. If it exists, this method removes it and returns the String representation of remaining    * relative path.    * @param path must not be null    * @param conf must not be null    * @return String representation of the remaining relative path    * @throws IOException from underlying filesystem    */
end_comment

begin_function
specifier|public
specifier|static
name|String
name|removeWALRootPath
parameter_list|(
name|Path
name|path
parameter_list|,
specifier|final
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|root
init|=
name|getWALRootDir
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|String
name|pathStr
init|=
name|path
operator|.
name|toString
argument_list|()
decl_stmt|;
comment|// check that the path is absolute... it has the root path in it.
if|if
condition|(
operator|!
name|pathStr
operator|.
name|startsWith
argument_list|(
name|root
operator|.
name|toString
argument_list|()
argument_list|)
condition|)
block|{
return|return
name|pathStr
return|;
block|}
comment|// if not, return as it is.
return|return
name|pathStr
operator|.
name|substring
argument_list|(
name|root
operator|.
name|toString
argument_list|()
operator|.
name|length
argument_list|()
operator|+
literal|1
argument_list|)
return|;
comment|// remove the "/" too.
block|}
end_function

begin_comment
comment|/**    * Return the 'path' component of a Path.  In Hadoop, Path is an URI.  This    * method returns the 'path' component of a Path's URI: e.g. If a Path is    *<code>hdfs://example.org:9000/hbase_trunk/TestTable/compaction.dir</code>,    * this method returns<code>/hbase_trunk/TestTable/compaction.dir</code>.    * This method is useful if you want to print out a Path without qualifying    * Filesystem instance.    * @param p Filesystem Path whose 'path' component we are to return.    * @return Path portion of the Filesystem    */
end_comment

begin_function
specifier|public
specifier|static
name|String
name|getPath
parameter_list|(
name|Path
name|p
parameter_list|)
block|{
return|return
name|p
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
return|;
block|}
end_function

begin_comment
comment|/**    * @param c configuration    * @return {@link Path} to hbase root directory from    *     configuration as a qualified Path.    * @throws IOException e    */
end_comment

begin_function
specifier|public
specifier|static
name|Path
name|getRootDir
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|c
operator|.
name|get
argument_list|(
name|HConstants
operator|.
name|HBASE_DIR
argument_list|)
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|c
argument_list|)
decl_stmt|;
return|return
name|p
operator|.
name|makeQualified
argument_list|(
name|fs
argument_list|)
return|;
block|}
end_function

begin_function
specifier|public
specifier|static
name|void
name|setRootDir
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|,
specifier|final
name|Path
name|root
parameter_list|)
throws|throws
name|IOException
block|{
name|c
operator|.
name|set
argument_list|(
name|HConstants
operator|.
name|HBASE_DIR
argument_list|,
name|root
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
specifier|public
specifier|static
name|void
name|setFsDefault
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|,
specifier|final
name|Path
name|root
parameter_list|)
throws|throws
name|IOException
block|{
name|c
operator|.
name|set
argument_list|(
literal|"fs.defaultFS"
argument_list|,
name|root
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// for hadoop 0.21+
block|}
end_function

begin_function
specifier|public
specifier|static
name|FileSystem
name|getRootDirFileSystem
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|p
init|=
name|getRootDir
argument_list|(
name|c
argument_list|)
decl_stmt|;
return|return
name|p
operator|.
name|getFileSystem
argument_list|(
name|c
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * @param c configuration    * @return {@link Path} to hbase log root directory: e.g. {@value HBASE_WAL_DIR} from    *     configuration as a qualified Path. Defaults to HBase root dir.    * @throws IOException e    */
end_comment

begin_function
specifier|public
specifier|static
name|Path
name|getWALRootDir
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|c
operator|.
name|get
argument_list|(
name|HBASE_WAL_DIR
argument_list|,
name|c
operator|.
name|get
argument_list|(
name|HConstants
operator|.
name|HBASE_DIR
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|isValidWALRootDir
argument_list|(
name|p
argument_list|,
name|c
argument_list|)
condition|)
block|{
return|return
name|getRootDir
argument_list|(
name|c
argument_list|)
return|;
block|}
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|c
argument_list|)
decl_stmt|;
return|return
name|p
operator|.
name|makeQualified
argument_list|(
name|fs
argument_list|)
return|;
block|}
end_function

begin_function
annotation|@
name|VisibleForTesting
specifier|public
specifier|static
name|void
name|setWALRootDir
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|,
specifier|final
name|Path
name|root
parameter_list|)
throws|throws
name|IOException
block|{
name|c
operator|.
name|set
argument_list|(
name|HBASE_WAL_DIR
argument_list|,
name|root
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
end_function

begin_function
specifier|public
specifier|static
name|FileSystem
name|getWALFileSystem
parameter_list|(
specifier|final
name|Configuration
name|c
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|p
init|=
name|getWALRootDir
argument_list|(
name|c
argument_list|)
decl_stmt|;
return|return
name|p
operator|.
name|getFileSystem
argument_list|(
name|c
argument_list|)
return|;
block|}
end_function

begin_function
specifier|private
specifier|static
name|boolean
name|isValidWALRootDir
parameter_list|(
name|Path
name|walDir
parameter_list|,
specifier|final
name|Configuration
name|c
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|rootDir
init|=
name|getRootDir
argument_list|(
name|c
argument_list|)
decl_stmt|;
if|if
condition|(
name|walDir
operator|!=
name|rootDir
condition|)
block|{
if|if
condition|(
name|walDir
operator|.
name|toString
argument_list|()
operator|.
name|startsWith
argument_list|(
name|rootDir
operator|.
name|toString
argument_list|()
operator|+
literal|"/"
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Illegal WAL directory specified. "
operator|+
literal|"WAL directories are not permitted to be under the root directory if set."
argument_list|)
throw|;
block|}
block|}
return|return
literal|true
return|;
block|}
end_function

begin_comment
comment|/**    * Returns the {@link org.apache.hadoop.fs.Path} object representing the table directory under    * path rootdir    *    * @param rootdir qualified path of HBase root directory    * @param tableName name of table    * @return {@link org.apache.hadoop.fs.Path} for table    */
end_comment

begin_function
specifier|public
specifier|static
name|Path
name|getTableDir
parameter_list|(
name|Path
name|rootdir
parameter_list|,
specifier|final
name|TableName
name|tableName
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|getNamespaceDir
argument_list|(
name|rootdir
argument_list|,
name|tableName
operator|.
name|getNamespaceAsString
argument_list|()
argument_list|)
argument_list|,
name|tableName
operator|.
name|getQualifierAsString
argument_list|()
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Returns the {@link org.apache.hadoop.hbase.TableName} object representing    * the table directory under    * path rootdir    *    * @param tablePath path of table    * @return {@link org.apache.hadoop.fs.Path} for table    */
end_comment

begin_function
specifier|public
specifier|static
name|TableName
name|getTableName
parameter_list|(
name|Path
name|tablePath
parameter_list|)
block|{
return|return
name|TableName
operator|.
name|valueOf
argument_list|(
name|tablePath
operator|.
name|getParent
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|,
name|tablePath
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Returns the {@link org.apache.hadoop.fs.Path} object representing    * the namespace directory under path rootdir    *    * @param rootdir qualified path of HBase root directory    * @param namespace namespace name    * @return {@link org.apache.hadoop.fs.Path} for table    */
end_comment

begin_function
specifier|public
specifier|static
name|Path
name|getNamespaceDir
parameter_list|(
name|Path
name|rootdir
parameter_list|,
specifier|final
name|String
name|namespace
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|rootdir
argument_list|,
operator|new
name|Path
argument_list|(
name|HConstants
operator|.
name|BASE_NAMESPACE_DIR
argument_list|,
operator|new
name|Path
argument_list|(
name|namespace
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Sets storage policy for given path according to config setting.    * If the passed path is a directory, we'll set the storage policy for all files    * created in the future in said directory. Note that this change in storage    * policy takes place at the FileSystem level; it will persist beyond this RS's lifecycle.    * If we're running on a FileSystem implementation that doesn't support the given storage policy    * (or storage policies at all), then we'll issue a log message and continue.    *    * See http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html    *    * @param fs We only do anything it implements a setStoragePolicy method    * @param conf used to look up storage policy with given key; not modified.    * @param path the Path whose storage policy is to be set    * @param policyKey Key to use pulling a policy from Configuration:    *   e.g. HConstants.WAL_STORAGE_POLICY (hbase.wal.storage.policy).    * @param defaultPolicy if the configured policy is equal to this policy name, we will skip    *   telling the FileSystem to set a storage policy.    */
end_comment

begin_function
specifier|public
specifier|static
name|void
name|setStoragePolicy
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Configuration
name|conf
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|,
specifier|final
name|String
name|policyKey
parameter_list|,
specifier|final
name|String
name|defaultPolicy
parameter_list|)
block|{
name|String
name|storagePolicy
init|=
name|conf
operator|.
name|get
argument_list|(
name|policyKey
argument_list|,
name|defaultPolicy
argument_list|)
operator|.
name|toUpperCase
argument_list|(
name|Locale
operator|.
name|ROOT
argument_list|)
decl_stmt|;
if|if
condition|(
name|storagePolicy
operator|.
name|equals
argument_list|(
name|defaultPolicy
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
literal|"default policy of "
operator|+
name|defaultPolicy
operator|+
literal|" requested, exiting early."
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
name|setStoragePolicy
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
name|storagePolicy
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|// this mapping means that under a federated FileSystem implementation, we'll
end_comment

begin_comment
comment|// only log the first failure from any of the underlying FileSystems at WARN and all others
end_comment

begin_comment
comment|// will be at DEBUG.
end_comment

begin_decl_stmt
specifier|private
specifier|static
specifier|final
name|Map
argument_list|<
name|FileSystem
argument_list|,
name|Boolean
argument_list|>
name|warningMap
init|=
operator|new
name|ConcurrentHashMap
argument_list|<
name|FileSystem
argument_list|,
name|Boolean
argument_list|>
argument_list|()
decl_stmt|;
end_decl_stmt

begin_comment
comment|/**    * Sets storage policy for given path.    * If the passed path is a directory, we'll set the storage policy for all files    * created in the future in said directory. Note that this change in storage    * policy takes place at the FileSystem level; it will persist beyond this RS's lifecycle.    * If we're running on a version of FileSystem that doesn't support the given storage policy    * (or storage policies at all), then we'll issue a log message and continue.    *    * See http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html    *    * @param fs We only do anything it implements a setStoragePolicy method    * @param path the Path whose storage policy is to be set    * @param storagePolicy Policy to set on<code>path</code>; see hadoop 2.6+    *   org.apache.hadoop.hdfs.protocol.HdfsConstants for possible list e.g    *   'COLD', 'WARM', 'HOT', 'ONE_SSD', 'ALL_SSD', 'LAZY_PERSIST'.    */
end_comment

begin_function
specifier|public
specifier|static
name|void
name|setStoragePolicy
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|,
specifier|final
name|String
name|storagePolicy
parameter_list|)
block|{
if|if
condition|(
name|storagePolicy
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
literal|"We were passed a null storagePolicy, exiting early."
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
specifier|final
name|String
name|trimmedStoragePolicy
init|=
name|storagePolicy
operator|.
name|trim
argument_list|()
decl_stmt|;
if|if
condition|(
name|trimmedStoragePolicy
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
literal|"We were passed an empty storagePolicy, exiting early."
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
name|invokeSetStoragePolicy
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
name|trimmedStoragePolicy
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/*    * All args have been checked and are good. Run the setStoragePolicy invocation.    */
end_comment

begin_function
specifier|private
specifier|static
name|void
name|invokeSetStoragePolicy
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|,
specifier|final
name|String
name|storagePolicy
parameter_list|)
block|{
name|Method
name|m
init|=
literal|null
decl_stmt|;
try|try
block|{
name|m
operator|=
name|fs
operator|.
name|getClass
argument_list|()
operator|.
name|getDeclaredMethod
argument_list|(
literal|"setStoragePolicy"
argument_list|,
operator|new
name|Class
argument_list|<
name|?
argument_list|>
index|[]
block|{
name|Path
operator|.
name|class
operator|,
name|String
operator|.
name|class
block|}
block|)
empty_stmt|;
name|m
operator|.
name|setAccessible
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
end_function

begin_catch
catch|catch
parameter_list|(
name|NoSuchMethodException
name|e
parameter_list|)
block|{
specifier|final
name|String
name|msg
init|=
literal|"FileSystem doesn't support setStoragePolicy; HDFS-6584, HDFS-9345 "
operator|+
literal|"not available. This is normal and expected on earlier Hadoop versions."
decl_stmt|;
if|if
condition|(
operator|!
name|warningMap
operator|.
name|containsKey
argument_list|(
name|fs
argument_list|)
condition|)
block|{
name|warningMap
operator|.
name|put
argument_list|(
name|fs
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
name|m
operator|=
literal|null
expr_stmt|;
block|}
end_catch

begin_catch
catch|catch
parameter_list|(
name|SecurityException
name|e
parameter_list|)
block|{
specifier|final
name|String
name|msg
init|=
literal|"No access to setStoragePolicy on FileSystem from the SecurityManager; "
operator|+
literal|"HDFS-6584, HDFS-9345 not available. This is unusual and probably warrants an email "
operator|+
literal|"to the user@hbase mailing list. Please be sure to include a link to your configs, and "
operator|+
literal|"logs that include this message and period of time before it. Logs around service "
operator|+
literal|"start up will probably be useful as well."
decl_stmt|;
if|if
condition|(
operator|!
name|warningMap
operator|.
name|containsKey
argument_list|(
name|fs
argument_list|)
condition|)
block|{
name|warningMap
operator|.
name|put
argument_list|(
name|fs
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
name|m
operator|=
literal|null
expr_stmt|;
comment|// could happen on setAccessible() or getDeclaredMethod()
block|}
end_catch

begin_if
if|if
condition|(
name|m
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|m
operator|.
name|invoke
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
name|storagePolicy
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Set storagePolicy="
operator|+
name|storagePolicy
operator|+
literal|" for path="
operator|+
name|path
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// This swallows FNFE, should we be throwing it? seems more likely to indicate dev
comment|// misuse than a runtime problem with HDFS.
if|if
condition|(
operator|!
name|warningMap
operator|.
name|containsKey
argument_list|(
name|fs
argument_list|)
condition|)
block|{
name|warningMap
operator|.
name|put
argument_list|(
name|fs
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to set storagePolicy="
operator|+
name|storagePolicy
operator|+
literal|" for path="
operator|+
name|path
operator|+
literal|". "
operator|+
literal|"DEBUG log level might have more details."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Unable to set storagePolicy="
operator|+
name|storagePolicy
operator|+
literal|" for path="
operator|+
name|path
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// check for lack of HDFS-7228
if|if
condition|(
name|e
operator|instanceof
name|InvocationTargetException
condition|)
block|{
specifier|final
name|Throwable
name|exception
init|=
name|e
operator|.
name|getCause
argument_list|()
decl_stmt|;
if|if
condition|(
name|exception
operator|instanceof
name|RemoteException
operator|&&
name|HadoopIllegalArgumentException
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
operator|(
operator|(
name|RemoteException
operator|)
name|exception
operator|)
operator|.
name|getClassName
argument_list|()
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Given storage policy, '"
operator|+
name|storagePolicy
operator|+
literal|"', was rejected and probably "
operator|+
literal|"isn't a valid policy for the version of Hadoop you're running. I.e. if you're "
operator|+
literal|"trying to use SSD related policies then you're likely missing HDFS-7228. For "
operator|+
literal|"more information see the 'ArchivalStorage' docs for your Hadoop release."
argument_list|)
expr_stmt|;
block|}
comment|// Hadoop 2.8+, 3.0-a1+ added FileSystem.setStoragePolicy with a default implementation
comment|// that throws UnsupportedOperationException
block|}
elseif|else
if|if
condition|(
name|exception
operator|instanceof
name|UnsupportedOperationException
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The underlying FileSystem implementation doesn't support "
operator|+
literal|"setStoragePolicy. This is probably intentional on their part, since HDFS-9345 "
operator|+
literal|"appears to be present in your version of Hadoop. For more information check "
operator|+
literal|"the Hadoop documentation on 'ArchivalStorage', the Hadoop FileSystem "
operator|+
literal|"specification docs from HADOOP-11981, and/or related documentation from the "
operator|+
literal|"provider of the underlying FileSystem (its name should appear in the "
operator|+
literal|"stacktrace that accompanies this message). Note in particular that Hadoop's "
operator|+
literal|"local filesystem implementation doesn't support storage policies."
argument_list|,
name|exception
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
end_if

begin_comment
unit|}
comment|/**    * @param conf must not be null    * @return True if this filesystem whose scheme is 'hdfs'.    * @throws IOException from underlying FileSystem    */
end_comment

begin_function
unit|public
specifier|static
name|boolean
name|isHDFS
parameter_list|(
specifier|final
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|String
name|scheme
init|=
name|fs
operator|.
name|getUri
argument_list|()
operator|.
name|getScheme
argument_list|()
decl_stmt|;
return|return
name|scheme
operator|.
name|equalsIgnoreCase
argument_list|(
literal|"hdfs"
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Checks if the given path is the one with 'recovered.edits' dir.    * @param path must not be null    * @return True if we recovered edits    */
end_comment

begin_function
specifier|public
specifier|static
name|boolean
name|isRecoveredEdits
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
return|return
name|path
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|HConstants
operator|.
name|RECOVERED_EDITS_DIR
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * @param conf must not be null    * @return Returns the filesystem of the hbase rootdir.    * @throws IOException from underlying FileSystem    */
end_comment

begin_function
specifier|public
specifier|static
name|FileSystem
name|getCurrentFileSystem
parameter_list|(
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|getRootDir
argument_list|(
name|conf
argument_list|)
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Calls fs.listStatus() and treats FileNotFoundException as non-fatal    * This accommodates differences between hadoop versions, where hadoop 1    * does not throw a FileNotFoundException, and return an empty FileStatus[]    * while Hadoop 2 will throw FileNotFoundException.    *    * Where possible, prefer FSUtils#listStatusWithStatusFilter(FileSystem,    * Path, FileStatusFilter) instead.    *    * @param fs file system    * @param dir directory    * @param filter path filter    * @return null if dir is empty or doesn't exist, otherwise FileStatus array    */
end_comment

begin_function
specifier|public
specifier|static
name|FileStatus
index|[]
name|listStatus
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|dir
parameter_list|,
specifier|final
name|PathFilter
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|FileStatus
index|[]
name|status
init|=
literal|null
decl_stmt|;
try|try
block|{
name|status
operator|=
name|filter
operator|==
literal|null
condition|?
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|)
else|:
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|,
name|filter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|fnfe
parameter_list|)
block|{
comment|// if directory doesn't exist, return null
if|if
condition|(
name|LOG
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
name|dir
operator|+
literal|" doesn't exist"
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|status
operator|==
literal|null
operator|||
name|status
operator|.
name|length
operator|<
literal|1
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|status
return|;
block|}
end_function

begin_comment
comment|/**    * Calls fs.listStatus() and treats FileNotFoundException as non-fatal    * This would accommodates differences between hadoop versions    *    * @param fs file system    * @param dir directory    * @return null if dir is empty or doesn't exist, otherwise FileStatus array    */
end_comment

begin_function
specifier|public
specifier|static
name|FileStatus
index|[]
name|listStatus
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|listStatus
argument_list|(
name|fs
argument_list|,
name|dir
argument_list|,
literal|null
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Calls fs.listFiles() to get FileStatus and BlockLocations together for reducing rpc call    *    * @param fs file system    * @param dir directory    * @return LocatedFileStatus list    */
end_comment

begin_function
specifier|public
specifier|static
name|List
argument_list|<
name|LocatedFileStatus
argument_list|>
name|listLocatedStatus
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|LocatedFileStatus
argument_list|>
name|status
init|=
literal|null
decl_stmt|;
try|try
block|{
name|RemoteIterator
argument_list|<
name|LocatedFileStatus
argument_list|>
name|locatedFileStatusRemoteIterator
init|=
name|fs
operator|.
name|listFiles
argument_list|(
name|dir
argument_list|,
literal|false
argument_list|)
decl_stmt|;
while|while
condition|(
name|locatedFileStatusRemoteIterator
operator|.
name|hasNext
argument_list|()
condition|)
block|{
if|if
condition|(
name|status
operator|==
literal|null
condition|)
block|{
name|status
operator|=
name|Lists
operator|.
name|newArrayList
argument_list|()
expr_stmt|;
block|}
name|status
operator|.
name|add
argument_list|(
name|locatedFileStatusRemoteIterator
operator|.
name|next
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|fnfe
parameter_list|)
block|{
comment|// if directory doesn't exist, return null
if|if
condition|(
name|LOG
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
name|dir
operator|+
literal|" doesn't exist"
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|status
return|;
block|}
end_function

begin_comment
comment|/**    * Calls fs.delete() and returns the value returned by the fs.delete()    *    * @param fs must not be null    * @param path must not be null    * @param recursive delete tree rooted at path    * @return the value returned by the fs.delete()    * @throws IOException from underlying FileSystem    */
end_comment

begin_function
specifier|public
specifier|static
name|boolean
name|delete
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|,
specifier|final
name|boolean
name|recursive
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|fs
operator|.
name|delete
argument_list|(
name|path
argument_list|,
name|recursive
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Calls fs.exists(). Checks if the specified path exists    *    * @param fs must not be null    * @param path must not be null    * @return the value returned by fs.exists()    * @throws IOException from underlying FileSystem    */
end_comment

begin_function
specifier|public
specifier|static
name|boolean
name|isExists
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|fs
operator|.
name|exists
argument_list|(
name|path
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Log the current state of the filesystem from a certain root directory    * @param fs filesystem to investigate    * @param root root file/directory to start logging from    * @param LOG log to output information    * @throws IOException if an unexpected exception occurs    */
end_comment

begin_function
specifier|public
specifier|static
name|void
name|logFileSystemState
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|root
parameter_list|,
name|Log
name|LOG
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"File system contents for path "
operator|+
name|root
argument_list|)
expr_stmt|;
name|logFSTree
argument_list|(
name|LOG
argument_list|,
name|fs
argument_list|,
name|root
argument_list|,
literal|"|-"
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/**    * Recursive helper to log the state of the FS    *    * @see #logFileSystemState(FileSystem, Path, Log)    */
end_comment

begin_function
specifier|private
specifier|static
name|void
name|logFSTree
parameter_list|(
name|Log
name|LOG
parameter_list|,
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|root
parameter_list|,
name|String
name|prefix
parameter_list|)
throws|throws
name|IOException
block|{
name|FileStatus
index|[]
name|files
init|=
name|listStatus
argument_list|(
name|fs
argument_list|,
name|root
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|files
operator|==
literal|null
condition|)
block|{
return|return;
block|}
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
if|if
condition|(
name|file
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|prefix
operator|+
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|"/"
argument_list|)
expr_stmt|;
name|logFSTree
argument_list|(
name|LOG
argument_list|,
name|fs
argument_list|,
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|prefix
operator|+
literal|"---"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|prefix
operator|+
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
end_function

begin_function
specifier|public
specifier|static
name|boolean
name|renameAndSetModifyTime
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|Path
name|src
parameter_list|,
specifier|final
name|Path
name|dest
parameter_list|)
throws|throws
name|IOException
block|{
comment|// set the modify time for TimeToLive Cleaner
name|fs
operator|.
name|setTimes
argument_list|(
name|src
argument_list|,
name|EnvironmentEdgeManager
operator|.
name|currentTime
argument_list|()
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
return|return
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dest
argument_list|)
return|;
block|}
end_function

begin_comment
comment|/**    * Do our short circuit read setup.    * Checks buffer size to use and whether to do checksumming in hbase or hdfs.    * @param conf must not be null    */
end_comment

begin_function
specifier|public
specifier|static
name|void
name|setupShortCircuitRead
parameter_list|(
specifier|final
name|Configuration
name|conf
parameter_list|)
block|{
comment|// Check that the user has not set the "dfs.client.read.shortcircuit.skip.checksum" property.
name|boolean
name|shortCircuitSkipChecksum
init|=
name|conf
operator|.
name|getBoolean
argument_list|(
literal|"dfs.client.read.shortcircuit.skip.checksum"
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|boolean
name|useHBaseChecksum
init|=
name|conf
operator|.
name|getBoolean
argument_list|(
name|HConstants
operator|.
name|HBASE_CHECKSUM_VERIFICATION
argument_list|,
literal|true
argument_list|)
decl_stmt|;
if|if
condition|(
name|shortCircuitSkipChecksum
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Configuration \"dfs.client.read.shortcircuit.skip.checksum\" should not "
operator|+
literal|"be set to true."
operator|+
operator|(
name|useHBaseChecksum
condition|?
literal|" HBase checksum doesn't require "
operator|+
literal|"it, see https://issues.apache.org/jira/browse/HBASE-6868."
else|:
literal|""
operator|)
argument_list|)
expr_stmt|;
assert|assert
operator|!
name|shortCircuitSkipChecksum
assert|;
comment|//this will fail if assertions are on
block|}
name|checkShortCircuitReadBufferSize
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|/**    * Check if short circuit read buffer size is set and if not, set it to hbase value.    * @param conf must not be null    */
end_comment

begin_function
specifier|public
specifier|static
name|void
name|checkShortCircuitReadBufferSize
parameter_list|(
specifier|final
name|Configuration
name|conf
parameter_list|)
block|{
specifier|final
name|int
name|defaultSize
init|=
name|HConstants
operator|.
name|DEFAULT_BLOCKSIZE
operator|*
literal|2
decl_stmt|;
specifier|final
name|int
name|notSet
init|=
operator|-
literal|1
decl_stmt|;
comment|// DFSConfigKeys.DFS_CLIENT_READ_SHORTCIRCUIT_BUFFER_SIZE_KEY is only defined in h2
specifier|final
name|String
name|dfsKey
init|=
literal|"dfs.client.read.shortcircuit.buffer.size"
decl_stmt|;
name|int
name|size
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|dfsKey
argument_list|,
name|notSet
argument_list|)
decl_stmt|;
comment|// If a size is set, return -- we will use it.
if|if
condition|(
name|size
operator|!=
name|notSet
condition|)
block|{
return|return;
block|}
comment|// But short circuit buffer size is normally not set.  Put in place the hbase wanted size.
name|int
name|hbaseSize
init|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase."
operator|+
name|dfsKey
argument_list|,
name|defaultSize
argument_list|)
decl_stmt|;
name|conf
operator|.
name|setIfUnset
argument_list|(
name|dfsKey
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|hbaseSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
end_function

begin_comment
comment|// Holder singleton idiom. JVM spec ensures this will be run at most once per Classloader, and
end_comment

begin_comment
comment|// not until we attempt to reference it.
end_comment

begin_class
specifier|private
specifier|static
class|class
name|StreamCapabilities
block|{
specifier|public
specifier|static
specifier|final
name|boolean
name|PRESENT
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|Class
argument_list|<
name|?
argument_list|>
name|CLASS
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|Method
name|METHOD
decl_stmt|;
static|static
block|{
name|boolean
name|tmp
init|=
literal|false
decl_stmt|;
name|Class
argument_list|<
name|?
argument_list|>
name|clazz
init|=
literal|null
decl_stmt|;
name|Method
name|method
init|=
literal|null
decl_stmt|;
try|try
block|{
name|clazz
operator|=
name|Class
operator|.
name|forName
argument_list|(
literal|"org.apache.hadoop.fs.StreamCapabilities"
argument_list|)
expr_stmt|;
name|method
operator|=
name|clazz
operator|.
name|getMethod
argument_list|(
literal|"hasCapability"
argument_list|,
name|String
operator|.
name|class
argument_list|)
expr_stmt|;
name|tmp
operator|=
literal|true
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
decl||
name|NoSuchMethodException
decl||
name|SecurityException
name|exception
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Your Hadoop installation does not include the StreamCapabilities class from "
operator|+
literal|"HDFS-11644, so we will skip checking if any FSDataOutputStreams actually "
operator|+
literal|"support hflush/hsync. If you are running on top of HDFS this probably just "
operator|+
literal|"means you have an older version and this can be ignored. If you are running on "
operator|+
literal|"top of an alternate FileSystem implementation you should manually verify that "
operator|+
literal|"hflush and hsync are implemented; otherwise you risk data loss and hard to "
operator|+
literal|"diagnose errors when our assumptions are violated."
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"The first request to check for StreamCapabilities came from this stacktrace."
argument_list|,
name|exception
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|PRESENT
operator|=
name|tmp
expr_stmt|;
name|CLASS
operator|=
name|clazz
expr_stmt|;
name|METHOD
operator|=
name|method
expr_stmt|;
block|}
block|}
block|}
end_class

begin_comment
comment|/**    * If our FileSystem version includes the StreamCapabilities class, check if    * the given stream has a particular capability.    * @param stream capabilities are per-stream instance, so check this one specifically. must not be    *        null    * @param capability what to look for, per Hadoop Common's FileSystem docs    * @return true if there are no StreamCapabilities. false if there are, but this stream doesn't    *         implement it. return result of asking the stream otherwise.    */
end_comment

begin_function
specifier|public
specifier|static
name|boolean
name|hasCapability
parameter_list|(
name|FSDataOutputStream
name|stream
parameter_list|,
name|String
name|capability
parameter_list|)
block|{
comment|// be consistent whether or not StreamCapabilities is present
if|if
condition|(
name|stream
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|NullPointerException
argument_list|(
literal|"stream parameter must not be null."
argument_list|)
throw|;
block|}
comment|// If o.a.h.fs.StreamCapabilities doesn't exist, assume everyone does everything
comment|// otherwise old versions of Hadoop will break.
name|boolean
name|result
init|=
literal|true
decl_stmt|;
if|if
condition|(
name|StreamCapabilities
operator|.
name|PRESENT
condition|)
block|{
comment|// if StreamCapabilities is present, but the stream doesn't implement it
comment|// or we run into a problem invoking the method,
comment|// we treat that as equivalent to not declaring anything
name|result
operator|=
literal|false
expr_stmt|;
if|if
condition|(
name|StreamCapabilities
operator|.
name|CLASS
operator|.
name|isAssignableFrom
argument_list|(
name|stream
operator|.
name|getClass
argument_list|()
argument_list|)
condition|)
block|{
try|try
block|{
name|result
operator|=
operator|(
operator|(
name|Boolean
operator|)
name|StreamCapabilities
operator|.
name|METHOD
operator|.
name|invoke
argument_list|(
name|stream
argument_list|,
name|capability
argument_list|)
operator|)
operator|.
name|booleanValue
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IllegalAccessException
decl||
name|IllegalArgumentException
decl||
name|InvocationTargetException
name|exception
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Your Hadoop installation's StreamCapabilities implementation doesn't match "
operator|+
literal|"our understanding of how it's supposed to work. Please file a JIRA and include "
operator|+
literal|"the following stack trace. In the mean time we're interpreting this behavior "
operator|+
literal|"difference as a lack of capability support, which will probably cause a failure."
argument_list|,
name|exception
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|result
return|;
block|}
end_function

begin_comment
comment|/**    * Helper exception for those cases where the place where we need to check a stream capability    * is not where we have the needed context to explain the impact and mitigation for a lack.    */
end_comment

begin_class
specifier|public
specifier|static
class|class
name|StreamLacksCapabilityException
extends|extends
name|Exception
block|{
specifier|public
name|StreamLacksCapabilityException
parameter_list|(
name|String
name|message
parameter_list|,
name|Throwable
name|cause
parameter_list|)
block|{
name|super
argument_list|(
name|message
argument_list|,
name|cause
argument_list|)
expr_stmt|;
block|}
specifier|public
name|StreamLacksCapabilityException
parameter_list|(
name|String
name|message
parameter_list|)
block|{
name|super
argument_list|(
name|message
argument_list|)
expr_stmt|;
block|}
block|}
end_class

unit|}
end_unit

