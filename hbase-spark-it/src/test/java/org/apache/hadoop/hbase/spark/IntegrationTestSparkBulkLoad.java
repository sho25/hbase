begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|spark
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Sets
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|CommandLine
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|RandomStringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|Cell
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|CellUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HBaseConfiguration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HBaseTestingUtility
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HTableDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|IntegrationTestBase
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|IntegrationTestingUtility
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|TableName
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Admin
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|ConnectionFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Consistency
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|RegionLocator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Result
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Scan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|ImmutableBytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|mapreduce
operator|.
name|IntegrationTestBulkLoad
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|tool
operator|.
name|LoadIncrementalHFiles
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|EnvironmentEdgeManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Pair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|RegionSplitter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ToolRunner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|SerializableWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|SparkConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|api
operator|.
name|java
operator|.
name|JavaRDD
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|api
operator|.
name|java
operator|.
name|JavaSparkContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|Partitioner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|api
operator|.
name|java
operator|.
name|function
operator|.
name|Function
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|api
operator|.
name|java
operator|.
name|function
operator|.
name|Function2
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|api
operator|.
name|java
operator|.
name|function
operator|.
name|PairFlatMapFunction
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|api
operator|.
name|java
operator|.
name|function
operator|.
name|VoidFunction
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Test
import|;
end_import

begin_import
import|import
name|scala
operator|.
name|Tuple2
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_comment
comment|/**  * Test Bulk Load and Spark on a distributed cluster.  * It starts an Spark job that creates linked chains.  * This test mimic {@link IntegrationTestBulkLoad} in mapreduce.  *  * Usage on cluster:  *   First add hbase related jars and hbase-spark.jar into spark classpath.  *  *   spark-submit --class org.apache.hadoop.hbase.spark.IntegrationTestSparkBulkLoad  *                HBASE_HOME/lib/hbase-spark-it-XXX-tests.jar -m slowDeterministic -Dhbase.spark.bulkload.chainlength=300  */
end_comment

begin_class
specifier|public
class|class
name|IntegrationTestSparkBulkLoad
extends|extends
name|IntegrationTestBase
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|IntegrationTestSparkBulkLoad
operator|.
name|class
argument_list|)
decl_stmt|;
comment|// The number of partitions for random generated data
specifier|private
specifier|static
name|String
name|BULKLOAD_PARTITIONS_NUM
init|=
literal|"hbase.spark.bulkload.partitionsnum"
decl_stmt|;
specifier|private
specifier|static
name|int
name|DEFAULT_BULKLOAD_PARTITIONS_NUM
init|=
literal|3
decl_stmt|;
specifier|private
specifier|static
name|String
name|BULKLOAD_CHAIN_LENGTH
init|=
literal|"hbase.spark.bulkload.chainlength"
decl_stmt|;
specifier|private
specifier|static
name|int
name|DEFAULT_BULKLOAD_CHAIN_LENGTH
init|=
literal|200000
decl_stmt|;
specifier|private
specifier|static
name|String
name|BULKLOAD_IMPORT_ROUNDS
init|=
literal|"hbase.spark.bulkload.importround"
decl_stmt|;
specifier|private
specifier|static
name|int
name|DEFAULT_BULKLOAD_IMPORT_ROUNDS
init|=
literal|1
decl_stmt|;
specifier|private
specifier|static
name|String
name|CURRENT_ROUND_NUM
init|=
literal|"hbase.spark.bulkload.current.roundnum"
decl_stmt|;
specifier|private
specifier|static
name|String
name|NUM_REPLICA_COUNT_KEY
init|=
literal|"hbase.spark.bulkload.replica.countkey"
decl_stmt|;
specifier|private
specifier|static
name|int
name|DEFAULT_NUM_REPLICA_COUNT
init|=
literal|1
decl_stmt|;
specifier|private
specifier|static
name|String
name|BULKLOAD_TABLE_NAME
init|=
literal|"hbase.spark.bulkload.tableName"
decl_stmt|;
specifier|private
specifier|static
name|String
name|DEFAULT_BULKLOAD_TABLE_NAME
init|=
literal|"IntegrationTestSparkBulkLoad"
decl_stmt|;
specifier|private
specifier|static
name|String
name|BULKLOAD_OUTPUT_PATH
init|=
literal|"hbase.spark.bulkload.output.path"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|OPT_LOAD
init|=
literal|"load"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|OPT_CHECK
init|=
literal|"check"
decl_stmt|;
specifier|private
name|boolean
name|load
init|=
literal|false
decl_stmt|;
specifier|private
name|boolean
name|check
init|=
literal|false
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|byte
index|[]
name|CHAIN_FAM
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
literal|"L"
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|byte
index|[]
name|SORT_FAM
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
literal|"S"
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|byte
index|[]
name|DATA_FAM
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
literal|"D"
argument_list|)
decl_stmt|;
comment|/**    * Running spark job to load data into hbase table    */
specifier|public
name|void
name|runLoad
parameter_list|()
throws|throws
name|Exception
block|{
name|setupTable
argument_list|()
expr_stmt|;
name|int
name|numImportRounds
init|=
name|getConf
argument_list|()
operator|.
name|getInt
argument_list|(
name|BULKLOAD_IMPORT_ROUNDS
argument_list|,
name|DEFAULT_BULKLOAD_IMPORT_ROUNDS
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Running load with numIterations:"
operator|+
name|numImportRounds
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numImportRounds
condition|;
name|i
operator|++
control|)
block|{
name|runLinkedListSparkJob
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Running spark job to create LinkedList for testing    * @param iteration iteration th of this job    * @throws Exception    */
specifier|public
name|void
name|runLinkedListSparkJob
parameter_list|(
name|int
name|iteration
parameter_list|)
throws|throws
name|Exception
block|{
name|String
name|jobName
init|=
name|IntegrationTestSparkBulkLoad
operator|.
name|class
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|" _load "
operator|+
name|EnvironmentEdgeManager
operator|.
name|currentTime
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Running iteration "
operator|+
name|iteration
operator|+
literal|"in Spark Job"
argument_list|)
expr_stmt|;
name|Path
name|output
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|conf
operator|.
name|get
argument_list|(
name|BULKLOAD_OUTPUT_PATH
argument_list|)
operator|==
literal|null
condition|)
block|{
name|output
operator|=
name|util
operator|.
name|getDataTestDirOnTestFS
argument_list|(
name|getTablename
argument_list|()
operator|+
literal|"-"
operator|+
name|iteration
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|output
operator|=
operator|new
name|Path
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|BULKLOAD_OUTPUT_PATH
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|SparkConf
name|sparkConf
init|=
operator|new
name|SparkConf
argument_list|()
operator|.
name|setAppName
argument_list|(
name|jobName
argument_list|)
operator|.
name|setMaster
argument_list|(
literal|"local"
argument_list|)
decl_stmt|;
name|Configuration
name|hbaseConf
init|=
operator|new
name|Configuration
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
name|hbaseConf
operator|.
name|setInt
argument_list|(
name|CURRENT_ROUND_NUM
argument_list|,
name|iteration
argument_list|)
expr_stmt|;
name|int
name|partitionNum
init|=
name|hbaseConf
operator|.
name|getInt
argument_list|(
name|BULKLOAD_PARTITIONS_NUM
argument_list|,
name|DEFAULT_BULKLOAD_PARTITIONS_NUM
argument_list|)
decl_stmt|;
name|JavaSparkContext
name|jsc
init|=
operator|new
name|JavaSparkContext
argument_list|(
name|sparkConf
argument_list|)
decl_stmt|;
name|JavaHBaseContext
name|hbaseContext
init|=
operator|new
name|JavaHBaseContext
argument_list|(
name|jsc
argument_list|,
name|hbaseConf
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Partition RDD into "
operator|+
name|partitionNum
operator|+
literal|" parts"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|temp
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|JavaRDD
argument_list|<
name|List
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|>
name|rdd
init|=
name|jsc
operator|.
name|parallelize
argument_list|(
name|temp
argument_list|,
name|partitionNum
argument_list|)
operator|.
name|mapPartitionsWithIndex
argument_list|(
operator|new
name|LinkedListCreationMapper
argument_list|(
operator|new
name|SerializableWritable
argument_list|<>
argument_list|(
name|hbaseConf
argument_list|)
argument_list|)
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|hbaseContext
operator|.
name|bulkLoad
argument_list|(
name|rdd
argument_list|,
name|getTablename
argument_list|()
argument_list|,
operator|new
name|ListToKeyValueFunc
argument_list|()
argument_list|,
name|output
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
operator|new
name|HashMap
argument_list|<>
argument_list|()
argument_list|,
literal|false
argument_list|,
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
argument_list|)
expr_stmt|;
try|try
init|(
name|Connection
name|conn
init|=
name|ConnectionFactory
operator|.
name|createConnection
argument_list|(
name|conf
argument_list|)
init|;
name|Admin
name|admin
operator|=
name|conn
operator|.
name|getAdmin
argument_list|()
init|;
name|Table
name|table
operator|=
name|conn
operator|.
name|getTable
argument_list|(
name|getTablename
argument_list|()
argument_list|)
init|;
name|RegionLocator
name|regionLocator
operator|=
name|conn
operator|.
name|getRegionLocator
argument_list|(
name|getTablename
argument_list|()
argument_list|)
init|)
block|{
comment|// Create a new loader.
name|LoadIncrementalHFiles
name|loader
init|=
operator|new
name|LoadIncrementalHFiles
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// Load the HFiles into table.
name|loader
operator|.
name|doBulkLoad
argument_list|(
name|output
argument_list|,
name|admin
argument_list|,
name|table
argument_list|,
name|regionLocator
argument_list|)
expr_stmt|;
block|}
comment|// Delete the files.
name|util
operator|.
name|getTestFileSystem
argument_list|()
operator|.
name|delete
argument_list|(
name|output
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|jsc
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// See mapreduce.IntegrationTestBulkLoad#LinkedListCreationMapper
comment|// Used to generate test data
specifier|public
specifier|static
class|class
name|LinkedListCreationMapper
implements|implements
name|Function2
argument_list|<
name|Integer
argument_list|,
name|Iterator
argument_list|<
name|String
argument_list|>
argument_list|,
name|Iterator
argument_list|<
name|List
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|>
argument_list|>
block|{
name|SerializableWritable
name|swConfig
init|=
literal|null
decl_stmt|;
specifier|private
name|Random
name|rand
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
specifier|public
name|LinkedListCreationMapper
parameter_list|(
name|SerializableWritable
name|conf
parameter_list|)
block|{
name|this
operator|.
name|swConfig
operator|=
name|conf
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Iterator
argument_list|<
name|List
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|>
name|call
parameter_list|(
name|Integer
name|v1
parameter_list|,
name|Iterator
name|v2
parameter_list|)
throws|throws
name|Exception
block|{
name|Configuration
name|config
init|=
operator|(
name|Configuration
operator|)
name|swConfig
operator|.
name|value
argument_list|()
decl_stmt|;
name|int
name|partitionId
init|=
name|v1
operator|.
name|intValue
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Starting create List in Partition "
operator|+
name|partitionId
argument_list|)
expr_stmt|;
name|int
name|partitionNum
init|=
name|config
operator|.
name|getInt
argument_list|(
name|BULKLOAD_PARTITIONS_NUM
argument_list|,
name|DEFAULT_BULKLOAD_PARTITIONS_NUM
argument_list|)
decl_stmt|;
name|int
name|chainLength
init|=
name|config
operator|.
name|getInt
argument_list|(
name|BULKLOAD_CHAIN_LENGTH
argument_list|,
name|DEFAULT_BULKLOAD_CHAIN_LENGTH
argument_list|)
decl_stmt|;
name|int
name|iterationsNum
init|=
name|config
operator|.
name|getInt
argument_list|(
name|BULKLOAD_IMPORT_ROUNDS
argument_list|,
name|DEFAULT_BULKLOAD_IMPORT_ROUNDS
argument_list|)
decl_stmt|;
name|int
name|iterationsCur
init|=
name|config
operator|.
name|getInt
argument_list|(
name|CURRENT_ROUND_NUM
argument_list|,
literal|0
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|List
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|>
name|res
init|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
decl_stmt|;
name|long
name|tempId
init|=
name|partitionId
operator|+
name|iterationsCur
operator|*
name|partitionNum
decl_stmt|;
name|long
name|totalPartitionNum
init|=
name|partitionNum
operator|*
name|iterationsNum
decl_stmt|;
name|long
name|chainId
init|=
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
decl_stmt|;
name|chainId
operator|=
name|chainId
operator|-
operator|(
name|chainId
operator|%
name|totalPartitionNum
operator|)
operator|+
name|tempId
expr_stmt|;
name|byte
index|[]
name|chainIdArray
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|chainId
argument_list|)
decl_stmt|;
name|long
name|currentRow
init|=
literal|0
decl_stmt|;
name|long
name|nextRow
init|=
name|getNextRow
argument_list|(
literal|0
argument_list|,
name|chainLength
argument_list|)
decl_stmt|;
for|for
control|(
name|long
name|i
init|=
literal|0
init|;
name|i
operator|<
name|chainLength
condition|;
name|i
operator|++
control|)
block|{
name|byte
index|[]
name|rk
init|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|currentRow
argument_list|)
decl_stmt|;
comment|// Insert record into a list
name|List
argument_list|<
name|byte
index|[]
argument_list|>
name|tmp1
init|=
name|Arrays
operator|.
name|asList
argument_list|(
name|rk
argument_list|,
name|CHAIN_FAM
argument_list|,
name|chainIdArray
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|nextRow
argument_list|)
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|byte
index|[]
argument_list|>
name|tmp2
init|=
name|Arrays
operator|.
name|asList
argument_list|(
name|rk
argument_list|,
name|SORT_FAM
argument_list|,
name|chainIdArray
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|i
argument_list|)
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|byte
index|[]
argument_list|>
name|tmp3
init|=
name|Arrays
operator|.
name|asList
argument_list|(
name|rk
argument_list|,
name|DATA_FAM
argument_list|,
name|chainIdArray
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|RandomStringUtils
operator|.
name|randomAlphabetic
argument_list|(
literal|50
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
name|res
operator|.
name|add
argument_list|(
name|tmp1
argument_list|)
expr_stmt|;
name|res
operator|.
name|add
argument_list|(
name|tmp2
argument_list|)
expr_stmt|;
name|res
operator|.
name|add
argument_list|(
name|tmp3
argument_list|)
expr_stmt|;
name|currentRow
operator|=
name|nextRow
expr_stmt|;
name|nextRow
operator|=
name|getNextRow
argument_list|(
name|i
operator|+
literal|1
argument_list|,
name|chainLength
argument_list|)
expr_stmt|;
block|}
return|return
name|res
operator|.
name|iterator
argument_list|()
return|;
block|}
comment|/** Returns a unique row id within this chain for this index */
specifier|private
name|long
name|getNextRow
parameter_list|(
name|long
name|index
parameter_list|,
name|long
name|chainLength
parameter_list|)
block|{
name|long
name|nextRow
init|=
name|Math
operator|.
name|abs
argument_list|(
operator|new
name|Random
argument_list|()
operator|.
name|nextLong
argument_list|()
argument_list|)
decl_stmt|;
comment|// use significant bits from the random number, but pad with index to ensure it is unique
comment|// this also ensures that we do not reuse row = 0
comment|// row collisions from multiple mappers are fine, since we guarantee unique chainIds
name|nextRow
operator|=
name|nextRow
operator|-
operator|(
name|nextRow
operator|%
name|chainLength
operator|)
operator|+
name|index
expr_stmt|;
return|return
name|nextRow
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|ListToKeyValueFunc
implements|implements
name|Function
argument_list|<
name|List
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|,
name|Pair
argument_list|<
name|KeyFamilyQualifier
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|Pair
argument_list|<
name|KeyFamilyQualifier
argument_list|,
name|byte
index|[]
argument_list|>
name|call
parameter_list|(
name|List
argument_list|<
name|byte
index|[]
argument_list|>
name|v1
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|v1
operator|==
literal|null
operator|||
name|v1
operator|.
name|size
argument_list|()
operator|!=
literal|4
condition|)
block|{
return|return
literal|null
return|;
block|}
name|KeyFamilyQualifier
name|kfq
init|=
operator|new
name|KeyFamilyQualifier
argument_list|(
name|v1
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|v1
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|v1
operator|.
name|get
argument_list|(
literal|2
argument_list|)
argument_list|)
decl_stmt|;
return|return
operator|new
name|Pair
argument_list|<>
argument_list|(
name|kfq
argument_list|,
name|v1
operator|.
name|get
argument_list|(
literal|3
argument_list|)
argument_list|)
return|;
block|}
block|}
comment|/**    * After adding data to the table start a mr job to    * @throws IOException    * @throws ClassNotFoundException    * @throws InterruptedException    */
specifier|public
name|void
name|runCheck
parameter_list|()
throws|throws
name|Exception
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Running check"
argument_list|)
expr_stmt|;
name|String
name|jobName
init|=
name|IntegrationTestSparkBulkLoad
operator|.
name|class
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|"_check"
operator|+
name|EnvironmentEdgeManager
operator|.
name|currentTime
argument_list|()
decl_stmt|;
name|SparkConf
name|sparkConf
init|=
operator|new
name|SparkConf
argument_list|()
operator|.
name|setAppName
argument_list|(
name|jobName
argument_list|)
operator|.
name|setMaster
argument_list|(
literal|"local"
argument_list|)
decl_stmt|;
name|Configuration
name|hbaseConf
init|=
operator|new
name|Configuration
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
name|JavaSparkContext
name|jsc
init|=
operator|new
name|JavaSparkContext
argument_list|(
name|sparkConf
argument_list|)
decl_stmt|;
name|JavaHBaseContext
name|hbaseContext
init|=
operator|new
name|JavaHBaseContext
argument_list|(
name|jsc
argument_list|,
name|hbaseConf
argument_list|)
decl_stmt|;
name|Scan
name|scan
init|=
operator|new
name|Scan
argument_list|()
decl_stmt|;
name|scan
operator|.
name|addFamily
argument_list|(
name|CHAIN_FAM
argument_list|)
expr_stmt|;
name|scan
operator|.
name|addFamily
argument_list|(
name|SORT_FAM
argument_list|)
expr_stmt|;
name|scan
operator|.
name|setMaxVersions
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|scan
operator|.
name|setCacheBlocks
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|scan
operator|.
name|setBatch
argument_list|(
literal|1000
argument_list|)
expr_stmt|;
name|int
name|replicaCount
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|NUM_REPLICA_COUNT_KEY
argument_list|,
name|DEFAULT_NUM_REPLICA_COUNT
argument_list|)
decl_stmt|;
if|if
condition|(
name|replicaCount
operator|!=
name|DEFAULT_NUM_REPLICA_COUNT
condition|)
block|{
name|scan
operator|.
name|setConsistency
argument_list|(
name|Consistency
operator|.
name|TIMELINE
argument_list|)
expr_stmt|;
block|}
comment|// 1. Using TableInputFormat to get data from HBase table
comment|// 2. Mimic LinkedListCheckingMapper in mapreduce.IntegrationTestBulkLoad
comment|// 3. Sort LinkKey by its order ID
comment|// 4. Group LinkKey if they have same chainId, and repartition RDD by NaturalKeyPartitioner
comment|// 5. Check LinkList in each Partition using LinkedListCheckingFlatMapFunc
name|hbaseContext
operator|.
name|hbaseRDD
argument_list|(
name|getTablename
argument_list|()
argument_list|,
name|scan
argument_list|)
operator|.
name|flatMapToPair
argument_list|(
operator|new
name|LinkedListCheckingFlatMapFunc
argument_list|()
argument_list|)
operator|.
name|sortByKey
argument_list|()
operator|.
name|combineByKey
argument_list|(
operator|new
name|createCombinerFunc
argument_list|()
argument_list|,
operator|new
name|mergeValueFunc
argument_list|()
argument_list|,
operator|new
name|mergeCombinersFunc
argument_list|()
argument_list|,
operator|new
name|NaturalKeyPartitioner
argument_list|(
operator|new
name|SerializableWritable
argument_list|<>
argument_list|(
name|hbaseConf
argument_list|)
argument_list|)
argument_list|)
operator|.
name|foreach
argument_list|(
operator|new
name|LinkedListCheckingForeachFunc
argument_list|(
operator|new
name|SerializableWritable
argument_list|<>
argument_list|(
name|hbaseConf
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
name|jsc
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
specifier|private
name|void
name|runCheckWithRetry
parameter_list|()
throws|throws
name|Exception
block|{
try|try
block|{
name|runCheck
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Received "
operator|+
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"Running the check MR Job again to see whether an ephemeral problem or not"
argument_list|)
expr_stmt|;
name|runCheck
argument_list|()
expr_stmt|;
throw|throw
name|t
throw|;
comment|// we should still fail the test even if second retry succeeds
block|}
comment|// everything green
block|}
comment|/**    * PairFlatMapFunction used to transfer<Row, Result> to Tuple<SparkLinkKey, SparkLinkChain>    */
specifier|public
specifier|static
class|class
name|LinkedListCheckingFlatMapFunc
implements|implements
name|PairFlatMapFunction
argument_list|<
name|Tuple2
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|Result
argument_list|>
argument_list|,
name|SparkLinkKey
argument_list|,
name|SparkLinkChain
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|Iterable
argument_list|<
name|Tuple2
argument_list|<
name|SparkLinkKey
argument_list|,
name|SparkLinkChain
argument_list|>
argument_list|>
name|call
parameter_list|(
name|Tuple2
argument_list|<
name|ImmutableBytesWritable
argument_list|,
name|Result
argument_list|>
name|v
parameter_list|)
throws|throws
name|Exception
block|{
name|Result
name|value
init|=
name|v
operator|.
name|_2
argument_list|()
decl_stmt|;
name|long
name|longRk
init|=
name|Bytes
operator|.
name|toLong
argument_list|(
name|value
operator|.
name|getRow
argument_list|()
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Tuple2
argument_list|<
name|SparkLinkKey
argument_list|,
name|SparkLinkChain
argument_list|>
argument_list|>
name|list
init|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|entry
range|:
name|value
operator|.
name|getFamilyMap
argument_list|(
name|CHAIN_FAM
argument_list|)
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|long
name|chainId
init|=
name|Bytes
operator|.
name|toLong
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|)
decl_stmt|;
name|long
name|next
init|=
name|Bytes
operator|.
name|toLong
argument_list|(
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
decl_stmt|;
name|Cell
name|c
init|=
name|value
operator|.
name|getColumnCells
argument_list|(
name|SORT_FAM
argument_list|,
name|entry
operator|.
name|getKey
argument_list|()
argument_list|)
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|long
name|order
init|=
name|Bytes
operator|.
name|toLong
argument_list|(
name|CellUtil
operator|.
name|cloneValue
argument_list|(
name|c
argument_list|)
argument_list|)
decl_stmt|;
name|Tuple2
argument_list|<
name|SparkLinkKey
argument_list|,
name|SparkLinkChain
argument_list|>
name|tuple2
init|=
operator|new
name|Tuple2
argument_list|<>
argument_list|(
operator|new
name|SparkLinkKey
argument_list|(
name|chainId
argument_list|,
name|order
argument_list|)
argument_list|,
operator|new
name|SparkLinkChain
argument_list|(
name|longRk
argument_list|,
name|next
argument_list|)
argument_list|)
decl_stmt|;
name|list
operator|.
name|add
argument_list|(
name|tuple2
argument_list|)
expr_stmt|;
block|}
return|return
name|list
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|createCombinerFunc
implements|implements
name|Function
argument_list|<
name|SparkLinkChain
argument_list|,
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|call
parameter_list|(
name|SparkLinkChain
name|v1
parameter_list|)
throws|throws
name|Exception
block|{
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|list
init|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
decl_stmt|;
name|list
operator|.
name|add
argument_list|(
name|v1
argument_list|)
expr_stmt|;
return|return
name|list
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|mergeValueFunc
implements|implements
name|Function2
argument_list|<
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|,
name|SparkLinkChain
argument_list|,
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|call
parameter_list|(
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|v1
parameter_list|,
name|SparkLinkChain
name|v2
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|v1
operator|==
literal|null
condition|)
name|v1
operator|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
expr_stmt|;
name|v1
operator|.
name|add
argument_list|(
name|v2
argument_list|)
expr_stmt|;
return|return
name|v1
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|mergeCombinersFunc
implements|implements
name|Function2
argument_list|<
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|,
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|,
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|call
parameter_list|(
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|v1
parameter_list|,
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|v2
parameter_list|)
throws|throws
name|Exception
block|{
name|v1
operator|.
name|addAll
argument_list|(
name|v2
argument_list|)
expr_stmt|;
return|return
name|v1
return|;
block|}
block|}
comment|/**    * Class to figure out what partition to send a link in the chain to.  This is based upon    * the linkKey's ChainId.    */
specifier|public
specifier|static
class|class
name|NaturalKeyPartitioner
extends|extends
name|Partitioner
block|{
specifier|private
name|int
name|numPartions
init|=
literal|0
decl_stmt|;
specifier|public
name|NaturalKeyPartitioner
parameter_list|(
name|SerializableWritable
name|swConf
parameter_list|)
block|{
name|Configuration
name|hbaseConf
init|=
operator|(
name|Configuration
operator|)
name|swConf
operator|.
name|value
argument_list|()
decl_stmt|;
name|numPartions
operator|=
name|hbaseConf
operator|.
name|getInt
argument_list|(
name|BULKLOAD_PARTITIONS_NUM
argument_list|,
name|DEFAULT_BULKLOAD_PARTITIONS_NUM
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|numPartitions
parameter_list|()
block|{
return|return
name|numPartions
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|getPartition
parameter_list|(
name|Object
name|key
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|key
operator|instanceof
name|SparkLinkKey
operator|)
condition|)
return|return
operator|-
literal|1
return|;
name|int
name|hash
init|=
operator|(
operator|(
name|SparkLinkKey
operator|)
name|key
operator|)
operator|.
name|getChainId
argument_list|()
operator|.
name|hashCode
argument_list|()
decl_stmt|;
return|return
name|Math
operator|.
name|abs
argument_list|(
name|hash
operator|%
name|numPartions
argument_list|)
return|;
block|}
block|}
comment|/**    * Sort all LinkChain for one LinkKey, and test List<LinkChain>    */
specifier|public
specifier|static
class|class
name|LinkedListCheckingForeachFunc
implements|implements
name|VoidFunction
argument_list|<
name|Tuple2
argument_list|<
name|SparkLinkKey
argument_list|,
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|>
argument_list|>
block|{
specifier|private
name|SerializableWritable
name|swConf
init|=
literal|null
decl_stmt|;
specifier|public
name|LinkedListCheckingForeachFunc
parameter_list|(
name|SerializableWritable
name|conf
parameter_list|)
block|{
name|swConf
operator|=
name|conf
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|call
parameter_list|(
name|Tuple2
argument_list|<
name|SparkLinkKey
argument_list|,
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
argument_list|>
name|v1
parameter_list|)
throws|throws
name|Exception
block|{
name|long
name|next
init|=
operator|-
literal|1L
decl_stmt|;
name|long
name|prev
init|=
operator|-
literal|1L
decl_stmt|;
name|long
name|count
init|=
literal|0L
decl_stmt|;
name|SparkLinkKey
name|key
init|=
name|v1
operator|.
name|_1
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|SparkLinkChain
argument_list|>
name|values
init|=
name|v1
operator|.
name|_2
argument_list|()
decl_stmt|;
for|for
control|(
name|SparkLinkChain
name|lc
range|:
name|values
control|)
block|{
if|if
condition|(
name|next
operator|==
operator|-
literal|1
condition|)
block|{
if|if
condition|(
name|lc
operator|.
name|getRk
argument_list|()
operator|!=
literal|0L
condition|)
block|{
name|String
name|msg
init|=
literal|"Chains should all start at rk 0, but read rk "
operator|+
name|lc
operator|.
name|getRk
argument_list|()
operator|+
literal|". Chain:"
operator|+
name|key
operator|.
name|getChainId
argument_list|()
operator|+
literal|", order:"
operator|+
name|key
operator|.
name|getOrder
argument_list|()
decl_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
name|next
operator|=
name|lc
operator|.
name|getNext
argument_list|()
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|next
operator|!=
name|lc
operator|.
name|getRk
argument_list|()
condition|)
block|{
name|String
name|msg
init|=
literal|"Missing a link in the chain. Prev rk "
operator|+
name|prev
operator|+
literal|" was, expecting "
operator|+
name|next
operator|+
literal|" but got "
operator|+
name|lc
operator|.
name|getRk
argument_list|()
operator|+
literal|". Chain:"
operator|+
name|key
operator|.
name|getChainId
argument_list|()
operator|+
literal|", order:"
operator|+
name|key
operator|.
name|getOrder
argument_list|()
decl_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
name|prev
operator|=
name|lc
operator|.
name|getRk
argument_list|()
expr_stmt|;
name|next
operator|=
name|lc
operator|.
name|getNext
argument_list|()
expr_stmt|;
block|}
name|count
operator|++
expr_stmt|;
block|}
name|Configuration
name|hbaseConf
init|=
operator|(
name|Configuration
operator|)
name|swConf
operator|.
name|value
argument_list|()
decl_stmt|;
name|int
name|expectedChainLen
init|=
name|hbaseConf
operator|.
name|getInt
argument_list|(
name|BULKLOAD_CHAIN_LENGTH
argument_list|,
name|DEFAULT_BULKLOAD_CHAIN_LENGTH
argument_list|)
decl_stmt|;
if|if
condition|(
name|count
operator|!=
name|expectedChainLen
condition|)
block|{
name|String
name|msg
init|=
literal|"Chain wasn't the correct length.  Expected "
operator|+
name|expectedChainLen
operator|+
literal|" got "
operator|+
name|count
operator|+
literal|". Chain:"
operator|+
name|key
operator|.
name|getChainId
argument_list|()
operator|+
literal|", order:"
operator|+
name|key
operator|.
name|getOrder
argument_list|()
decl_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Writable class used as the key to group links in the linked list.    *    * Used as the key emited from a pass over the table.    */
specifier|public
specifier|static
class|class
name|SparkLinkKey
implements|implements
name|java
operator|.
name|io
operator|.
name|Serializable
implements|,
name|Comparable
argument_list|<
name|SparkLinkKey
argument_list|>
block|{
specifier|private
name|Long
name|chainId
decl_stmt|;
specifier|private
name|Long
name|order
decl_stmt|;
specifier|public
name|Long
name|getOrder
parameter_list|()
block|{
return|return
name|order
return|;
block|}
specifier|public
name|Long
name|getChainId
parameter_list|()
block|{
return|return
name|chainId
return|;
block|}
specifier|public
name|SparkLinkKey
parameter_list|(
name|long
name|chainId
parameter_list|,
name|long
name|order
parameter_list|)
block|{
name|this
operator|.
name|chainId
operator|=
name|chainId
expr_stmt|;
name|this
operator|.
name|order
operator|=
name|order
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|hashCode
parameter_list|()
block|{
return|return
name|this
operator|.
name|getChainId
argument_list|()
operator|.
name|hashCode
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|equals
parameter_list|(
name|Object
name|other
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|other
operator|instanceof
name|SparkLinkKey
operator|)
condition|)
return|return
literal|false
return|;
name|SparkLinkKey
name|otherKey
init|=
operator|(
name|SparkLinkKey
operator|)
name|other
decl_stmt|;
return|return
name|this
operator|.
name|getChainId
argument_list|()
operator|.
name|equals
argument_list|(
name|otherKey
operator|.
name|getChainId
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|SparkLinkKey
name|other
parameter_list|)
block|{
name|int
name|res
init|=
name|getChainId
argument_list|()
operator|.
name|compareTo
argument_list|(
name|other
operator|.
name|getChainId
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|res
operator|==
literal|0
condition|)
name|res
operator|=
name|getOrder
argument_list|()
operator|.
name|compareTo
argument_list|(
name|other
operator|.
name|getOrder
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|res
return|;
block|}
block|}
comment|/**    * Writable used as the value emitted from a pass over the hbase table.    */
specifier|public
specifier|static
class|class
name|SparkLinkChain
implements|implements
name|java
operator|.
name|io
operator|.
name|Serializable
implements|,
name|Comparable
argument_list|<
name|SparkLinkChain
argument_list|>
block|{
specifier|public
name|Long
name|getNext
parameter_list|()
block|{
return|return
name|next
return|;
block|}
specifier|public
name|Long
name|getRk
parameter_list|()
block|{
return|return
name|rk
return|;
block|}
specifier|public
name|SparkLinkChain
parameter_list|(
name|Long
name|rk
parameter_list|,
name|Long
name|next
parameter_list|)
block|{
name|this
operator|.
name|rk
operator|=
name|rk
expr_stmt|;
name|this
operator|.
name|next
operator|=
name|next
expr_stmt|;
block|}
specifier|private
name|Long
name|rk
decl_stmt|;
specifier|private
name|Long
name|next
decl_stmt|;
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|SparkLinkChain
name|linkChain
parameter_list|)
block|{
name|int
name|res
init|=
name|getRk
argument_list|()
operator|.
name|compareTo
argument_list|(
name|linkChain
operator|.
name|getRk
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|res
operator|==
literal|0
condition|)
block|{
name|res
operator|=
name|getNext
argument_list|()
operator|.
name|compareTo
argument_list|(
name|linkChain
operator|.
name|getNext
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|res
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|hashCode
parameter_list|()
block|{
return|return
name|getRk
argument_list|()
operator|.
name|hashCode
argument_list|()
operator|^
name|getNext
argument_list|()
operator|.
name|hashCode
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|equals
parameter_list|(
name|Object
name|other
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|other
operator|instanceof
name|SparkLinkChain
operator|)
condition|)
return|return
literal|false
return|;
name|SparkLinkChain
name|otherKey
init|=
operator|(
name|SparkLinkChain
operator|)
name|other
decl_stmt|;
return|return
name|this
operator|.
name|getRk
argument_list|()
operator|.
name|equals
argument_list|(
name|otherKey
operator|.
name|getRk
argument_list|()
argument_list|)
operator|&&
name|this
operator|.
name|getNext
argument_list|()
operator|.
name|equals
argument_list|(
name|otherKey
operator|.
name|getNext
argument_list|()
argument_list|)
return|;
block|}
block|}
comment|/**    * Allow the scan to go to replica, this would not affect the runCheck()    * Since data are BulkLoaded from HFile into table    * @throws IOException    * @throws InterruptedException    */
specifier|private
name|void
name|installSlowingCoproc
parameter_list|()
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
name|int
name|replicaCount
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|NUM_REPLICA_COUNT_KEY
argument_list|,
name|DEFAULT_NUM_REPLICA_COUNT
argument_list|)
decl_stmt|;
if|if
condition|(
name|replicaCount
operator|==
name|DEFAULT_NUM_REPLICA_COUNT
condition|)
return|return;
name|TableName
name|t
init|=
name|getTablename
argument_list|()
decl_stmt|;
name|Admin
name|admin
init|=
name|util
operator|.
name|getAdmin
argument_list|()
decl_stmt|;
name|HTableDescriptor
name|desc
init|=
name|admin
operator|.
name|getTableDescriptor
argument_list|(
name|t
argument_list|)
decl_stmt|;
name|desc
operator|.
name|addCoprocessor
argument_list|(
name|IntegrationTestBulkLoad
operator|.
name|SlowMeCoproScanOperations
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|HBaseTestingUtility
operator|.
name|modifyTableSync
argument_list|(
name|admin
argument_list|,
name|desc
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Test
specifier|public
name|void
name|testBulkLoad
parameter_list|()
throws|throws
name|Exception
block|{
name|runLoad
argument_list|()
expr_stmt|;
name|installSlowingCoproc
argument_list|()
expr_stmt|;
name|runCheckWithRetry
argument_list|()
expr_stmt|;
block|}
specifier|private
name|byte
index|[]
index|[]
name|getSplits
parameter_list|(
name|int
name|numRegions
parameter_list|)
block|{
name|RegionSplitter
operator|.
name|UniformSplit
name|split
init|=
operator|new
name|RegionSplitter
operator|.
name|UniformSplit
argument_list|()
decl_stmt|;
name|split
operator|.
name|setFirstRow
argument_list|(
name|Bytes
operator|.
name|toBytes
argument_list|(
literal|0L
argument_list|)
argument_list|)
expr_stmt|;
name|split
operator|.
name|setLastRow
argument_list|(
name|Bytes
operator|.
name|toBytes
argument_list|(
name|Long
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|split
operator|.
name|split
argument_list|(
name|numRegions
argument_list|)
return|;
block|}
specifier|private
name|void
name|setupTable
parameter_list|()
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
if|if
condition|(
name|util
operator|.
name|getAdmin
argument_list|()
operator|.
name|tableExists
argument_list|(
name|getTablename
argument_list|()
argument_list|)
condition|)
block|{
name|util
operator|.
name|deleteTable
argument_list|(
name|getTablename
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|util
operator|.
name|createTable
argument_list|(
name|getTablename
argument_list|()
argument_list|,
operator|new
name|byte
index|[]
index|[]
block|{
name|CHAIN_FAM
block|,
name|SORT_FAM
block|,
name|DATA_FAM
block|}
argument_list|,
name|getSplits
argument_list|(
literal|16
argument_list|)
argument_list|)
expr_stmt|;
name|int
name|replicaCount
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|NUM_REPLICA_COUNT_KEY
argument_list|,
name|DEFAULT_NUM_REPLICA_COUNT
argument_list|)
decl_stmt|;
if|if
condition|(
name|replicaCount
operator|==
name|DEFAULT_NUM_REPLICA_COUNT
condition|)
return|return;
name|TableName
name|t
init|=
name|getTablename
argument_list|()
decl_stmt|;
name|HBaseTestingUtility
operator|.
name|setReplicas
argument_list|(
name|util
operator|.
name|getAdmin
argument_list|()
argument_list|,
name|t
argument_list|,
name|replicaCount
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|setUpCluster
parameter_list|()
throws|throws
name|Exception
block|{
name|util
operator|=
name|getTestingUtil
argument_list|(
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|util
operator|.
name|initializeCluster
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|int
name|replicaCount
init|=
name|getConf
argument_list|()
operator|.
name|getInt
argument_list|(
name|NUM_REPLICA_COUNT_KEY
argument_list|,
name|DEFAULT_NUM_REPLICA_COUNT
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
operator|&&
name|replicaCount
operator|!=
name|DEFAULT_NUM_REPLICA_COUNT
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Region Replicas enabled: "
operator|+
name|replicaCount
argument_list|)
expr_stmt|;
block|}
comment|// Scale this up on a real cluster
if|if
condition|(
name|util
operator|.
name|isDistributedCluster
argument_list|()
condition|)
block|{
name|util
operator|.
name|getConfiguration
argument_list|()
operator|.
name|setIfUnset
argument_list|(
name|BULKLOAD_PARTITIONS_NUM
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|DEFAULT_BULKLOAD_PARTITIONS_NUM
argument_list|)
argument_list|)
expr_stmt|;
name|util
operator|.
name|getConfiguration
argument_list|()
operator|.
name|setIfUnset
argument_list|(
name|BULKLOAD_IMPORT_ROUNDS
argument_list|,
literal|"1"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|util
operator|.
name|startMiniMapReduceCluster
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addOptions
parameter_list|()
block|{
name|super
operator|.
name|addOptions
argument_list|()
expr_stmt|;
name|super
operator|.
name|addOptNoArg
argument_list|(
name|OPT_CHECK
argument_list|,
literal|"Run check only"
argument_list|)
expr_stmt|;
name|super
operator|.
name|addOptNoArg
argument_list|(
name|OPT_LOAD
argument_list|,
literal|"Run load only"
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|processOptions
parameter_list|(
name|CommandLine
name|cmd
parameter_list|)
block|{
name|super
operator|.
name|processOptions
argument_list|(
name|cmd
argument_list|)
expr_stmt|;
name|check
operator|=
name|cmd
operator|.
name|hasOption
argument_list|(
name|OPT_CHECK
argument_list|)
expr_stmt|;
name|load
operator|=
name|cmd
operator|.
name|hasOption
argument_list|(
name|OPT_LOAD
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|runTestFromCommandLine
parameter_list|()
throws|throws
name|Exception
block|{
if|if
condition|(
name|load
condition|)
block|{
name|runLoad
argument_list|()
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|check
condition|)
block|{
name|installSlowingCoproc
argument_list|()
expr_stmt|;
name|runCheckWithRetry
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|testBulkLoad
argument_list|()
expr_stmt|;
block|}
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|TableName
name|getTablename
parameter_list|()
block|{
return|return
name|getTableName
argument_list|(
name|getConf
argument_list|()
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|TableName
name|getTableName
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|TableName
operator|.
name|valueOf
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|BULKLOAD_TABLE_NAME
argument_list|,
name|DEFAULT_BULKLOAD_TABLE_NAME
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|Set
argument_list|<
name|String
argument_list|>
name|getColumnFamilies
parameter_list|()
block|{
return|return
name|Sets
operator|.
name|newHashSet
argument_list|(
name|Bytes
operator|.
name|toString
argument_list|(
name|CHAIN_FAM
argument_list|)
argument_list|,
name|Bytes
operator|.
name|toString
argument_list|(
name|DATA_FAM
argument_list|)
argument_list|,
name|Bytes
operator|.
name|toString
argument_list|(
name|SORT_FAM
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|Configuration
name|conf
init|=
name|HBaseConfiguration
operator|.
name|create
argument_list|()
decl_stmt|;
name|IntegrationTestingUtility
operator|.
name|setUseDistributedCluster
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|int
name|status
init|=
name|ToolRunner
operator|.
name|run
argument_list|(
name|conf
argument_list|,
operator|new
name|IntegrationTestSparkBulkLoad
argument_list|()
argument_list|,
name|args
argument_list|)
decl_stmt|;
name|System
operator|.
name|exit
argument_list|(
name|status
argument_list|)
expr_stmt|;
block|}
block|}
end_class

end_unit

