begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|codec
operator|.
name|prefixtree
operator|.
name|encode
operator|.
name|tokenize
package|;
end_package

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|classification
operator|.
name|InterfaceAudience
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ByteRange
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ByteRangeUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|CollectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|SimpleMutableByteRange
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Strings
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|shaded
operator|.
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_comment
comment|/**  * Individual node in a Trie structure.  Each node is one of 3 types:  *<ul>  *<li>Branch: an internal trie node that may have a token and must have multiple children, but does  * not represent an actual input byte[], hence its numOccurrences is 0  *<li>Leaf: a node with no children and where numOccurrences is&gt;= 1.  It's token represents the  * last bytes in the input byte[]s.  *<li>Nub: a combination of a branch and leaf.  Its token represents the last bytes of input  * byte[]s and has numOccurrences&gt;= 1, but it also has child nodes which represent input byte[]s  * that add bytes to this nodes input byte[].  *</ul>  *<br><br>  * Example inputs (numInputs=7):  * 0: AAA  * 1: AAA  * 2: AAB  * 3: AAB  * 4: AAB  * 5: AABQQ  * 6: AABQQ  *<br><br>  * Resulting TokenizerNodes:  * AA&lt;- branch, numOccurrences=0, tokenStartOffset=0, token.length=2  * A&lt;- leaf, numOccurrences=2, tokenStartOffset=2, token.length=1  * B&lt;- nub, numOccurrences=3, tokenStartOffset=2, token.length=1  * QQ&lt;- leaf, numOccurrences=2, tokenStartOffset=3, token.length=2  *<br><br>  * numInputs == 7 == sum(numOccurrences) == 0 + 2 + 3 + 2  */
end_comment

begin_class
annotation|@
name|InterfaceAudience
operator|.
name|Private
specifier|public
class|class
name|TokenizerNode
block|{
comment|/*    * Ref to data structure wrapper    */
specifier|protected
name|Tokenizer
name|builder
decl_stmt|;
comment|/******************************************************************     * Tree content/structure used during tokenization     * ****************************************************************/
comment|/*    * ref to parent trie node    */
specifier|protected
name|TokenizerNode
name|parent
decl_stmt|;
comment|/*    * node depth in trie, irrespective of each node's token length    */
specifier|protected
name|int
name|nodeDepth
decl_stmt|;
comment|/*    * start index of this token in original byte[]    */
specifier|protected
name|int
name|tokenStartOffset
decl_stmt|;
comment|/*    * bytes for this trie node.  can be length 0 in root node    */
specifier|protected
name|ByteRange
name|token
decl_stmt|;
comment|/*    * A count of occurrences in the input byte[]s, not the trie structure. 0 for branch nodes, 1+ for    * nubs and leaves. If the same byte[] is added to the trie multiple times, this is the only thing    * that changes in the tokenizer. As a result, duplicate byte[]s are very inexpensive to encode.    */
specifier|protected
name|int
name|numOccurrences
decl_stmt|;
comment|/*    * The maximum fan-out of a byte[] trie is 256, so there are a maximum of 256    * child nodes.    */
specifier|protected
name|ArrayList
argument_list|<
name|TokenizerNode
argument_list|>
name|children
decl_stmt|;
comment|/*    * Fields used later in the encoding process for sorting the nodes into the order they'll be    * written to the output byte[].  With these fields, the TokenizerNode and therefore Tokenizer    * are not generic data structures but instead are specific to HBase PrefixTree encoding.     */
comment|/*    * unique id assigned to each TokenizerNode    */
specifier|protected
name|long
name|id
decl_stmt|;
comment|/*    * set>=0 for nubs and leaves    */
specifier|protected
name|int
name|firstInsertionIndex
init|=
operator|-
literal|1
decl_stmt|;
comment|/*    * A positive value indicating how many bytes before the end of the block this node will start. If    * the section is 55 bytes and negativeOffset is 9, then the node will start at 46.    */
specifier|protected
name|int
name|negativeIndex
init|=
literal|0
decl_stmt|;
comment|/*    * The offset in the output array at which to start writing this node's token bytes.  Influenced    * by the lengths of all tokens sorted before this one.    */
specifier|protected
name|int
name|outputArrayOffset
init|=
operator|-
literal|1
decl_stmt|;
comment|/*********************** construct *****************************/
specifier|public
name|TokenizerNode
parameter_list|(
name|Tokenizer
name|builder
parameter_list|,
name|TokenizerNode
name|parent
parameter_list|,
name|int
name|nodeDepth
parameter_list|,
name|int
name|tokenStartOffset
parameter_list|,
name|int
name|tokenOffset
parameter_list|,
name|int
name|tokenLength
parameter_list|)
block|{
name|this
operator|.
name|token
operator|=
operator|new
name|SimpleMutableByteRange
argument_list|()
expr_stmt|;
name|reconstruct
argument_list|(
name|builder
argument_list|,
name|parent
argument_list|,
name|nodeDepth
argument_list|,
name|tokenStartOffset
argument_list|,
name|tokenOffset
argument_list|,
name|tokenLength
argument_list|)
expr_stmt|;
name|this
operator|.
name|children
operator|=
name|Lists
operator|.
name|newArrayList
argument_list|()
expr_stmt|;
block|}
comment|/*    * Sub-constructor for initializing all fields without allocating a new object.  Used by the    * regular constructor.    */
specifier|public
name|void
name|reconstruct
parameter_list|(
name|Tokenizer
name|builder
parameter_list|,
name|TokenizerNode
name|parent
parameter_list|,
name|int
name|nodeDepth
parameter_list|,
name|int
name|tokenStartOffset
parameter_list|,
name|int
name|tokenOffset
parameter_list|,
name|int
name|tokenLength
parameter_list|)
block|{
name|this
operator|.
name|builder
operator|=
name|builder
expr_stmt|;
name|this
operator|.
name|id
operator|=
name|builder
operator|.
name|nextNodeId
argument_list|()
expr_stmt|;
name|this
operator|.
name|parent
operator|=
name|parent
expr_stmt|;
name|this
operator|.
name|nodeDepth
operator|=
name|nodeDepth
expr_stmt|;
name|builder
operator|.
name|submitMaxNodeDepthCandidate
argument_list|(
name|nodeDepth
argument_list|)
expr_stmt|;
name|this
operator|.
name|tokenStartOffset
operator|=
name|tokenStartOffset
expr_stmt|;
name|this
operator|.
name|token
operator|.
name|set
argument_list|(
name|builder
operator|.
name|tokens
argument_list|,
name|tokenOffset
argument_list|,
name|tokenLength
argument_list|)
expr_stmt|;
name|this
operator|.
name|numOccurrences
operator|=
literal|1
expr_stmt|;
block|}
comment|/*    * Clear the state of this node so that it looks like it was just allocated.    */
specifier|public
name|void
name|reset
parameter_list|()
block|{
name|builder
operator|=
literal|null
expr_stmt|;
name|parent
operator|=
literal|null
expr_stmt|;
name|nodeDepth
operator|=
literal|0
expr_stmt|;
name|tokenStartOffset
operator|=
literal|0
expr_stmt|;
name|token
operator|.
name|unset
argument_list|()
expr_stmt|;
name|numOccurrences
operator|=
literal|0
expr_stmt|;
name|children
operator|.
name|clear
argument_list|()
expr_stmt|;
comment|// branches& nubs
comment|// ids/offsets. used during writing to byte[]
name|id
operator|=
literal|0
expr_stmt|;
name|firstInsertionIndex
operator|=
operator|-
literal|1
expr_stmt|;
comment|// set>=0 for nubs and leaves
name|negativeIndex
operator|=
literal|0
expr_stmt|;
name|outputArrayOffset
operator|=
operator|-
literal|1
expr_stmt|;
block|}
comment|/************************* building *********************************/
comment|/*    *<li>Only public method used during the tokenization process    *<li>Requires that the input ByteRange sort after the previous, and therefore after all previous    * inputs    *<li>Only looks at bytes of the input array that align with this node's token    */
specifier|public
name|void
name|addSorted
parameter_list|(
specifier|final
name|ByteRange
name|bytes
parameter_list|)
block|{
comment|// recursively build the tree
comment|/*      * Recurse deeper into the existing trie structure      */
if|if
condition|(
name|matchesToken
argument_list|(
name|bytes
argument_list|)
operator|&&
name|CollectionUtils
operator|.
name|notEmpty
argument_list|(
name|children
argument_list|)
condition|)
block|{
name|TokenizerNode
name|lastChild
init|=
name|CollectionUtils
operator|.
name|getLast
argument_list|(
name|children
argument_list|)
decl_stmt|;
if|if
condition|(
name|lastChild
operator|.
name|partiallyMatchesToken
argument_list|(
name|bytes
argument_list|)
condition|)
block|{
name|lastChild
operator|.
name|addSorted
argument_list|(
name|bytes
argument_list|)
expr_stmt|;
return|return;
block|}
block|}
comment|/*      * Recursion ended.  We must either      *<li>1: increment numOccurrences if this input was equal to the previous      *<li>2: convert this node from a leaf to a nub, and add a new child leaf      *<li>3: split this node into a branch and leaf, and then add a second leaf      */
comment|// add it as a child of this node
name|int
name|numIdenticalTokenBytes
init|=
name|numIdenticalBytes
argument_list|(
name|bytes
argument_list|)
decl_stmt|;
comment|// should be<= token.length
name|int
name|tailOffset
init|=
name|tokenStartOffset
operator|+
name|numIdenticalTokenBytes
decl_stmt|;
name|int
name|tailLength
init|=
name|bytes
operator|.
name|getLength
argument_list|()
operator|-
name|tailOffset
decl_stmt|;
if|if
condition|(
name|numIdenticalTokenBytes
operator|==
name|token
operator|.
name|getLength
argument_list|()
condition|)
block|{
if|if
condition|(
name|tailLength
operator|==
literal|0
condition|)
block|{
comment|// identical to this node (case 1)
name|incrementNumOccurrences
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// identical to this node, but with a few extra tailing bytes. (leaf -> nub) (case 2)
name|int
name|childNodeDepth
init|=
name|nodeDepth
operator|+
literal|1
decl_stmt|;
name|int
name|childTokenStartOffset
init|=
name|tokenStartOffset
operator|+
name|numIdenticalTokenBytes
decl_stmt|;
name|TokenizerNode
name|newChildNode
init|=
name|builder
operator|.
name|addNode
argument_list|(
name|this
argument_list|,
name|childNodeDepth
argument_list|,
name|childTokenStartOffset
argument_list|,
name|bytes
argument_list|,
name|tailOffset
argument_list|)
decl_stmt|;
name|addChild
argument_list|(
name|newChildNode
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|//numIdenticalBytes> 0, split into branch/leaf and then add second leaf (case 3)
name|split
argument_list|(
name|numIdenticalTokenBytes
argument_list|,
name|bytes
argument_list|)
expr_stmt|;
block|}
block|}
specifier|protected
name|void
name|addChild
parameter_list|(
name|TokenizerNode
name|node
parameter_list|)
block|{
name|node
operator|.
name|setParent
argument_list|(
name|this
argument_list|)
expr_stmt|;
name|children
operator|.
name|add
argument_list|(
name|node
argument_list|)
expr_stmt|;
block|}
comment|/**    * Called when we need to convert a leaf node into a branch with 2 leaves. Comments inside the    * method assume we have token BAA starting at tokenStartOffset=0 and are adding BOO. The output    * will be 3 nodes:<br>    *<ul>    *<li>1: B&lt;- branch    *<li>2: AA&lt;- leaf    *<li>3: OO&lt;- leaf    *</ul>    *    * @param numTokenBytesToRetain =&gt; 1 (the B)    * @param bytes =&gt; BOO    */
specifier|protected
name|void
name|split
parameter_list|(
name|int
name|numTokenBytesToRetain
parameter_list|,
specifier|final
name|ByteRange
name|bytes
parameter_list|)
block|{
name|int
name|childNodeDepth
init|=
name|nodeDepth
decl_stmt|;
name|int
name|childTokenStartOffset
init|=
name|tokenStartOffset
operator|+
name|numTokenBytesToRetain
decl_stmt|;
comment|//create leaf AA
name|TokenizerNode
name|firstChild
init|=
name|builder
operator|.
name|addNode
argument_list|(
name|this
argument_list|,
name|childNodeDepth
argument_list|,
name|childTokenStartOffset
argument_list|,
name|token
argument_list|,
name|numTokenBytesToRetain
argument_list|)
decl_stmt|;
name|firstChild
operator|.
name|setNumOccurrences
argument_list|(
name|numOccurrences
argument_list|)
expr_stmt|;
comment|// do before clearing this node's numOccurrences
name|token
operator|.
name|setLength
argument_list|(
name|numTokenBytesToRetain
argument_list|)
expr_stmt|;
comment|//shorten current token from BAA to B
name|numOccurrences
operator|=
literal|0
expr_stmt|;
comment|//current node is now a branch
name|moveChildrenToDifferentParent
argument_list|(
name|firstChild
argument_list|)
expr_stmt|;
comment|//point the new leaf (AA) to the new branch (B)
name|addChild
argument_list|(
name|firstChild
argument_list|)
expr_stmt|;
comment|//add the new leaf (AA) to the branch's (B's) children
comment|//create leaf OO
name|TokenizerNode
name|secondChild
init|=
name|builder
operator|.
name|addNode
argument_list|(
name|this
argument_list|,
name|childNodeDepth
argument_list|,
name|childTokenStartOffset
argument_list|,
name|bytes
argument_list|,
name|tokenStartOffset
operator|+
name|numTokenBytesToRetain
argument_list|)
decl_stmt|;
name|addChild
argument_list|(
name|secondChild
argument_list|)
expr_stmt|;
comment|//add the new leaf (00) to the branch's (B's) children
comment|// we inserted branch node B as a new level above/before the two children, so increment the
comment|// depths of the children below
name|firstChild
operator|.
name|incrementNodeDepthRecursively
argument_list|()
expr_stmt|;
name|secondChild
operator|.
name|incrementNodeDepthRecursively
argument_list|()
expr_stmt|;
block|}
specifier|protected
name|void
name|incrementNodeDepthRecursively
parameter_list|()
block|{
operator|++
name|nodeDepth
expr_stmt|;
name|builder
operator|.
name|submitMaxNodeDepthCandidate
argument_list|(
name|nodeDepth
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|incrementNodeDepthRecursively
argument_list|()
expr_stmt|;
block|}
block|}
specifier|protected
name|void
name|moveChildrenToDifferentParent
parameter_list|(
name|TokenizerNode
name|newParent
parameter_list|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|child
operator|.
name|setParent
argument_list|(
name|newParent
argument_list|)
expr_stmt|;
name|newParent
operator|.
name|children
operator|.
name|add
argument_list|(
name|child
argument_list|)
expr_stmt|;
block|}
name|children
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
comment|/************************ byte[] utils *************************/
specifier|protected
name|boolean
name|partiallyMatchesToken
parameter_list|(
name|ByteRange
name|bytes
parameter_list|)
block|{
return|return
name|numIdenticalBytes
argument_list|(
name|bytes
argument_list|)
operator|>
literal|0
return|;
block|}
specifier|protected
name|boolean
name|matchesToken
parameter_list|(
name|ByteRange
name|bytes
parameter_list|)
block|{
return|return
name|numIdenticalBytes
argument_list|(
name|bytes
argument_list|)
operator|==
name|getTokenLength
argument_list|()
return|;
block|}
specifier|protected
name|int
name|numIdenticalBytes
parameter_list|(
name|ByteRange
name|bytes
parameter_list|)
block|{
return|return
name|ByteRangeUtils
operator|.
name|numEqualPrefixBytes
argument_list|(
name|token
argument_list|,
name|bytes
argument_list|,
name|tokenStartOffset
argument_list|)
return|;
block|}
comment|/***************** moving nodes around ************************/
specifier|public
name|void
name|appendNodesToExternalList
parameter_list|(
name|List
argument_list|<
name|TokenizerNode
argument_list|>
name|appendTo
parameter_list|,
name|boolean
name|includeNonLeaves
parameter_list|,
name|boolean
name|includeLeaves
parameter_list|)
block|{
if|if
condition|(
name|includeNonLeaves
operator|&&
operator|!
name|isLeaf
argument_list|()
operator|||
name|includeLeaves
operator|&&
name|isLeaf
argument_list|()
condition|)
block|{
name|appendTo
operator|.
name|add
argument_list|(
name|this
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|child
operator|.
name|appendNodesToExternalList
argument_list|(
name|appendTo
argument_list|,
name|includeNonLeaves
argument_list|,
name|includeLeaves
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
name|int
name|setInsertionIndexes
parameter_list|(
name|int
name|nextIndex
parameter_list|)
block|{
name|int
name|newNextIndex
init|=
name|nextIndex
decl_stmt|;
if|if
condition|(
name|hasOccurrences
argument_list|()
condition|)
block|{
name|setFirstInsertionIndex
argument_list|(
name|nextIndex
argument_list|)
expr_stmt|;
name|newNextIndex
operator|+=
name|numOccurrences
expr_stmt|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|newNextIndex
operator|=
name|child
operator|.
name|setInsertionIndexes
argument_list|(
name|newNextIndex
argument_list|)
expr_stmt|;
block|}
return|return
name|newNextIndex
return|;
block|}
specifier|public
name|void
name|appendOutputArrayOffsets
parameter_list|(
name|List
argument_list|<
name|Integer
argument_list|>
name|offsets
parameter_list|)
block|{
if|if
condition|(
name|hasOccurrences
argument_list|()
condition|)
block|{
name|offsets
operator|.
name|add
argument_list|(
name|outputArrayOffset
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|child
operator|.
name|appendOutputArrayOffsets
argument_list|(
name|offsets
argument_list|)
expr_stmt|;
block|}
block|}
comment|/***************** searching *********************************/
comment|/*    * Do a trie style search through the tokenizer.  One option for looking up families or qualifiers    * during encoding, but currently unused in favor of tracking this information as they are added.    *    * Keeping code pending further performance testing.    */
specifier|public
name|void
name|getNode
parameter_list|(
name|TokenizerRowSearchResult
name|resultHolder
parameter_list|,
name|byte
index|[]
name|key
parameter_list|,
name|int
name|keyOffset
parameter_list|,
name|int
name|keyLength
parameter_list|)
block|{
name|int
name|thisNodeDepthPlusLength
init|=
name|tokenStartOffset
operator|+
name|token
operator|.
name|getLength
argument_list|()
decl_stmt|;
comment|// quick check if the key is shorter than this node (may not work for binary search)
if|if
condition|(
name|CollectionUtils
operator|.
name|isEmpty
argument_list|(
name|children
argument_list|)
condition|)
block|{
if|if
condition|(
name|thisNodeDepthPlusLength
operator|<
name|keyLength
condition|)
block|{
comment|// ran out of bytes
name|resultHolder
operator|.
name|set
argument_list|(
name|TokenizerRowSearchPosition
operator|.
name|NO_MATCH
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
block|}
comment|// all token bytes must match
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|token
operator|.
name|getLength
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|key
index|[
name|tokenStartOffset
operator|+
name|keyOffset
operator|+
name|i
index|]
operator|!=
name|token
operator|.
name|get
argument_list|(
name|i
argument_list|)
condition|)
block|{
comment|// TODO return whether it's before or after so we can binary search
name|resultHolder
operator|.
name|set
argument_list|(
name|TokenizerRowSearchPosition
operator|.
name|NO_MATCH
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
block|}
if|if
condition|(
name|thisNodeDepthPlusLength
operator|==
name|keyLength
operator|&&
name|numOccurrences
operator|>
literal|0
condition|)
block|{
name|resultHolder
operator|.
name|set
argument_list|(
name|TokenizerRowSearchPosition
operator|.
name|MATCH
argument_list|,
name|this
argument_list|)
expr_stmt|;
comment|// MATCH
return|return;
block|}
if|if
condition|(
name|CollectionUtils
operator|.
name|notEmpty
argument_list|(
name|children
argument_list|)
condition|)
block|{
comment|// TODO binary search the children
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|child
operator|.
name|getNode
argument_list|(
name|resultHolder
argument_list|,
name|key
argument_list|,
name|keyOffset
argument_list|,
name|keyLength
argument_list|)
expr_stmt|;
if|if
condition|(
name|resultHolder
operator|.
name|isMatch
argument_list|()
condition|)
block|{
return|return;
block|}
elseif|else
if|if
condition|(
name|resultHolder
operator|.
name|getDifference
argument_list|()
operator|==
name|TokenizerRowSearchPosition
operator|.
name|BEFORE
condition|)
block|{
comment|// passed it, so it doesn't exist
name|resultHolder
operator|.
name|set
argument_list|(
name|TokenizerRowSearchPosition
operator|.
name|NO_MATCH
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// key is still AFTER the current node, so continue searching
block|}
block|}
comment|// checked all children (or there were no children), and didn't find it
name|resultHolder
operator|.
name|set
argument_list|(
name|TokenizerRowSearchPosition
operator|.
name|NO_MATCH
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
comment|/****************** writing back to byte[]'s *************************/
specifier|public
name|byte
index|[]
name|getNewByteArray
parameter_list|()
block|{
name|byte
index|[]
name|arrayToFill
init|=
operator|new
name|byte
index|[
name|tokenStartOffset
operator|+
name|token
operator|.
name|getLength
argument_list|()
index|]
decl_stmt|;
name|fillInBytes
argument_list|(
name|arrayToFill
argument_list|)
expr_stmt|;
return|return
name|arrayToFill
return|;
block|}
specifier|public
name|void
name|fillInBytes
parameter_list|(
name|byte
index|[]
name|arrayToFill
parameter_list|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|token
operator|.
name|getLength
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|arrayToFill
index|[
name|tokenStartOffset
operator|+
name|i
index|]
operator|=
name|token
operator|.
name|get
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|parent
operator|!=
literal|null
condition|)
block|{
name|parent
operator|.
name|fillInBytes
argument_list|(
name|arrayToFill
argument_list|)
expr_stmt|;
block|}
block|}
comment|/************************** printing ***********************/
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
name|String
name|s
init|=
literal|""
decl_stmt|;
if|if
condition|(
name|parent
operator|==
literal|null
condition|)
block|{
name|s
operator|+=
literal|"R "
expr_stmt|;
block|}
else|else
block|{
name|s
operator|+=
name|getBnlIndicator
argument_list|(
literal|false
argument_list|)
operator|+
literal|" "
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|parent
operator|.
name|getNewByteArray
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|s
operator|+=
literal|"["
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|token
operator|.
name|deepCopyToNewArray
argument_list|()
argument_list|)
operator|+
literal|"]"
expr_stmt|;
if|if
condition|(
name|numOccurrences
operator|>
literal|0
condition|)
block|{
name|s
operator|+=
literal|"x"
operator|+
name|numOccurrences
expr_stmt|;
block|}
return|return
name|s
return|;
block|}
specifier|public
name|String
name|getPaddedTokenAndOccurrenceString
parameter_list|()
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|getBnlIndicator
argument_list|(
literal|true
argument_list|)
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|Strings
operator|.
name|padFront
argument_list|(
name|numOccurrences
operator|+
literal|""
argument_list|,
literal|' '
argument_list|,
literal|3
argument_list|)
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|Strings
operator|.
name|padFront
argument_list|(
name|nodeDepth
operator|+
literal|""
argument_list|,
literal|' '
argument_list|,
literal|3
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|outputArrayOffset
operator|>=
literal|0
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|Strings
operator|.
name|padFront
argument_list|(
name|outputArrayOffset
operator|+
literal|""
argument_list|,
literal|' '
argument_list|,
literal|3
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
literal|"  "
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|tokenStartOffset
condition|;
operator|++
name|i
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|" "
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
name|Bytes
operator|.
name|toString
argument_list|(
name|token
operator|.
name|deepCopyToNewArray
argument_list|()
argument_list|)
operator|.
name|replaceAll
argument_list|(
literal|" "
argument_list|,
literal|"_"
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
name|String
name|getBnlIndicator
parameter_list|(
name|boolean
name|indent
parameter_list|)
block|{
if|if
condition|(
name|indent
condition|)
block|{
if|if
condition|(
name|isNub
argument_list|()
condition|)
block|{
return|return
literal|" N "
return|;
block|}
return|return
name|isBranch
argument_list|()
condition|?
literal|"B  "
else|:
literal|"  L"
return|;
block|}
if|if
condition|(
name|isNub
argument_list|()
condition|)
block|{
return|return
literal|"N"
return|;
block|}
return|return
name|isBranch
argument_list|()
condition|?
literal|"B"
else|:
literal|"L"
return|;
block|}
comment|/********************** count different node types ********************/
specifier|public
name|int
name|getNumBranchNodesIncludingThisNode
parameter_list|()
block|{
if|if
condition|(
name|isLeaf
argument_list|()
condition|)
block|{
return|return
literal|0
return|;
block|}
name|int
name|totalFromThisPlusChildren
init|=
name|isBranch
argument_list|()
condition|?
literal|1
else|:
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|totalFromThisPlusChildren
operator|+=
name|child
operator|.
name|getNumBranchNodesIncludingThisNode
argument_list|()
expr_stmt|;
block|}
return|return
name|totalFromThisPlusChildren
return|;
block|}
specifier|public
name|int
name|getNumNubNodesIncludingThisNode
parameter_list|()
block|{
if|if
condition|(
name|isLeaf
argument_list|()
condition|)
block|{
return|return
literal|0
return|;
block|}
name|int
name|totalFromThisPlusChildren
init|=
name|isNub
argument_list|()
condition|?
literal|1
else|:
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|totalFromThisPlusChildren
operator|+=
name|child
operator|.
name|getNumNubNodesIncludingThisNode
argument_list|()
expr_stmt|;
block|}
return|return
name|totalFromThisPlusChildren
return|;
block|}
specifier|public
name|int
name|getNumLeafNodesIncludingThisNode
parameter_list|()
block|{
if|if
condition|(
name|isLeaf
argument_list|()
condition|)
block|{
return|return
literal|1
return|;
block|}
name|int
name|totalFromChildren
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|TokenizerNode
name|child
init|=
name|children
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|totalFromChildren
operator|+=
name|child
operator|.
name|getNumLeafNodesIncludingThisNode
argument_list|()
expr_stmt|;
block|}
return|return
name|totalFromChildren
return|;
block|}
comment|/*********************** simple read-only methods *******************************/
specifier|public
name|int
name|getNodeDepth
parameter_list|()
block|{
return|return
name|nodeDepth
return|;
block|}
specifier|public
name|int
name|getTokenLength
parameter_list|()
block|{
return|return
name|token
operator|.
name|getLength
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|hasOccurrences
parameter_list|()
block|{
return|return
name|numOccurrences
operator|>
literal|0
return|;
block|}
specifier|public
name|boolean
name|isRoot
parameter_list|()
block|{
return|return
name|this
operator|.
name|parent
operator|==
literal|null
return|;
block|}
specifier|public
name|int
name|getNumChildren
parameter_list|()
block|{
return|return
name|CollectionUtils
operator|.
name|nullSafeSize
argument_list|(
name|children
argument_list|)
return|;
block|}
specifier|public
name|TokenizerNode
name|getLastChild
parameter_list|()
block|{
if|if
condition|(
name|CollectionUtils
operator|.
name|isEmpty
argument_list|(
name|children
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|CollectionUtils
operator|.
name|getLast
argument_list|(
name|children
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|isLeaf
parameter_list|()
block|{
return|return
name|CollectionUtils
operator|.
name|isEmpty
argument_list|(
name|children
argument_list|)
operator|&&
name|hasOccurrences
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|isBranch
parameter_list|()
block|{
return|return
name|CollectionUtils
operator|.
name|notEmpty
argument_list|(
name|children
argument_list|)
operator|&&
operator|!
name|hasOccurrences
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|isNub
parameter_list|()
block|{
return|return
name|CollectionUtils
operator|.
name|notEmpty
argument_list|(
name|children
argument_list|)
operator|&&
name|hasOccurrences
argument_list|()
return|;
block|}
comment|/********************** simple mutation methods *************************/
comment|/**    * Each occurrence&gt; 1 indicates a repeat of the previous entry.    * This can be called directly by    * an external class without going through the process of detecting a repeat if it is a known    * repeat by some external mechanism.  PtEncoder uses this when adding cells to a row if it knows    * the new cells are part of the current row.    * @param d increment by this amount    */
specifier|public
name|void
name|incrementNumOccurrences
parameter_list|(
name|int
name|d
parameter_list|)
block|{
name|numOccurrences
operator|+=
name|d
expr_stmt|;
block|}
comment|/************************* autogenerated get/set ******************/
specifier|public
name|int
name|getTokenOffset
parameter_list|()
block|{
return|return
name|tokenStartOffset
return|;
block|}
specifier|public
name|TokenizerNode
name|getParent
parameter_list|()
block|{
return|return
name|parent
return|;
block|}
specifier|public
name|ByteRange
name|getToken
parameter_list|()
block|{
return|return
name|token
return|;
block|}
specifier|public
name|int
name|getNumOccurrences
parameter_list|()
block|{
return|return
name|numOccurrences
return|;
block|}
specifier|public
name|void
name|setParent
parameter_list|(
name|TokenizerNode
name|parent
parameter_list|)
block|{
name|this
operator|.
name|parent
operator|=
name|parent
expr_stmt|;
block|}
specifier|public
name|void
name|setNumOccurrences
parameter_list|(
name|int
name|numOccurrences
parameter_list|)
block|{
name|this
operator|.
name|numOccurrences
operator|=
name|numOccurrences
expr_stmt|;
block|}
specifier|public
name|ArrayList
argument_list|<
name|TokenizerNode
argument_list|>
name|getChildren
parameter_list|()
block|{
return|return
name|children
return|;
block|}
specifier|public
name|long
name|getId
parameter_list|()
block|{
return|return
name|id
return|;
block|}
specifier|public
name|int
name|getFirstInsertionIndex
parameter_list|()
block|{
return|return
name|firstInsertionIndex
return|;
block|}
specifier|public
name|void
name|setFirstInsertionIndex
parameter_list|(
name|int
name|firstInsertionIndex
parameter_list|)
block|{
name|this
operator|.
name|firstInsertionIndex
operator|=
name|firstInsertionIndex
expr_stmt|;
block|}
specifier|public
name|int
name|getNegativeIndex
parameter_list|()
block|{
return|return
name|negativeIndex
return|;
block|}
specifier|public
name|void
name|setNegativeIndex
parameter_list|(
name|int
name|negativeIndex
parameter_list|)
block|{
name|this
operator|.
name|negativeIndex
operator|=
name|negativeIndex
expr_stmt|;
block|}
specifier|public
name|int
name|getOutputArrayOffset
parameter_list|()
block|{
return|return
name|outputArrayOffset
return|;
block|}
specifier|public
name|void
name|setOutputArrayOffset
parameter_list|(
name|int
name|outputArrayOffset
parameter_list|)
block|{
name|this
operator|.
name|outputArrayOffset
operator|=
name|outputArrayOffset
expr_stmt|;
block|}
specifier|public
name|void
name|setId
parameter_list|(
name|long
name|id
parameter_list|)
block|{
name|this
operator|.
name|id
operator|=
name|id
expr_stmt|;
block|}
specifier|public
name|void
name|setBuilder
parameter_list|(
name|Tokenizer
name|builder
parameter_list|)
block|{
name|this
operator|.
name|builder
operator|=
name|builder
expr_stmt|;
block|}
specifier|public
name|void
name|setTokenOffset
parameter_list|(
name|int
name|tokenOffset
parameter_list|)
block|{
name|this
operator|.
name|tokenStartOffset
operator|=
name|tokenOffset
expr_stmt|;
block|}
specifier|public
name|void
name|setToken
parameter_list|(
name|ByteRange
name|token
parameter_list|)
block|{
name|this
operator|.
name|token
operator|=
name|token
expr_stmt|;
block|}
block|}
end_class

end_unit

