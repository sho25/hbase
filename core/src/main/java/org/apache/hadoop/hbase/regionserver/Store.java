begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2010 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HRegionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
operator|.
name|KeyComparator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|RemoteExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Get
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Scan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|HeapSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|Compression
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFile
operator|.
name|Reader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFileScanner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|wal
operator|.
name|HLog
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|wal
operator|.
name|HLogKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|wal
operator|.
name|WALEdit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ClassSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|FSUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Progressable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|EOFException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentSkipListMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CopyOnWriteArraySet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|locks
operator|.
name|ReentrantReadWriteLock
import|;
end_import

begin_comment
comment|/**   * A Store holds a column family in a Region.  Its a memstore and a set of zero   * or more StoreFiles, which stretch backwards over time.   *   *<p>There's no reason to consider append-logging at this level; all logging   * and locking is handled at the HRegion level.  Store just provides   * services to manage sets of StoreFiles.  One of the most important of those   * services is compaction services where files are aggregated once they pass   * a configurable threshold.   *   *<p>The only thing having to do with logs that Store needs to deal with is   * the reconstructionLog.  This is a segment of an HRegion's log that might   * NOT be present upon startup.  If the param is NULL, there's nothing to do.   * If the param is non-NULL, we need to process the log to reconstruct   * a TreeMap that might not have been written to disk before the process   * died.   *   *<p>It's assumed that after this constructor returns, the reconstructionLog   * file will be deleted (by whoever has instantiated the Store).  *  *<p>Locking and transactions are handled at a higher level.  This API should  * not be called directly but by an HRegion manager.  */
end_comment

begin_class
specifier|public
class|class
name|Store
implements|implements
name|HConstants
implements|,
name|HeapSize
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|Store
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/**    * Comparator that looks at columns and compares their family portions.    * Presumes columns have already been checked for presence of delimiter.    * If no delimiter present, presume the buffer holds a store name so no need    * of a delimiter.    */
specifier|protected
specifier|final
name|MemStore
name|memstore
decl_stmt|;
comment|// This stores directory in the filesystem.
specifier|private
specifier|final
name|Path
name|homedir
decl_stmt|;
specifier|private
specifier|final
name|HRegion
name|region
decl_stmt|;
specifier|private
specifier|final
name|HColumnDescriptor
name|family
decl_stmt|;
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|Configuration
name|conf
decl_stmt|;
comment|// ttl in milliseconds.
specifier|protected
name|long
name|ttl
decl_stmt|;
specifier|private
name|long
name|majorCompactionTime
decl_stmt|;
specifier|private
name|int
name|maxFilesToCompact
decl_stmt|;
specifier|private
specifier|final
name|long
name|desiredMaxFileSize
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|storeSize
init|=
literal|0L
decl_stmt|;
specifier|private
specifier|final
name|Object
name|flushLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|final
name|ReentrantReadWriteLock
name|lock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
specifier|final
name|byte
index|[]
name|storeName
decl_stmt|;
specifier|private
specifier|final
name|String
name|storeNameStr
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|inMemory
decl_stmt|;
comment|/*    * Sorted Map of readers keyed by maximum edit sequence id (Most recent should    * be last in in list).  ConcurrentSkipListMap is lazily consistent so no    * need to lock it down when iterating; iterator view is that of when the    * iterator was taken out.    */
specifier|private
specifier|final
name|NavigableMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|storefiles
init|=
operator|new
name|ConcurrentSkipListMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
comment|// All access must be synchronized.
specifier|private
specifier|final
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
name|changedReaderObservers
init|=
operator|new
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
argument_list|()
decl_stmt|;
comment|// The most-recent log-seq-ID.  The most-recent such ID means we can ignore
comment|// all log messages up to and including that ID (because they're already
comment|// reflected in the TreeMaps).
specifier|private
specifier|volatile
name|long
name|maxSeqId
init|=
operator|-
literal|1
decl_stmt|;
comment|// The most-recent log-seq-id before we recovered from the LOG.
specifier|private
name|long
name|maxSeqIdBeforeLogRecovery
init|=
operator|-
literal|1
decl_stmt|;
specifier|private
specifier|final
name|Path
name|regionCompactionDir
decl_stmt|;
specifier|private
specifier|final
name|Object
name|compactLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|int
name|compactionThreshold
decl_stmt|;
specifier|private
specifier|final
name|int
name|blocksize
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|blockcache
decl_stmt|;
specifier|private
specifier|final
name|Compression
operator|.
name|Algorithm
name|compression
decl_stmt|;
comment|// Comparing KeyValues
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|comparator
decl_stmt|;
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|comparatorIgnoringType
decl_stmt|;
comment|/**    * Constructor    * @param basedir qualified path under which the region directory lives;    * generally the table subdirectory    * @param region    * @param family HColumnDescriptor for this column    * @param fs file system object    * @param reconstructionLog existing log file to apply if any    * @param conf configuration object    * @param reporter Call on a period so hosting server can report we're    * making progress to master -- otherwise master might think region deploy    * failed.  Can be null.    * @throws IOException    */
specifier|protected
name|Store
parameter_list|(
name|Path
name|basedir
parameter_list|,
name|HRegion
name|region
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|reconstructionLog
parameter_list|,
name|Configuration
name|conf
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|HRegionInfo
name|info
init|=
name|region
operator|.
name|regionInfo
decl_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|homedir
operator|=
name|getStoreHomedir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|mkdirs
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
condition|)
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Failed create of: "
operator|+
name|this
operator|.
name|homedir
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
name|this
operator|.
name|region
operator|=
name|region
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|blockcache
operator|=
name|family
operator|.
name|isBlockCacheEnabled
argument_list|()
expr_stmt|;
name|this
operator|.
name|blocksize
operator|=
name|family
operator|.
name|getBlocksize
argument_list|()
expr_stmt|;
name|this
operator|.
name|compression
operator|=
name|family
operator|.
name|getCompression
argument_list|()
expr_stmt|;
name|this
operator|.
name|comparator
operator|=
name|info
operator|.
name|getComparator
argument_list|()
expr_stmt|;
name|this
operator|.
name|comparatorIgnoringType
operator|=
name|this
operator|.
name|comparator
operator|.
name|getComparatorIgnoringType
argument_list|()
expr_stmt|;
comment|// getTimeToLive returns ttl in seconds.  Convert to milliseconds.
name|this
operator|.
name|ttl
operator|=
name|family
operator|.
name|getTimeToLive
argument_list|()
expr_stmt|;
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
condition|)
block|{
comment|// default is unlimited ttl.
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|ttl
operator|==
operator|-
literal|1
condition|)
block|{
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
else|else
block|{
comment|// second -> ms adjust for user data
name|this
operator|.
name|ttl
operator|*=
literal|1000
expr_stmt|;
block|}
name|this
operator|.
name|memstore
operator|=
operator|new
name|MemStore
argument_list|(
name|this
operator|.
name|comparator
argument_list|)
expr_stmt|;
name|this
operator|.
name|regionCompactionDir
operator|=
operator|new
name|Path
argument_list|(
name|HRegion
operator|.
name|getCompactionDir
argument_list|(
name|basedir
argument_list|)
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeName
operator|=
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
expr_stmt|;
name|this
operator|.
name|storeNameStr
operator|=
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
comment|// By default, we compact if an HStore has more than
comment|// MIN_COMMITS_FOR_COMPACTION map files
name|this
operator|.
name|compactionThreshold
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
expr_stmt|;
comment|// Check if this is in-memory store
name|this
operator|.
name|inMemory
operator|=
name|family
operator|.
name|isInMemory
argument_list|()
expr_stmt|;
comment|// By default we split region if a file> DEFAULT_MAX_FILE_SIZE.
name|long
name|maxFileSize
init|=
name|info
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getMaxFileSize
argument_list|()
decl_stmt|;
if|if
condition|(
name|maxFileSize
operator|==
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
condition|)
block|{
name|maxFileSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hregion.max.filesize"
argument_list|,
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|desiredMaxFileSize
operator|=
name|maxFileSize
expr_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|,
literal|86400000
argument_list|)
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|String
name|strCompactionTime
init|=
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
decl_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
operator|(
operator|new
name|Long
argument_list|(
name|strCompactionTime
argument_list|)
operator|)
operator|.
name|longValue
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|maxFilesToCompact
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.max"
argument_list|,
literal|10
argument_list|)
expr_stmt|;
comment|// loadStoreFiles calculates this.maxSeqId. as side-effect.
name|this
operator|.
name|storefiles
operator|.
name|putAll
argument_list|(
name|loadStoreFiles
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|maxSeqIdBeforeLogRecovery
operator|=
name|this
operator|.
name|maxSeqId
expr_stmt|;
comment|// Do reconstruction log.
name|long
name|newId
init|=
name|runReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|this
operator|.
name|maxSeqId
argument_list|,
name|reporter
argument_list|)
decl_stmt|;
if|if
condition|(
name|newId
operator|!=
operator|-
literal|1
condition|)
block|{
name|this
operator|.
name|maxSeqId
operator|=
name|newId
expr_stmt|;
comment|// start with the log id we just recovered.
block|}
block|}
name|HColumnDescriptor
name|getFamily
parameter_list|()
block|{
return|return
name|this
operator|.
name|family
return|;
block|}
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|this
operator|.
name|maxSeqId
return|;
block|}
name|long
name|getMaxSeqIdBeforeLogRecovery
parameter_list|()
block|{
return|return
name|maxSeqIdBeforeLogRecovery
return|;
block|}
comment|/**    * @param tabledir    * @param encodedName Encoded region name.    * @param family    * @return Path to family/Store home directory.    */
specifier|public
specifier|static
name|Path
name|getStoreHomedir
parameter_list|(
specifier|final
name|Path
name|tabledir
parameter_list|,
specifier|final
name|int
name|encodedName
parameter_list|,
specifier|final
name|byte
index|[]
name|family
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|tabledir
argument_list|,
operator|new
name|Path
argument_list|(
name|Integer
operator|.
name|toString
argument_list|(
name|encodedName
argument_list|)
argument_list|,
operator|new
name|Path
argument_list|(
name|Bytes
operator|.
name|toString
argument_list|(
name|family
argument_list|)
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
comment|/*    * Run reconstruction log    * @param reconstructionLog    * @param msid    * @param reporter    * @return the new max sequence id as per the log    * @throws IOException    */
specifier|private
name|long
name|runReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|msid
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
return|return
name|doReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|msid
argument_list|,
name|reporter
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|EOFException
name|e
parameter_list|)
block|{
comment|// Presume we got here because of lack of HADOOP-1700; for now keep going
comment|// but this is probably not what we want long term.  If we got here there
comment|// has been data-loss
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception processing reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" opening "
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
operator|+
literal|" -- continuing.  Probably lack-of-HADOOP-1700 causing DATA LOSS!"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// Presume we got here because of some HDFS issue. Don't just keep going.
comment|// Fail to open the HStore.  Probably means we'll fail over and over
comment|// again until human intervention but alternative has us skipping logs
comment|// and losing edits: HBASE-642.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception processing reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" opening "
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
return|return
operator|-
literal|1
return|;
block|}
comment|/*    * Read the reconstructionLog and put into memstore.    *    * We can ignore any log message that has a sequence ID that's equal to or    * lower than maxSeqID.  (Because we know such log messages are already    * reflected in the HFiles.)    *    * @return the new max sequence id as per the log, or -1 if no log recovered    */
specifier|private
name|long
name|doReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|maxSeqID
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|UnsupportedEncodingException
throws|,
name|IOException
block|{
if|if
condition|(
name|reconstructionLog
operator|==
literal|null
operator|||
operator|!
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
condition|)
block|{
comment|// Nothing to do.
return|return
operator|-
literal|1
return|;
block|}
name|FileStatus
name|stat
init|=
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|reconstructionLog
argument_list|)
decl_stmt|;
if|if
condition|(
name|stat
operator|.
name|getLen
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Passed reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" is zero-length. Deleting existing file"
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|reconstructionLog
argument_list|,
literal|false
argument_list|)
expr_stmt|;
return|return
operator|-
literal|1
return|;
block|}
comment|// TODO: This could grow large and blow heap out.  Need to get it into
comment|// general memory usage accounting.
name|long
name|maxSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|long
name|firstSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|HLog
operator|.
name|Reader
name|logReader
init|=
name|HLog
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|reconstructionLog
argument_list|,
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|long
name|skippedEdits
init|=
literal|0
decl_stmt|;
name|long
name|editsCount
init|=
literal|0
decl_stmt|;
comment|// How many edits to apply before we send a progress report.
name|int
name|reportInterval
init|=
name|this
operator|.
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.report.interval.edits"
argument_list|,
literal|2000
argument_list|)
decl_stmt|;
name|HLog
operator|.
name|Entry
name|entry
decl_stmt|;
comment|// TBD: Need to add an exception handler around logReader.next.
comment|//
comment|// A transaction now appears as a single edit. If logReader.next()
comment|// returns an exception, then it must be a incomplete/partial
comment|// transaction at the end of the file. Rather than bubble up
comment|// the exception, we should catch it and simply ignore the
comment|// partial transaction during this recovery phase.
comment|//
while|while
condition|(
operator|(
name|entry
operator|=
name|logReader
operator|.
name|next
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|HLogKey
name|key
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|WALEdit
name|val
init|=
name|entry
operator|.
name|getEdit
argument_list|()
decl_stmt|;
if|if
condition|(
name|firstSeqIdInLog
operator|==
operator|-
literal|1
condition|)
block|{
name|firstSeqIdInLog
operator|=
name|key
operator|.
name|getLogSeqNum
argument_list|()
expr_stmt|;
block|}
name|maxSeqIdInLog
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSeqIdInLog
argument_list|,
name|key
operator|.
name|getLogSeqNum
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|<=
name|maxSeqID
condition|)
block|{
name|skippedEdits
operator|++
expr_stmt|;
continue|continue;
block|}
for|for
control|(
name|KeyValue
name|kv
range|:
name|val
operator|.
name|getKeyValues
argument_list|()
control|)
block|{
comment|// Check this edit is for me. Also, guard against writing the special
comment|// METACOLUMN info such as HBASE::CACHEFLUSH entries
if|if
condition|(
name|kv
operator|.
name|matchingFamily
argument_list|(
name|HLog
operator|.
name|METAFAMILY
argument_list|)
operator|||
operator|!
name|Bytes
operator|.
name|equals
argument_list|(
name|key
operator|.
name|getRegionName
argument_list|()
argument_list|,
name|region
operator|.
name|regionInfo
operator|.
name|getRegionName
argument_list|()
argument_list|)
operator|||
operator|!
name|kv
operator|.
name|matchingFamily
argument_list|(
name|family
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|kv
operator|.
name|isDelete
argument_list|()
condition|)
block|{
name|this
operator|.
name|memstore
operator|.
name|delete
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|memstore
operator|.
name|add
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
name|editsCount
operator|++
expr_stmt|;
block|}
comment|// Every 2k edits, tell the reporter we're making progress.
comment|// Have seen 60k edits taking 3minutes to complete.
if|if
condition|(
name|reporter
operator|!=
literal|null
operator|&&
operator|(
name|editsCount
operator|%
name|reportInterval
operator|)
operator|==
literal|0
condition|)
block|{
name|reporter
operator|.
name|progress
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Applied "
operator|+
name|editsCount
operator|+
literal|", skipped "
operator|+
name|skippedEdits
operator|+
literal|"; store maxSeqID="
operator|+
name|maxSeqID
operator|+
literal|", firstSeqIdInLog="
operator|+
name|firstSeqIdInLog
operator|+
literal|", maxSeqIdInLog="
operator|+
name|maxSeqIdInLog
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|logReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|maxSeqIdInLog
operator|>
operator|-
literal|1
condition|)
block|{
comment|// We read some edits, so we should flush the memstore
name|StoreFlusher
name|flusher
init|=
name|getStoreFlusher
argument_list|(
name|maxSeqIdInLog
argument_list|)
decl_stmt|;
name|flusher
operator|.
name|prepare
argument_list|()
expr_stmt|;
name|flusher
operator|.
name|flushCache
argument_list|()
expr_stmt|;
name|boolean
name|needCompaction
init|=
name|flusher
operator|.
name|commit
argument_list|()
decl_stmt|;
if|if
condition|(
name|needCompaction
condition|)
block|{
name|this
operator|.
name|compact
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|maxSeqIdInLog
return|;
block|}
comment|/*    * Creates a series of StoreFile loaded from the given directory.    * @throws IOException    */
specifier|private
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|loadStoreFiles
parameter_list|()
throws|throws
name|IOException
block|{
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|results
init|=
operator|new
name|HashMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|FileStatus
name|files
index|[]
init|=
name|this
operator|.
name|fs
operator|.
name|listStatus
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|files
operator|!=
literal|null
operator|&&
name|i
operator|<
name|files
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
comment|// Skip directories.
if|if
condition|(
name|files
index|[
name|i
index|]
operator|.
name|isDir
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|Path
name|p
init|=
name|files
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Check for empty file.  Should never be the case but can happen
comment|// after data loss in hdfs for whatever reason (upgrade, etc.): HBASE-646
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|p
argument_list|)
operator|.
name|getLen
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Skipping "
operator|+
name|p
operator|+
literal|" because its empty. HBASE-646 DATA LOSS?"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|StoreFile
name|curfile
init|=
literal|null
decl_stmt|;
try|try
block|{
name|curfile
operator|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|p
argument_list|,
name|blockcache
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|inMemory
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed open of "
operator|+
name|p
operator|+
literal|"; presumption is that file was "
operator|+
literal|"corrupted at flush and lost edits picked up by commit log replay. "
operator|+
literal|"Verify!"
argument_list|,
name|ioe
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|storeSeqId
init|=
name|curfile
operator|.
name|getMaxSequenceId
argument_list|()
decl_stmt|;
if|if
condition|(
name|storeSeqId
operator|>
name|this
operator|.
name|maxSeqId
condition|)
block|{
name|this
operator|.
name|maxSeqId
operator|=
name|storeSeqId
expr_stmt|;
block|}
name|long
name|length
init|=
name|curfile
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|length
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|p
argument_list|)
operator|+
literal|", isReference="
operator|+
name|curfile
operator|.
name|isReference
argument_list|()
operator|+
literal|", sequence id="
operator|+
name|storeSeqId
operator|+
literal|", length="
operator|+
name|length
operator|+
literal|", majorCompaction="
operator|+
name|curfile
operator|.
name|isMajorCompaction
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|results
operator|.
name|put
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|storeSeqId
argument_list|)
argument_list|,
name|curfile
argument_list|)
expr_stmt|;
block|}
return|return
name|results
return|;
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|add
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|add
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|delete
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|delete
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return All store files.    */
name|NavigableMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|getStorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
return|;
block|}
comment|/**    * Close all the readers    *    * We don't need to worry about subsequent requests because the HRegion holds    * a write lock that will prevent any more reads or writes.    *    * @throws IOException    */
name|List
argument_list|<
name|StoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
decl_stmt|;
comment|// Clear so metrics doesn't find them.
name|this
operator|.
name|storefiles
operator|.
name|clear
argument_list|()
expr_stmt|;
for|for
control|(
name|StoreFile
name|f
range|:
name|result
control|)
block|{
name|f
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"closed "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Snapshot this stores memstore.  Call before running    * {@link #flushCache(long, SortedSet<KeyValue>)} so it has some work to do.    */
name|void
name|snapshot
parameter_list|()
block|{
name|this
operator|.
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
block|}
comment|/**    * Write out current snapshot.  Presumes {@link #snapshot()} has been called    * previously.    * @param logCacheFlushId flush sequence number    * @return true if a compaction is needed    * @throws IOException    */
specifier|private
name|StoreFile
name|flushCache
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If an exception happens flushing, we let it out without clearing
comment|// the memstore snapshot.  The old snapshot will be returned when we say
comment|// 'snapshot', the next time flush comes around.
return|return
name|internalFlushCache
argument_list|(
name|snapshot
argument_list|,
name|logCacheFlushId
argument_list|)
return|;
block|}
comment|/*    * @param cache    * @param logCacheFlushId    * @return StoreFile created.    * @throws IOException    */
specifier|private
name|StoreFile
name|internalFlushCache
parameter_list|(
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|,
specifier|final
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
name|HFile
operator|.
name|Writer
name|writer
init|=
literal|null
decl_stmt|;
name|long
name|flushed
init|=
literal|0
decl_stmt|;
comment|// Don't flush if there are no entries.
if|if
condition|(
name|set
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
name|long
name|oldestTimestamp
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|-
name|ttl
decl_stmt|;
comment|// TODO:  We can fail in the below block before we complete adding this
comment|// flush to list of store files.  Add cleanup of anything put on filesystem
comment|// if we fail.
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
comment|// A. Write the map out to the disk
name|writer
operator|=
name|getWriter
argument_list|()
expr_stmt|;
name|int
name|entries
init|=
literal|0
decl_stmt|;
try|try
block|{
for|for
control|(
name|KeyValue
name|kv
range|:
name|set
control|)
block|{
if|if
condition|(
operator|!
name|isExpired
argument_list|(
name|kv
argument_list|,
name|oldestTimestamp
argument_list|)
condition|)
block|{
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
name|entries
operator|++
expr_stmt|;
name|flushed
operator|+=
name|this
operator|.
name|memstore
operator|.
name|heapSizeChange
argument_list|(
name|kv
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
comment|// Write out the log sequence number that corresponds to this output
comment|// hfile.  The hfile is current up to and including logCacheFlushId.
name|StoreFile
operator|.
name|appendMetadata
argument_list|(
name|writer
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|writer
operator|.
name|getPath
argument_list|()
argument_list|,
name|blockcache
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|inMemory
argument_list|)
decl_stmt|;
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Added "
operator|+
name|sf
operator|+
literal|", entries="
operator|+
name|r
operator|.
name|getEntries
argument_list|()
operator|+
literal|", sequenceid="
operator|+
name|logCacheFlushId
operator|+
literal|", memsize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|flushed
argument_list|)
operator|+
literal|", filesize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|r
operator|.
name|length
argument_list|()
argument_list|)
operator|+
literal|" to "
operator|+
name|this
operator|.
name|region
operator|.
name|regionInfo
operator|.
name|getRegionNameAsString
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|sf
return|;
block|}
comment|/**    * @return Writer for this store.    * @throws IOException    */
name|HFile
operator|.
name|Writer
name|getWriter
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|getWriter
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
return|;
block|}
comment|/*    * @return Writer for this store.    * @param basedir Directory to put writer in.    * @throws IOException    */
specifier|private
name|HFile
operator|.
name|Writer
name|getWriter
parameter_list|(
specifier|final
name|Path
name|basedir
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|StoreFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|basedir
argument_list|,
name|this
operator|.
name|blocksize
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|comparator
operator|.
name|getRawComparator
argument_list|()
argument_list|)
return|;
block|}
comment|/*    * Change storefiles adding into place the Reader produced by this new flush.    * @param logCacheFlushId    * @param sf    * @param set That was used to make the passed file<code>p</code>.    * @throws IOException    * @return Whether compaction is required.    */
specifier|private
name|boolean
name|updateStorefiles
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
specifier|final
name|StoreFile
name|sf
parameter_list|,
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|logCacheFlushId
argument_list|)
argument_list|,
name|sf
argument_list|)
expr_stmt|;
name|this
operator|.
name|memstore
operator|.
name|clearSnapshot
argument_list|(
name|set
argument_list|)
expr_stmt|;
comment|// Tell listeners of the change in readers.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>=
name|this
operator|.
name|compactionThreshold
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Notify all observers that set of Readers has changed.    * @throws IOException    */
specifier|private
name|void
name|notifyChangedReadersObservers
parameter_list|()
throws|throws
name|IOException
block|{
for|for
control|(
name|ChangedReadersObserver
name|o
range|:
name|this
operator|.
name|changedReaderObservers
control|)
block|{
name|o
operator|.
name|updateReaders
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * @param o Observer who wants to know about changes in set of Readers    */
name|void
name|addChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
name|this
operator|.
name|changedReaderObservers
operator|.
name|add
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|/*    * @param o Observer no longer interested in changes in set of Readers.    */
name|void
name|deleteChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|changedReaderObservers
operator|.
name|remove
argument_list|(
name|o
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Not in set"
operator|+
name|o
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the StoreFiles.  This method may take some time, so the calling    * thread must be able to block for long periods.    *    *<p>During this time, the Store can work as usual, getting values from    * StoreFiles and writing new StoreFiles from the memstore.    *    * Existing StoreFiles are not destroyed until the new compacted StoreFile is    * completely written-out to disk.    *    *<p>The compactLock prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *    *<p>We don't want to hold the structureLock for the whole time, as a compact()    * can be lengthy and we want to allow cache-flushes during this period.    *    * @param mc True to force a major compaction regardless of thresholds    * @return row to split around if a split is needed, null otherwise    * @throws IOException    */
name|StoreSize
name|compact
parameter_list|(
specifier|final
name|boolean
name|mc
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|forceSplit
init|=
name|this
operator|.
name|region
operator|.
name|shouldSplit
argument_list|(
literal|false
argument_list|)
decl_stmt|;
name|boolean
name|majorcompaction
init|=
name|mc
decl_stmt|;
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
comment|// filesToCompact are sorted oldest to newest.
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|this
operator|.
name|storeNameStr
operator|+
literal|": no store files to compact"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// Max-sequenceID is the last key of the storefiles TreeMap
name|long
name|maxId
init|=
name|this
operator|.
name|storefiles
operator|.
name|lastKey
argument_list|()
operator|.
name|longValue
argument_list|()
decl_stmt|;
comment|// Check to see if we need to do a major compaction on this region.
comment|// If so, change doMajorCompaction to true to skip the incremental
comment|// compacting below. Only check if doMajorCompaction is not true.
if|if
condition|(
operator|!
name|majorcompaction
condition|)
block|{
name|majorcompaction
operator|=
name|isMajorCompaction
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
name|boolean
name|references
init|=
name|hasReferences
argument_list|(
name|filesToCompact
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|references
operator|&&
operator|(
name|forceSplit
operator|||
operator|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|compactionThreshold
operator|)
operator|)
condition|)
block|{
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
operator|&&
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mkdir on "
operator|+
name|this
operator|.
name|regionCompactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" failed"
argument_list|)
expr_stmt|;
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
comment|// HBASE-745, preparing all store file sizes for incremental compacting
comment|// selection.
name|int
name|countOfFiles
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
name|totalSize
init|=
literal|0
decl_stmt|;
name|long
index|[]
name|fileSizes
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
name|long
name|skipped
init|=
literal|0
decl_stmt|;
name|int
name|point
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|countOfFiles
condition|;
name|i
operator|++
control|)
block|{
name|StoreFile
name|file
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|Path
name|path
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Path is null for "
operator|+
name|file
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|Reader
name|r
init|=
name|file
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|file
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|len
init|=
name|file
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|fileSizes
index|[
name|i
index|]
operator|=
name|len
expr_stmt|;
name|totalSize
operator|+=
name|len
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|references
condition|)
block|{
comment|// Here we select files for incremental compaction.
comment|// The rule is: if the largest(oldest) one is more than twice the
comment|// size of the second, skip the largest, and continue to next...,
comment|// until we meet the compactionThreshold limit.
for|for
control|(
name|point
operator|=
literal|0
init|;
name|point
operator|<
name|countOfFiles
operator|-
literal|1
condition|;
name|point
operator|++
control|)
block|{
if|if
condition|(
operator|(
name|fileSizes
index|[
name|point
index|]
operator|<
name|fileSizes
index|[
name|point
operator|+
literal|1
index|]
operator|*
literal|2
operator|)
operator|&&
operator|(
name|countOfFiles
operator|-
name|point
operator|)
operator|<=
name|maxFilesToCompact
condition|)
block|{
break|break;
block|}
name|skipped
operator|+=
name|fileSizes
index|[
name|point
index|]
expr_stmt|;
block|}
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|filesToCompact
operator|.
name|subList
argument_list|(
name|point
argument_list|,
name|countOfFiles
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<=
literal|1
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipped compaction of 1 file; compaction size of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|": "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|"; Skipped "
operator|+
name|point
operator|+
literal|" files, size: "
operator|+
name|skipped
argument_list|)
expr_stmt|;
block|}
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Compaction size of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|": "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|"; Skipped "
operator|+
name|point
operator|+
literal|" file(s), size: "
operator|+
name|skipped
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Ready to go.  Have list of files to compact.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Started compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s)"
operator|+
operator|(
name|references
condition|?
literal|", hasReferences=true,"
else|:
literal|" "
operator|)
operator|+
literal|" into "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
operator|+
literal|", seqid="
operator|+
name|maxId
argument_list|)
expr_stmt|;
name|HFile
operator|.
name|Writer
name|writer
init|=
name|compact
argument_list|(
name|filesToCompact
argument_list|,
name|majorcompaction
argument_list|,
name|maxId
argument_list|)
decl_stmt|;
comment|// Move the compaction into place.
name|StoreFile
name|sf
init|=
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|writer
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Completed"
operator|+
operator|(
name|majorcompaction
condition|?
literal|" major "
else|:
literal|" "
operator|)
operator|+
literal|"compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|"; new storefile is "
operator|+
operator|(
name|sf
operator|==
literal|null
condition|?
literal|"none"
else|:
name|sf
operator|.
name|toString
argument_list|()
operator|)
operator|+
literal|"; store size is "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|storeSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
comment|/*    * @param files    * @return True if any of the files in<code>files</code> are References.    */
specifier|private
name|boolean
name|hasReferences
parameter_list|(
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|StoreFile
name|hsf
range|:
name|files
control|)
block|{
if|if
condition|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/*    * Gets lowest timestamp from files in a dir    *    * @param fs    * @param dir    * @throws IOException    */
specifier|private
specifier|static
name|long
name|getLowestTimestamp
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
name|FileStatus
index|[]
name|stats
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|)
decl_stmt|;
if|if
condition|(
name|stats
operator|==
literal|null
operator|||
name|stats
operator|.
name|length
operator|==
literal|0
condition|)
block|{
return|return
literal|0l
return|;
block|}
name|long
name|lowTimestamp
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|stats
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|long
name|timestamp
init|=
name|stats
index|[
name|i
index|]
operator|.
name|getModificationTime
argument_list|()
decl_stmt|;
if|if
condition|(
name|timestamp
operator|<
name|lowTimestamp
condition|)
block|{
name|lowTimestamp
operator|=
name|timestamp
expr_stmt|;
block|}
block|}
return|return
name|lowTimestamp
return|;
block|}
comment|/*    * @return True if we should run a major compaction.    */
name|boolean
name|isMajorCompaction
parameter_list|()
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
literal|null
decl_stmt|;
comment|// filesToCompact are sorted oldest to newest.
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|isMajorCompaction
argument_list|(
name|filesToCompact
argument_list|)
return|;
block|}
comment|/*    * @param filesToCompact Files to compact. Can be null.    * @return True if we should run a major compaction.    */
specifier|private
name|boolean
name|isMajorCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|==
literal|null
operator|||
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
name|result
return|;
block|}
name|long
name|lowTimestamp
init|=
name|getLowestTimestamp
argument_list|(
name|fs
argument_list|,
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
argument_list|)
decl_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
if|if
condition|(
name|lowTimestamp
operator|>
literal|0l
operator|&&
name|lowTimestamp
operator|<
operator|(
name|now
operator|-
name|this
operator|.
name|majorCompactionTime
operator|)
condition|)
block|{
comment|// Major compaction time has elapsed.
name|long
name|elapsedTime
init|=
name|now
operator|-
name|lowTimestamp
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
literal|1
operator|&&
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|isMajorCompaction
argument_list|()
operator|&&
operator|(
name|this
operator|.
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|elapsedTime
operator|<
name|this
operator|.
name|ttl
operator|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping major compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" because one (major) compacted file only and elapsedTime "
operator|+
name|elapsedTime
operator|+
literal|"ms is< ttl="
operator|+
name|this
operator|.
name|ttl
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Major compaction triggered on store "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|"; time since last major compaction "
operator|+
operator|(
name|now
operator|-
name|lowTimestamp
operator|)
operator|+
literal|"ms"
argument_list|)
expr_stmt|;
block|}
name|result
operator|=
literal|true
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
comment|/**    * Do a minor/major compaction.  Uses the scan infrastructure to make it easy.    *    * @param filesToCompact which files to compact    * @param majorCompaction true to major compact (prune all deletes, max versions, etc)    * @param maxId Readers maximum sequence id.    * @return Product of compaction or null if all cells expired or deleted and    * nothing made it through the compaction.    * @throws IOException    */
specifier|private
name|HFile
operator|.
name|Writer
name|compact
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|,
specifier|final
name|boolean
name|majorCompaction
parameter_list|,
specifier|final
name|long
name|maxId
parameter_list|)
throws|throws
name|IOException
block|{
comment|// For each file, obtain a scanner:
name|KeyValueScanner
index|[]
name|scanners
init|=
operator|new
name|KeyValueScanner
index|[
name|filesToCompact
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|filesToCompact
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|Reader
name|r
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|filesToCompact
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|// Instantiate HFile.Reader.Scanner to not cache blocks and not use pread
name|scanners
index|[
name|i
index|]
operator|=
operator|new
name|StoreFileScanner
argument_list|(
name|r
operator|.
name|getScanner
argument_list|(
literal|false
argument_list|,
literal|false
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// Make the instantiation lazy in case compaction produces no product; i.e.
comment|// where all source cells are expired or deleted.
name|HFile
operator|.
name|Writer
name|writer
init|=
literal|null
decl_stmt|;
try|try
block|{
if|if
condition|(
name|majorCompaction
condition|)
block|{
name|InternalScanner
name|scanner
init|=
literal|null
decl_stmt|;
try|try
block|{
name|Scan
name|scan
init|=
operator|new
name|Scan
argument_list|()
decl_stmt|;
name|scan
operator|.
name|setMaxVersions
argument_list|(
name|family
operator|.
name|getMaxVersions
argument_list|()
argument_list|)
expr_stmt|;
name|scanner
operator|=
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|scanners
argument_list|)
expr_stmt|;
comment|// since scanner.next() can return 'false' but still be delivering data,
comment|// we have to use a do/while loop.
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
name|kvs
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|more
init|=
literal|true
decl_stmt|;
while|while
condition|(
name|more
condition|)
block|{
name|more
operator|=
name|scanner
operator|.
name|next
argument_list|(
name|kvs
argument_list|)
expr_stmt|;
comment|// output to writer:
for|for
control|(
name|KeyValue
name|kv
range|:
name|kvs
control|)
block|{
if|if
condition|(
name|writer
operator|==
literal|null
condition|)
block|{
name|writer
operator|=
name|getWriter
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
expr_stmt|;
block|}
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
name|kvs
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|scanner
operator|!=
literal|null
condition|)
block|{
name|scanner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|MinorCompactingStoreScanner
name|scanner
init|=
literal|null
decl_stmt|;
try|try
block|{
name|scanner
operator|=
operator|new
name|MinorCompactingStoreScanner
argument_list|(
name|this
argument_list|,
name|scanners
argument_list|)
expr_stmt|;
name|writer
operator|=
name|getWriter
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
expr_stmt|;
while|while
condition|(
name|scanner
operator|.
name|next
argument_list|(
name|writer
argument_list|)
condition|)
block|{
comment|// Nothing to do
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|scanner
operator|!=
literal|null
condition|)
name|scanner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|writer
operator|!=
literal|null
condition|)
block|{
name|StoreFile
operator|.
name|appendMetadata
argument_list|(
name|writer
argument_list|,
name|maxId
argument_list|,
name|majorCompaction
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|writer
return|;
block|}
comment|/*    * It's assumed that the compactLock  will be acquired prior to calling this    * method!  Otherwise, it is not thread-safe!    *    *<p>It works by processing a compaction that's been written to disk.    *    *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *    *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Moving the new compacted StoreFile into place    * 2) Unload all replaced StoreFile, close and collect list to delete.    * 3) Loading the new TreeMap.    * 4) Compute new store size    *</pre>    *    * @param compactedFiles list of files that were compacted    * @param compactedFile StoreFile that is the result of the compaction    * @return StoreFile created. May be null.    * @throws IOException    */
specifier|private
name|StoreFile
name|completeCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|compactedFiles
parameter_list|,
specifier|final
name|HFile
operator|.
name|Writer
name|compactedFile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// 1. Moving the new files into place -- if there is a new file (may not
comment|// be if all cells were expired or deleted).
name|StoreFile
name|result
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|compactedFile
operator|!=
literal|null
condition|)
block|{
name|Path
name|p
init|=
literal|null
decl_stmt|;
try|try
block|{
name|p
operator|=
name|StoreFile
operator|.
name|rename
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|this
operator|.
name|homedir
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|result
operator|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|p
argument_list|,
name|blockcache
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|inMemory
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
try|try
block|{
comment|// 2. Unloading
comment|// 3. Loading the new TreeMap.
comment|// Change this.storefiles so it reflects new state but do not
comment|// delete old store files until we have sent out notification of
comment|// change in case old files are still being accessed by outstanding
comment|// scanners.
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
name|compactedFiles
operator|.
name|contains
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
condition|)
block|{
name|this
operator|.
name|storefiles
operator|.
name|remove
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|// If a StoreFile result, move it into place.  May be null.
if|if
condition|(
name|result
operator|!=
literal|null
condition|)
block|{
name|Long
name|orderVal
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|result
operator|.
name|getMaxSequenceId
argument_list|()
argument_list|)
decl_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
comment|// WARN ugly hack here, but necessary sadly.
name|ReadWriteConsistencyControl
operator|.
name|resetThreadReadPoint
argument_list|(
name|region
operator|.
name|getRWCC
argument_list|()
argument_list|)
expr_stmt|;
comment|// Tell observers that list of StoreFiles has changed.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
comment|// Finally, delete old store files.
for|for
control|(
name|StoreFile
name|hsf
range|:
name|compactedFiles
control|)
block|{
name|hsf
operator|.
name|delete
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|=
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files in "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|". Compacted file is "
operator|+
operator|(
name|result
operator|==
literal|null
condition|?
literal|"none"
else|:
name|result
operator|.
name|toString
argument_list|()
operator|)
operator|+
literal|".  Files replaced "
operator|+
name|compactedFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// 4. Compute new store size
name|this
operator|.
name|storeSize
operator|=
literal|0L
expr_stmt|;
for|for
control|(
name|StoreFile
name|hsf
range|:
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
name|Reader
name|r
init|=
name|hsf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|hsf
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
comment|// ////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * @return the number of files in this store    */
specifier|public
name|int
name|getNumberOfstorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/*    * @param wantedVersions How many versions were asked for.    * @return wantedVersions or this families' VERSIONS.    */
name|int
name|versionsToReturn
parameter_list|(
specifier|final
name|int
name|wantedVersions
parameter_list|)
block|{
if|if
condition|(
name|wantedVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
comment|// Make sure we do not return more than maximum versions for this store.
name|int
name|maxVersions
init|=
name|this
operator|.
name|family
operator|.
name|getMaxVersions
argument_list|()
decl_stmt|;
return|return
name|wantedVersions
operator|>
name|maxVersions
condition|?
name|maxVersions
else|:
name|wantedVersions
return|;
block|}
specifier|static
name|void
name|expiredOrDeleted
parameter_list|(
specifier|final
name|Set
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|,
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|boolean
name|b
init|=
name|set
operator|.
name|remove
argument_list|(
name|kv
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|kv
operator|.
name|toString
argument_list|()
operator|+
literal|" expired: "
operator|+
name|b
argument_list|)
expr_stmt|;
block|}
block|}
specifier|static
name|boolean
name|isExpired
parameter_list|(
specifier|final
name|KeyValue
name|key
parameter_list|,
specifier|final
name|long
name|oldestTimestamp
parameter_list|)
block|{
return|return
name|key
operator|.
name|getTimestamp
argument_list|()
operator|<
name|oldestTimestamp
return|;
block|}
comment|/**    * Find the key that matches<i>row</i> exactly, or the one that immediately    * preceeds it. WARNING: Only use this method on a table where writes occur    * with strictly increasing timestamps. This method assumes this pattern of    * writes in order to make it reasonably performant.  Also our search is    * dependent on the axiom that deletes are for cells that are in the container    * that follows whether a memstore snapshot or a storefile, not for the    * current container: i.e. we'll see deletes before we come across cells we    * are to delete. Presumption is that the memstore#kvset is processed before    * memstore#snapshot and so on.    * @param kv First possible item on targeted row; i.e. empty columns, latest    * timestamp and maximum type.    * @return Found keyvalue or null if none found.    * @throws IOException    */
name|KeyValue
name|getRowKeyAtOrBefore
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
throws|throws
name|IOException
block|{
name|GetClosestRowBeforeTracker
name|state
init|=
operator|new
name|GetClosestRowBeforeTracker
argument_list|(
name|this
operator|.
name|comparator
argument_list|,
name|kv
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// First go to the memstore.  Pick up deletes and candidates.
name|this
operator|.
name|memstore
operator|.
name|getRowKeyAtOrBefore
argument_list|(
name|state
argument_list|)
expr_stmt|;
comment|// Check if match, if we got a candidate on the asked for 'kv' row.
comment|// Process each store file. Run through from newest to oldest.
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|m
init|=
name|this
operator|.
name|storefiles
operator|.
name|descendingMap
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|e
range|:
name|m
operator|.
name|entrySet
argument_list|()
control|)
block|{
comment|// Update the candidate keys from the current map file
name|rowAtOrBeforeFromStoreFile
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
return|return
name|state
operator|.
name|getCandidate
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Check an individual MapFile for the row at or before a given row.    * @param f    * @param state    * @throws IOException    */
specifier|private
name|void
name|rowAtOrBeforeFromStoreFile
parameter_list|(
specifier|final
name|StoreFile
name|f
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|Reader
name|r
init|=
name|f
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|f
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// TODO: Cache these keys rather than make each time?
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
name|KeyValue
name|firstOnRow
init|=
name|state
operator|.
name|getTargetKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|lastKV
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
block|{
comment|// If last key in file is not of the target table, no candidates in this
comment|// file.  Return.
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|lastKV
argument_list|)
condition|)
return|return;
comment|// If the row we're looking for is past the end of file, set search key to
comment|// last key. TODO: Cache last and first key rather than make each time.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|lastKV
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
block|}
comment|// Get a scanner that caches blocks and that uses pread.
name|HFileScanner
name|scanner
init|=
name|r
operator|.
name|getScanner
argument_list|(
literal|true
argument_list|,
literal|true
argument_list|)
decl_stmt|;
comment|// Seek scanner.  If can't seek it, return.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
return|return;
comment|// If we found candidate on firstOnRow, just return. THIS WILL NEVER HAPPEN!
comment|// Unlikely that there'll be an instance of actual first row in table.
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
return|return;
comment|// If here, need to start backing up.
while|while
condition|(
name|scanner
operator|.
name|seekBefore
argument_list|(
name|firstOnRow
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyLength
argument_list|()
argument_list|)
condition|)
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
if|if
condition|(
operator|!
name|state
operator|.
name|isBetterCandidate
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
comment|// Make new first on row.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|kv
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
comment|// Seek scanner.  If can't seek it, break.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
break|break;
comment|// If we find something, break;
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
break|break;
block|}
block|}
comment|/*    * Seek the file scanner to firstOnRow or first entry in file.    * @param scanner    * @param firstOnRow    * @param firstKV    * @return True if we successfully seeked scanner.    * @throws IOException    */
specifier|private
name|boolean
name|seekToScanner
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|KeyValue
name|firstKV
parameter_list|)
throws|throws
name|IOException
block|{
name|KeyValue
name|kv
init|=
name|firstOnRow
decl_stmt|;
comment|// If firstOnRow< firstKV, set to firstKV
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|firstKV
argument_list|,
name|firstOnRow
argument_list|)
operator|==
literal|0
condition|)
name|kv
operator|=
name|firstKV
expr_stmt|;
name|int
name|result
init|=
name|scanner
operator|.
name|seekTo
argument_list|(
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|result
operator|>=
literal|0
return|;
block|}
comment|/*    * When we come in here, we are probably at the kv just before we break into    * the row that firstOnRow is on.  Usually need to increment one time to get    * on to the row we are interested in.    * @param scanner    * @param firstOnRow    * @param state    * @return True we found a candidate.    * @throws IOException    */
specifier|private
name|boolean
name|walkForwardInSingleRow
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|foundCandidate
init|=
literal|false
decl_stmt|;
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
comment|// If we are not in the row, skip.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
continue|continue;
comment|// Did we go beyond the target row? If so break.
if|if
condition|(
name|state
operator|.
name|isTooFar
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
condition|)
break|break;
if|if
condition|(
name|state
operator|.
name|isExpired
argument_list|(
name|kv
argument_list|)
condition|)
block|{
continue|continue;
block|}
comment|// If we added something, this row is a contender. break.
if|if
condition|(
name|state
operator|.
name|handle
argument_list|(
name|kv
argument_list|)
condition|)
block|{
name|foundCandidate
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
return|return
name|foundCandidate
return|;
block|}
comment|/**    * Determines if HStore can be split    * @param force Whether to force a split or not.    * @return a StoreSize if store can be split, null otherwise.    */
name|StoreSize
name|checkSplit
parameter_list|(
specifier|final
name|boolean
name|force
parameter_list|)
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Iterate through all store files
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
operator|!
name|force
operator|&&
operator|(
name|storeSize
operator|<
name|this
operator|.
name|desiredMaxFileSize
operator|)
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
condition|)
block|{
if|if
condition|(
name|force
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Cannot split meta regions in HBase 0.20"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|// Not splitable if we find a reference store file present in the store.
name|boolean
name|splitable
init|=
literal|true
decl_stmt|;
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|Long
name|mapIndex
init|=
name|Long
operator|.
name|valueOf
argument_list|(
literal|0L
argument_list|)
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|e
range|:
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|StoreFile
name|sf
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|splitable
condition|)
block|{
name|splitable
operator|=
operator|!
name|sf
operator|.
name|isReference
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|splitable
condition|)
block|{
comment|// RETURN IN MIDDLE OF FUNCTION!!! If not splitable, just return.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|sf
operator|+
literal|" is not splittable"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
block|}
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|sf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|size
init|=
name|r
operator|.
name|length
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|mapIndex
operator|=
name|e
operator|.
name|getKey
argument_list|()
expr_stmt|;
block|}
block|}
name|StoreFile
name|sf
init|=
name|this
operator|.
name|storefiles
operator|.
name|get
argument_list|(
name|mapIndex
argument_list|)
decl_stmt|;
name|HFile
operator|.
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|sf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// Get first, last, and mid keys.  Midkey is the key that starts block
comment|// in middle of hfile.  Has column and timestamp.  Need to return just
comment|// the row we want to split on as midkey.
name|byte
index|[]
name|midkey
init|=
name|r
operator|.
name|midkey
argument_list|()
decl_stmt|;
if|if
condition|(
name|midkey
operator|!=
literal|null
condition|)
block|{
name|KeyValue
name|mk
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|midkey
argument_list|,
literal|0
argument_list|,
name|midkey
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
comment|// if the midkey is the same as the first and last keys, then we cannot
comment|// (ever) split this region.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|firstKey
argument_list|)
operator|==
literal|0
operator|&&
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|lastKey
argument_list|)
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"cannot split because midkey is the same as first or "
operator|+
literal|"last row"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
return|return
operator|new
name|StoreSize
argument_list|(
name|maxSize
argument_list|,
name|mk
operator|.
name|getRow
argument_list|()
argument_list|)
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed getting store size for "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** @return aggregate size of HStore */
specifier|public
name|long
name|getSize
parameter_list|()
block|{
return|return
name|storeSize
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return a scanner for both the memstore and the HStore files    */
specifier|protected
name|KeyValueScanner
name|getScanner
parameter_list|(
name|Scan
name|scan
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|targetCols
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|targetCols
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|this
operator|.
name|storeNameStr
return|;
block|}
comment|/**    * @return Count of store files    */
name|int
name|getStorefilesCount
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/**    * @return The size of the store files, in bytes.    */
name|long
name|getStorefilesSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * @return The size of the store file indexes, in bytes.    */
name|long
name|getStorefilesIndexSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|indexSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/*    * Datastructure that holds size and row to split a file around.    * TODO: Take a KeyValue rather than row.    */
specifier|static
class|class
name|StoreSize
block|{
specifier|private
specifier|final
name|long
name|size
decl_stmt|;
specifier|private
specifier|final
name|byte
index|[]
name|row
decl_stmt|;
name|StoreSize
parameter_list|(
name|long
name|size
parameter_list|,
name|byte
index|[]
name|row
parameter_list|)
block|{
name|this
operator|.
name|size
operator|=
name|size
expr_stmt|;
name|this
operator|.
name|row
operator|=
name|row
expr_stmt|;
block|}
comment|/* @return the size */
name|long
name|getSize
parameter_list|()
block|{
return|return
name|size
return|;
block|}
name|byte
index|[]
name|getSplitRow
parameter_list|()
block|{
return|return
name|this
operator|.
name|row
return|;
block|}
block|}
name|HRegion
name|getHRegion
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
return|;
block|}
name|HRegionInfo
name|getHRegionInfo
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
operator|.
name|regionInfo
return|;
block|}
comment|/**    * Convenience method that implements the old MapFile.getClosest on top of    * HFile Scanners.  getClosest used seek to the asked-for key or just after    * (HFile seeks to the key or just before).    * @param s Scanner to use    * @param kv Key to find.    * @return True if we were able to seek the scanner to<code>b</code> or to    * the key just after.    * @throws IOException    */
specifier|static
name|boolean
name|getClosest
parameter_list|(
specifier|final
name|HFileScanner
name|s
parameter_list|,
specifier|final
name|KeyValue
name|kv
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Pass offsets to key content of a KeyValue; thats whats in the hfile index.
name|int
name|result
init|=
name|s
operator|.
name|seekTo
argument_list|(
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|result
operator|<
literal|0
condition|)
block|{
comment|// Not in file.  Will the first key do?
if|if
condition|(
operator|!
name|s
operator|.
name|seekTo
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
elseif|else
if|if
condition|(
name|result
operator|>
literal|0
condition|)
block|{
comment|// Less than what was asked for but maybe< because we're asking for
comment|// r/c/LATEST_TIMESTAMP -- what was returned was r/c-1/SOME_TS...
comment|// A next will get us a r/c/SOME_TS.
if|if
condition|(
operator|!
name|s
operator|.
name|next
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Retrieve results from this store given the specified Get parameters.    * @param get Get operation    * @param columns List of columns to match, can be empty (not null)    * @param result List to add results to    * @throws IOException    */
specifier|public
name|void
name|get
parameter_list|(
name|Get
name|get
parameter_list|,
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|KeyValue
argument_list|>
name|result
parameter_list|)
throws|throws
name|IOException
block|{
name|KeyComparator
name|keyComparator
init|=
name|this
operator|.
name|comparator
operator|.
name|getRawComparator
argument_list|()
decl_stmt|;
comment|// Column matching and version enforcement
name|QueryMatcher
name|matcher
init|=
operator|new
name|QueryMatcher
argument_list|(
name|get
argument_list|,
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|,
name|columns
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|keyComparator
argument_list|,
name|versionsToReturn
argument_list|(
name|get
operator|.
name|getMaxVersions
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Read from memstore
if|if
condition|(
name|this
operator|.
name|memstore
operator|.
name|get
argument_list|(
name|matcher
argument_list|,
name|result
argument_list|)
condition|)
block|{
comment|// Received early-out from memstore
return|return;
block|}
comment|// Check if we even have storefiles
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return;
block|}
comment|// Get storefiles for this store
name|List
argument_list|<
name|HFileScanner
argument_list|>
name|storefileScanners
init|=
operator|new
name|ArrayList
argument_list|<
name|HFileScanner
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|StoreFile
name|sf
range|:
name|this
operator|.
name|storefiles
operator|.
name|descendingMap
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|HFile
operator|.
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|sf
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|// Get a scanner that caches the block and uses pread
name|storefileScanners
operator|.
name|add
argument_list|(
name|r
operator|.
name|getScanner
argument_list|(
literal|true
argument_list|,
literal|true
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// StoreFileGetScan will handle reading this store's storefiles
name|StoreFileGetScan
name|scanner
init|=
operator|new
name|StoreFileGetScan
argument_list|(
name|storefileScanners
argument_list|,
name|matcher
argument_list|)
decl_stmt|;
comment|// Run a GET scan and put results into the specified list
name|scanner
operator|.
name|get
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Increments the value for the given row/family/qualifier.    *    * This function will always be seen as atomic by other readers    * because it only puts a single KV to memstore. Thus no    * read/write control necessary.    *     * @param row    * @param f    * @param qualifier    * @param newValue the new value to set into memstore    * @return memstore size delta    * @throws IOException    */
specifier|public
name|long
name|updateColumnValue
parameter_list|(
name|byte
index|[]
name|row
parameter_list|,
name|byte
index|[]
name|f
parameter_list|,
name|byte
index|[]
name|qualifier
parameter_list|,
name|long
name|newValue
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|KeyValue
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
name|KeyComparator
name|keyComparator
init|=
name|this
operator|.
name|comparator
operator|.
name|getRawComparator
argument_list|()
decl_stmt|;
name|KeyValue
name|kv
init|=
literal|null
decl_stmt|;
comment|// Setting up the QueryMatcher
name|Get
name|get
init|=
operator|new
name|Get
argument_list|(
name|row
argument_list|)
decl_stmt|;
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|qualifiers
init|=
operator|new
name|TreeSet
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|(
name|Bytes
operator|.
name|BYTES_COMPARATOR
argument_list|)
decl_stmt|;
name|qualifiers
operator|.
name|add
argument_list|(
name|qualifier
argument_list|)
expr_stmt|;
name|QueryMatcher
name|matcher
init|=
operator|new
name|QueryMatcher
argument_list|(
name|get
argument_list|,
name|f
argument_list|,
name|qualifiers
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|keyComparator
argument_list|,
literal|1
argument_list|)
decl_stmt|;
comment|// lock memstore snapshot for this critical section:
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
name|memstore
operator|.
name|readLockLock
argument_list|()
expr_stmt|;
try|try
block|{
name|int
name|memstoreCode
init|=
name|this
operator|.
name|memstore
operator|.
name|getWithCode
argument_list|(
name|matcher
argument_list|,
name|result
argument_list|)
decl_stmt|;
if|if
condition|(
name|memstoreCode
operator|!=
literal|0
condition|)
block|{
comment|// was in memstore (or snapshot)
name|kv
operator|=
name|result
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|clone
argument_list|()
expr_stmt|;
name|byte
index|[]
name|buffer
init|=
name|kv
operator|.
name|getBuffer
argument_list|()
decl_stmt|;
name|int
name|valueOffset
init|=
name|kv
operator|.
name|getValueOffset
argument_list|()
decl_stmt|;
name|Bytes
operator|.
name|putBytes
argument_list|(
name|buffer
argument_list|,
name|valueOffset
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|newValue
argument_list|)
argument_list|,
literal|0
argument_list|,
name|Bytes
operator|.
name|SIZEOF_LONG
argument_list|)
expr_stmt|;
if|if
condition|(
name|memstoreCode
operator|==
literal|2
condition|)
block|{
comment|// from snapshot, assign new TS
name|long
name|currTs
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
if|if
condition|(
name|currTs
operator|==
name|kv
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|currTs
operator|++
expr_stmt|;
comment|// unlikely but catastrophic
block|}
name|Bytes
operator|.
name|putBytes
argument_list|(
name|buffer
argument_list|,
name|kv
operator|.
name|getTimestampOffset
argument_list|()
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|currTs
argument_list|)
argument_list|,
literal|0
argument_list|,
name|Bytes
operator|.
name|SIZEOF_LONG
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|kv
operator|=
operator|new
name|KeyValue
argument_list|(
name|row
argument_list|,
name|f
argument_list|,
name|qualifier
argument_list|,
name|System
operator|.
name|currentTimeMillis
argument_list|()
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|newValue
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|add
argument_list|(
name|kv
argument_list|)
return|;
comment|// end lock
block|}
finally|finally
block|{
name|memstore
operator|.
name|readLockUnlock
argument_list|()
expr_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|public
name|StoreFlusher
name|getStoreFlusher
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
return|return
operator|new
name|StoreFlusherImpl
argument_list|(
name|cacheFlushId
argument_list|)
return|;
block|}
specifier|private
class|class
name|StoreFlusherImpl
implements|implements
name|StoreFlusher
block|{
specifier|private
name|long
name|cacheFlushId
decl_stmt|;
specifier|private
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
decl_stmt|;
specifier|private
name|StoreFile
name|storeFile
decl_stmt|;
specifier|private
name|StoreFlusherImpl
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
name|this
operator|.
name|cacheFlushId
operator|=
name|cacheFlushId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|prepare
parameter_list|()
block|{
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
name|this
operator|.
name|snapshot
operator|=
name|memstore
operator|.
name|getSnapshot
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|flushCache
parameter_list|()
throws|throws
name|IOException
block|{
name|storeFile
operator|=
name|Store
operator|.
name|this
operator|.
name|flushCache
argument_list|(
name|cacheFlushId
argument_list|,
name|snapshot
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|commit
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|storeFile
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Add new file to store files.  Clear snapshot too while we have
comment|// the Store write lock.
return|return
name|Store
operator|.
name|this
operator|.
name|updateStorefiles
argument_list|(
name|cacheFlushId
argument_list|,
name|storeFile
argument_list|,
name|snapshot
argument_list|)
return|;
block|}
block|}
comment|/**    * See if there's too much store files in this store    * @return true if number of store files is greater than    *  the number defined in compactionThreshold    */
specifier|public
name|boolean
name|hasTooManyStoreFiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>
name|this
operator|.
name|compactionThreshold
return|;
block|}
specifier|public
specifier|static
specifier|final
name|long
name|FIXED_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|ClassSize
operator|.
name|OBJECT
operator|+
operator|(
literal|17
operator|*
name|ClassSize
operator|.
name|REFERENCE
operator|)
operator|+
operator|(
literal|6
operator|*
name|Bytes
operator|.
name|SIZEOF_LONG
operator|)
operator|+
operator|(
literal|3
operator|*
name|Bytes
operator|.
name|SIZEOF_INT
operator|)
operator|+
name|Bytes
operator|.
name|SIZEOF_BOOLEAN
operator|+
name|ClassSize
operator|.
name|align
argument_list|(
name|ClassSize
operator|.
name|ARRAY
argument_list|)
argument_list|)
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|long
name|DEEP_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|FIXED_OVERHEAD
operator|+
name|ClassSize
operator|.
name|OBJECT
operator|+
name|ClassSize
operator|.
name|REENTRANT_LOCK
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP_ENTRY
operator|+
name|ClassSize
operator|.
name|OBJECT
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|long
name|heapSize
parameter_list|()
block|{
return|return
name|DEEP_OVERHEAD
operator|+
name|this
operator|.
name|memstore
operator|.
name|heapSize
argument_list|()
return|;
block|}
block|}
end_class

end_unit

