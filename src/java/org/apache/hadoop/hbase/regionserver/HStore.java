begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2007 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|locks
operator|.
name|ReentrantReadWriteLock
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Matcher
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Pattern
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataInputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|filter
operator|.
name|RowFilterInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|ImmutableBytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|TextSequence
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|MapFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Progressable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|BloomFilterDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|BloomFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|CountingBloomFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|Filter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|RetouchedBloomFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HBaseConfiguration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HRegionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HStoreKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|RemoteExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|Cell
import|;
end_import

begin_comment
comment|/**  * HStore maintains a bunch of data files.  It is responsible for maintaining   * the memory/file hierarchy and for periodic flushes to disk and compacting   * edits to the file.  *  * Locking and transactions are handled at a higher level.  This API should not   * be called directly by any writer, but rather by an HRegion manager.  */
end_comment

begin_class
specifier|public
class|class
name|HStore
implements|implements
name|HConstants
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HStore
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/*    * Regex that will work for straight filenames and for reference names.    * If reference, then the regex has more than just one group.  Group 1 is    * this files id.  Group 2 the referenced region name, etc.    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|REF_NAME_PARSER
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^(\\d+)(?:\\.(.+))?$"
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|BLOOMFILTER_FILE_NAME
init|=
literal|"filter"
decl_stmt|;
specifier|protected
specifier|final
name|Memcache
name|memcache
init|=
operator|new
name|Memcache
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|Path
name|basedir
decl_stmt|;
specifier|private
specifier|final
name|HRegionInfo
name|info
decl_stmt|;
specifier|private
specifier|final
name|HColumnDescriptor
name|family
decl_stmt|;
specifier|private
specifier|final
name|SequenceFile
operator|.
name|CompressionType
name|compression
decl_stmt|;
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|HBaseConfiguration
name|conf
decl_stmt|;
specifier|private
specifier|final
name|Path
name|filterDir
decl_stmt|;
specifier|final
name|Filter
name|bloomFilter
decl_stmt|;
specifier|private
specifier|final
name|long
name|desiredMaxFileSize
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|storeSize
decl_stmt|;
specifier|private
specifier|final
name|Integer
name|flushLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|ReentrantReadWriteLock
name|lock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
specifier|final
name|AtomicInteger
name|activeScanners
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|final
name|Text
name|storeName
decl_stmt|;
comment|/*    * Sorted Map of readers keyed by sequence id (Most recent should be last in    * in list).    */
specifier|final
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|storefiles
init|=
name|Collections
operator|.
name|synchronizedSortedMap
argument_list|(
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
comment|/*    * Sorted Map of readers keyed by sequence id (Most recent should be last in    * in list).    */
specifier|private
specifier|final
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
name|readers
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
argument_list|()
decl_stmt|;
comment|// The most-recent log-seq-ID that's present.  The most-recent such ID means
comment|// we can ignore all log messages up to and including that ID (because they're
comment|// already reflected in the TreeMaps).
specifier|private
specifier|volatile
name|long
name|maxSeqId
decl_stmt|;
specifier|private
specifier|final
name|Path
name|compactionDir
decl_stmt|;
specifier|private
specifier|final
name|Integer
name|compactLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|int
name|compactionThreshold
decl_stmt|;
specifier|private
specifier|final
name|ReentrantReadWriteLock
name|newScannerLock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
comment|/**    * An HStore is a set of zero or more MapFiles, which stretch backwards over     * time.  A given HStore is responsible for a certain set of columns for a    * row in the HRegion.    *    *<p>The HRegion starts writing to its set of HStores when the HRegion's     * memcache is flushed.  This results in a round of new MapFiles, one for    * each HStore.    *    *<p>There's no reason to consider append-logging at this level; all logging     * and locking is handled at the HRegion level.  HStore just provides    * services to manage sets of MapFiles.  One of the most important of those    * services is MapFile-compaction services.    *    *<p>The only thing having to do with logs that HStore needs to deal with is    * the reconstructionLog.  This is a segment of an HRegion's log that might    * NOT be present upon startup.  If the param is NULL, there's nothing to do.    * If the param is non-NULL, we need to process the log to reconstruct    * a TreeMap that might not have been written to disk before the process    * died.    *    *<p>It's assumed that after this constructor returns, the reconstructionLog    * file will be deleted (by whoever has instantiated the HStore).    *    * @param basedir qualified path under which the region directory lives    * @param info HRegionInfo for this region    * @param family HColumnDescriptor for this column    * @param fs file system object    * @param reconstructionLog existing log file to apply if any    * @param conf configuration object    * @param reporter Call on a period so hosting server can report we're    * making progress to master -- otherwise master might think region deploy    * failed.  Can be null.    * @throws IOException    */
name|HStore
parameter_list|(
name|Path
name|basedir
parameter_list|,
name|HRegionInfo
name|info
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|reconstructionLog
parameter_list|,
name|HBaseConfiguration
name|conf
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|basedir
operator|=
name|basedir
expr_stmt|;
name|this
operator|.
name|info
operator|=
name|info
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|compactionDir
operator|=
name|HRegion
operator|.
name|getCompactionDir
argument_list|(
name|basedir
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeName
operator|=
operator|new
name|Text
argument_list|(
name|this
operator|.
name|info
operator|.
name|getEncodedName
argument_list|()
operator|+
literal|"/"
operator|+
name|this
operator|.
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|)
expr_stmt|;
comment|// By default, we compact if an HStore has more than
comment|// MIN_COMMITS_FOR_COMPACTION map files
name|this
operator|.
name|compactionThreshold
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
expr_stmt|;
comment|// By default we split region if a file> DEFAULT_MAX_FILE_SIZE.
name|this
operator|.
name|desiredMaxFileSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hregion.max.filesize"
argument_list|,
name|DEFAULT_MAX_FILE_SIZE
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeSize
operator|=
literal|0L
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|BLOCK
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|BLOCK
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|RECORD
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|RECORD
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|NONE
expr_stmt|;
block|}
name|Path
name|mapdir
init|=
name|HStoreFile
operator|.
name|getMapDir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|mapdir
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|mapdir
argument_list|)
expr_stmt|;
block|}
name|Path
name|infodir
init|=
name|HStoreFile
operator|.
name|getInfoDir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|infodir
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|infodir
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|==
literal|null
condition|)
block|{
name|this
operator|.
name|filterDir
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|bloomFilter
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|filterDir
operator|=
name|HStoreFile
operator|.
name|getFilterDir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|filterDir
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|filterDir
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|bloomFilter
operator|=
name|loadOrCreateBloomFilter
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"starting "
operator|+
name|storeName
argument_list|)
expr_stmt|;
block|}
comment|// Go through the 'mapdir' and 'infodir' together, make sure that all
comment|// MapFiles are in a reliable state.  Every entry in 'mapdir' must have a
comment|// corresponding one in 'loginfodir'. Without a corresponding log info
comment|// file, the entry in 'mapdir' must be deleted.
comment|// loadHStoreFiles also computes the max sequence id
name|this
operator|.
name|maxSeqId
operator|=
operator|-
literal|1L
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|putAll
argument_list|(
name|loadHStoreFiles
argument_list|(
name|infodir
argument_list|,
name|mapdir
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"maximum sequence id for hstore "
operator|+
name|storeName
operator|+
literal|" is "
operator|+
name|this
operator|.
name|maxSeqId
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|doReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|maxSeqId
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// Presume we got here because of some HDFS issue or because of a lack of
comment|// HADOOP-1700; for now keep going but this is probably not what we want
comment|// long term.  If we got here there has been data-loss
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception processing reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" opening "
operator|+
name|this
operator|.
name|storeName
operator|+
literal|" -- continuing.  Probably DATA LOSS!"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// Move maxSeqId on by one. Why here?  And not in HRegion?
name|this
operator|.
name|maxSeqId
operator|+=
literal|1
expr_stmt|;
comment|// Finally, start up all the map readers! (There could be more than one
comment|// since we haven't compacted yet.)
name|boolean
name|first
init|=
literal|true
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
name|first
condition|)
block|{
comment|// Use a block cache (if configured) for the first reader only
comment|// so as to control memory usage.
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|,
name|e
operator|.
name|getValue
argument_list|()
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|,
name|family
operator|.
name|isBlockCacheEnabled
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|first
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|,
name|e
operator|.
name|getValue
argument_list|()
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|HColumnDescriptor
name|getFamily
parameter_list|()
block|{
return|return
name|this
operator|.
name|family
return|;
block|}
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|this
operator|.
name|maxSeqId
return|;
block|}
comment|/*    * Read the reconstructionLog to see whether we need to build a brand-new     * MapFile out of non-flushed log entries.      *    * We can ignore any log message that has a sequence ID that's equal to or     * lower than maxSeqID.  (Because we know such log messages are already     * reflected in the MapFiles.)    */
specifier|private
name|void
name|doReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|maxSeqID
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|UnsupportedEncodingException
throws|,
name|IOException
block|{
if|if
condition|(
name|reconstructionLog
operator|==
literal|null
operator|||
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
condition|)
block|{
comment|// Nothing to do.
return|return;
block|}
comment|// Check its not empty.
name|FileStatus
index|[]
name|stats
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|reconstructionLog
argument_list|)
decl_stmt|;
if|if
condition|(
name|stats
operator|==
literal|null
operator|||
name|stats
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Passed reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" is zero-length"
argument_list|)
expr_stmt|;
return|return;
block|}
name|long
name|maxSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|reconstructedCache
init|=
operator|new
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|()
decl_stmt|;
name|SequenceFile
operator|.
name|Reader
name|logReader
init|=
operator|new
name|SequenceFile
operator|.
name|Reader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|reconstructionLog
argument_list|,
name|this
operator|.
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|HLogKey
name|key
init|=
operator|new
name|HLogKey
argument_list|()
decl_stmt|;
name|HLogEdit
name|val
init|=
operator|new
name|HLogEdit
argument_list|()
decl_stmt|;
name|long
name|skippedEdits
init|=
literal|0
decl_stmt|;
name|long
name|editsCount
init|=
literal|0
decl_stmt|;
comment|// How many edits to apply before we send a progress report.
name|int
name|reportInterval
init|=
name|this
operator|.
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.report.interval.edits"
argument_list|,
literal|2000
argument_list|)
decl_stmt|;
while|while
condition|(
name|logReader
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
condition|)
block|{
name|maxSeqIdInLog
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSeqIdInLog
argument_list|,
name|key
operator|.
name|getLogSeqNum
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|<=
name|maxSeqID
condition|)
block|{
name|skippedEdits
operator|++
expr_stmt|;
continue|continue;
block|}
comment|// Check this edit is for me. Also, guard against writing
comment|// METACOLUMN info such as HBASE::CACHEFLUSH entries
name|Text
name|column
init|=
name|val
operator|.
name|getColumn
argument_list|()
decl_stmt|;
if|if
condition|(
name|column
operator|.
name|equals
argument_list|(
name|HLog
operator|.
name|METACOLUMN
argument_list|)
operator|||
operator|!
name|key
operator|.
name|getRegionName
argument_list|()
operator|.
name|equals
argument_list|(
name|info
operator|.
name|getRegionName
argument_list|()
argument_list|)
operator|||
operator|!
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|column
argument_list|)
operator|.
name|equals
argument_list|(
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|HStoreKey
name|k
init|=
operator|new
name|HStoreKey
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|column
argument_list|,
name|val
operator|.
name|getTimestamp
argument_list|()
argument_list|)
decl_stmt|;
name|reconstructedCache
operator|.
name|put
argument_list|(
name|k
argument_list|,
name|val
operator|.
name|getVal
argument_list|()
argument_list|)
expr_stmt|;
name|editsCount
operator|++
expr_stmt|;
comment|// Every 2k edits, tell the reporter we're making progress.
comment|// Have seen 60k edits taking 3minutes to complete.
if|if
condition|(
name|reporter
operator|!=
literal|null
operator|&&
operator|(
name|editsCount
operator|%
name|reportInterval
operator|)
operator|==
literal|0
condition|)
block|{
name|reporter
operator|.
name|progress
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Applied "
operator|+
name|editsCount
operator|+
literal|", skipped "
operator|+
name|skippedEdits
operator|+
literal|" because sequence id<= "
operator|+
name|maxSeqID
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|logReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|reconstructedCache
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// We create a "virtual flush" at maxSeqIdInLog+1.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing reconstructionCache"
argument_list|)
expr_stmt|;
block|}
name|internalFlushCache
argument_list|(
name|reconstructedCache
argument_list|,
name|maxSeqIdInLog
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
block|}
comment|/*    * Creates a series of HStoreFiles loaded from the given directory.    * There must be a matching 'mapdir' and 'loginfo' pair of files.    * If only one exists, we'll delete it.    *    * @param infodir qualified path for info file directory    * @param mapdir qualified path for map file directory    * @throws IOException    */
specifier|private
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|loadHStoreFiles
parameter_list|(
name|Path
name|infodir
parameter_list|,
name|Path
name|mapdir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"infodir: "
operator|+
name|infodir
operator|.
name|toString
argument_list|()
operator|+
literal|" mapdir: "
operator|+
name|mapdir
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// Look first at info files.  If a reference, these contain info we need
comment|// to create the HStoreFile.
name|FileStatus
name|infofiles
index|[]
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|infodir
argument_list|)
decl_stmt|;
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|results
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|Path
argument_list|>
name|mapfiles
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|(
name|infofiles
operator|.
name|length
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|infofiles
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|Path
name|p
init|=
name|infofiles
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Matcher
name|m
init|=
name|REF_NAME_PARSER
operator|.
name|matcher
argument_list|(
name|p
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
comment|/*        *  *  *  *  *  N O T E  *  *  *  *  *        *          *  We call isReference(Path, Matcher) here because it calls        *  Matcher.matches() which must be called before Matcher.group(int)        *  and we don't want to call Matcher.matches() twice.        *          *  *  *  *  *  N O T E  *  *  *  *  *        */
name|boolean
name|isReference
init|=
name|isReference
argument_list|(
name|p
argument_list|,
name|m
argument_list|)
decl_stmt|;
name|long
name|fid
init|=
name|Long
operator|.
name|parseLong
argument_list|(
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loading file "
operator|+
name|p
operator|.
name|toString
argument_list|()
operator|+
literal|", isReference="
operator|+
name|isReference
operator|+
literal|", file id="
operator|+
name|fid
argument_list|)
expr_stmt|;
block|}
name|HStoreFile
name|curfile
init|=
literal|null
decl_stmt|;
name|HStoreFile
operator|.
name|Reference
name|reference
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isReference
condition|)
block|{
name|reference
operator|=
name|readSplitInfo
argument_list|(
name|p
argument_list|,
name|fs
argument_list|)
expr_stmt|;
block|}
name|curfile
operator|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|,
name|fid
argument_list|,
name|reference
argument_list|)
expr_stmt|;
name|storeSize
operator|+=
name|curfile
operator|.
name|length
argument_list|()
expr_stmt|;
name|long
name|storeSeqId
init|=
operator|-
literal|1
decl_stmt|;
try|try
block|{
name|storeSeqId
operator|=
name|curfile
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|storeSeqId
operator|>
name|this
operator|.
name|maxSeqId
condition|)
block|{
name|this
operator|.
name|maxSeqId
operator|=
name|storeSeqId
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// If the HSTORE_LOGINFOFILE doesn't contain a number, just ignore it.
comment|// That means it was built prior to the previous run of HStore, and so
comment|// it cannot contain any updates also contained in the log.
name|LOG
operator|.
name|info
argument_list|(
literal|"HSTORE_LOGINFOFILE "
operator|+
name|curfile
operator|+
literal|" does not contain a sequence number - ignoring"
argument_list|)
expr_stmt|;
block|}
name|Path
name|mapfile
init|=
name|curfile
operator|.
name|getMapFilePath
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|mapfile
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|curfile
operator|.
name|getInfoFilePath
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mapfile "
operator|+
name|mapfile
operator|.
name|toString
argument_list|()
operator|+
literal|" does not exist. "
operator|+
literal|"Cleaned up info file.  Continuing..."
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|// TODO: Confirm referent exists.
comment|// Found map and sympathetic info file.  Add this hstorefile to result.
name|results
operator|.
name|put
argument_list|(
name|storeSeqId
argument_list|,
name|curfile
argument_list|)
expr_stmt|;
comment|// Keep list of sympathetic data mapfiles for cleaning info dir in next
comment|// section.  Make sure path is fully qualified for compare.
name|mapfiles
operator|.
name|add
argument_list|(
name|mapfile
argument_list|)
expr_stmt|;
block|}
comment|// List paths by experience returns fully qualified names -- at least when
comment|// running on a mini hdfs cluster.
name|FileStatus
name|datfiles
index|[]
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|mapdir
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|datfiles
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|Path
name|p
init|=
name|datfiles
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// If does not have sympathetic info file, delete.
if|if
condition|(
operator|!
name|mapfiles
operator|.
name|contains
argument_list|(
name|fs
operator|.
name|makeQualified
argument_list|(
name|p
argument_list|)
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|results
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Bloom filters
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Called by constructor if a bloom filter is enabled for this column family.    * If the HStore already exists, it will read in the bloom filter saved    * previously. Otherwise, it will create a new bloom filter.    */
specifier|private
name|Filter
name|loadOrCreateBloomFilter
parameter_list|()
throws|throws
name|IOException
block|{
name|Path
name|filterFile
init|=
operator|new
name|Path
argument_list|(
name|filterDir
argument_list|,
name|BLOOMFILTER_FILE_NAME
argument_list|)
decl_stmt|;
name|Filter
name|bloomFilter
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|filterFile
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loading bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
name|BloomFilterDescriptor
operator|.
name|BloomFilterType
name|type
init|=
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getType
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|type
condition|)
block|{
case|case
name|BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|BloomFilter
argument_list|()
expr_stmt|;
break|break;
case|case
name|COUNTING_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|CountingBloomFilter
argument_list|()
expr_stmt|;
break|break;
case|case
name|RETOUCHED_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|RetouchedBloomFilter
argument_list|()
expr_stmt|;
break|break;
default|default:
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"unknown bloom filter type: "
operator|+
name|type
argument_list|)
throw|;
block|}
name|FSDataInputStream
name|in
init|=
name|fs
operator|.
name|open
argument_list|(
name|filterFile
argument_list|)
decl_stmt|;
try|try
block|{
name|bloomFilter
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|fs
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"creating bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
name|BloomFilterDescriptor
operator|.
name|BloomFilterType
name|type
init|=
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getType
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|type
condition|)
block|{
case|case
name|BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|BloomFilter
argument_list|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getVectorSize
argument_list|()
argument_list|,
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getNbHash
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|COUNTING_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|CountingBloomFilter
argument_list|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getVectorSize
argument_list|()
argument_list|,
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getNbHash
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|RETOUCHED_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|RetouchedBloomFilter
argument_list|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getVectorSize
argument_list|()
argument_list|,
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|getNbHash
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|bloomFilter
return|;
block|}
comment|/**    * Flushes bloom filter to disk    *     * @throws IOException    */
specifier|private
name|void
name|flushBloomFilter
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
operator|new
name|Path
argument_list|(
name|filterDir
argument_list|,
name|BLOOMFILTER_FILE_NAME
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|bloomFilter
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushed bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// End bloom filters
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Adds a value to the memcache    *     * @param key    * @param value    */
name|void
name|add
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|byte
index|[]
name|value
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|this
operator|.
name|memcache
operator|.
name|add
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Close all the MapFile readers    *     * We don't need to worry about subsequent requests because the HRegion holds    * a write lock that will prevent any more reads or writes.    *     * @throws IOException    */
name|List
argument_list|<
name|HStoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|ArrayList
argument_list|<
name|HStoreFile
argument_list|>
name|result
init|=
literal|null
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
for|for
control|(
name|MapFile
operator|.
name|Reader
name|reader
range|:
name|this
operator|.
name|readers
operator|.
name|values
argument_list|()
control|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|result
operator|=
operator|new
name|ArrayList
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"closed "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Flush changes to disk
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Prior to doing a cache flush, we need to snapshot the memcache. Locking is    * handled by the memcache.    */
name|void
name|snapshotMemcache
parameter_list|()
block|{
name|this
operator|.
name|memcache
operator|.
name|snapshot
argument_list|()
expr_stmt|;
block|}
comment|/**    * Write out a brand-new set of items to the disk.    *    * We should only store key/vals that are appropriate for the data-columns     * stored in this HStore.    *    * Also, we are not expecting any reads of this MapFile just yet.    *    * Return the entire list of HStoreFiles currently used by the HStore.    *    * @param logCacheFlushId flush sequence number    * @throws IOException    */
name|void
name|flushCache
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
name|internalFlushCache
argument_list|(
name|memcache
operator|.
name|getSnapshot
argument_list|()
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|internalFlushCache
parameter_list|(
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|cache
parameter_list|,
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Don't flush if there are no entries.
if|if
condition|(
name|cache
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return;
block|}
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
comment|// A. Write the Maps out to the disk
name|HStoreFile
name|flushedFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|,
operator|-
literal|1L
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|String
name|name
init|=
name|flushedFile
operator|.
name|toString
argument_list|()
decl_stmt|;
name|MapFile
operator|.
name|Writer
name|out
init|=
name|flushedFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
decl_stmt|;
comment|// Here we tried picking up an existing HStoreFile from disk and
comment|// interlacing the memcache flush compacting as we go.  The notion was
comment|// that interlacing would take as long as a pure flush with the added
comment|// benefit of having one less file in the store.  Experiments showed that
comment|// it takes two to three times the amount of time flushing -- more column
comment|// families makes it so the two timings come closer together -- but it
comment|// also complicates the flush. The code was removed.  Needed work picking
comment|// which file to interlace (favor references first, etc.)
comment|//
comment|// Related, looks like 'merging compactions' in BigTable paper interlaces
comment|// a memcache flush.  We don't.
name|int
name|entries
init|=
literal|0
decl_stmt|;
name|long
name|cacheSize
init|=
literal|0
decl_stmt|;
try|try
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|es
range|:
name|cache
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreKey
name|curkey
init|=
name|es
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|byte
index|[]
name|bytes
init|=
name|es
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|TextSequence
name|f
init|=
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|curkey
operator|.
name|getColumn
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|f
operator|.
name|equals
argument_list|(
name|this
operator|.
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|)
condition|)
block|{
name|entries
operator|++
expr_stmt|;
name|out
operator|.
name|append
argument_list|(
name|curkey
argument_list|,
operator|new
name|ImmutableBytesWritable
argument_list|(
name|bytes
argument_list|)
argument_list|)
expr_stmt|;
name|cacheSize
operator|+=
name|curkey
operator|.
name|getSize
argument_list|()
operator|+
operator|(
name|bytes
operator|!=
literal|null
condition|?
name|bytes
operator|.
name|length
else|:
literal|0
operator|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|long
name|newStoreSize
init|=
name|flushedFile
operator|.
name|length
argument_list|()
decl_stmt|;
name|storeSize
operator|+=
name|newStoreSize
expr_stmt|;
comment|// B. Write out the log sequence number that corresponds to this output
comment|// MapFile.  The MapFile is current up to and including the log seq num.
name|flushedFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
comment|// C. Flush the bloom filter if any
if|if
condition|(
name|bloomFilter
operator|!=
literal|null
condition|)
block|{
name|flushBloomFilter
argument_list|()
expr_stmt|;
block|}
comment|// D. Finally, make the new MapFile available.
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|Long
name|flushid
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|logCacheFlushId
argument_list|)
decl_stmt|;
comment|// Open the map file reader.
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|flushedFile
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|flushedFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Added "
operator|+
name|name
operator|+
literal|" with "
operator|+
name|entries
operator|+
literal|" entries, sequence id "
operator|+
name|logCacheFlushId
operator|+
literal|", data size "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|cacheSize
argument_list|)
operator|+
literal|", file size "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|newStoreSize
argument_list|)
operator|+
literal|" for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the back-HStores.  This method may take some time, so the calling     * thread must be able to block for long periods.    *     *<p>During this time, the HStore can work as usual, getting values from    * MapFiles and writing new MapFiles from the Memcache.    *     * Existing MapFiles are not destroyed until the new compacted TreeMap is     * completely written-out to disk.    *    * The compactLock prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *     * We don't want to hold the structureLock for the whole time, as a compact()     * can be lengthy and we want to allow cache-flushes during this period.    *     * @return mid key if a split is needed, null otherwise    * @throws IOException    */
name|Text
name|compact
parameter_list|()
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
name|long
name|maxId
init|=
operator|-
literal|1
decl_stmt|;
name|List
argument_list|<
name|HStoreFile
argument_list|>
name|filesToCompact
init|=
literal|null
decl_stmt|;
synchronized|synchronized
init|(
name|storefiles
init|)
block|{
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
literal|1
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Not compacting "
operator|+
name|this
operator|.
name|storeName
operator|+
literal|" because no store files to compact."
argument_list|)
expr_stmt|;
block|}
return|return
name|checkSplit
argument_list|()
return|;
block|}
elseif|else
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
if|if
condition|(
operator|!
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|isReference
argument_list|()
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Not compacting "
operator|+
name|this
operator|.
name|storeName
operator|+
literal|" because only one store file and it is not a reference"
argument_list|)
expr_stmt|;
block|}
return|return
name|checkSplit
argument_list|()
return|;
block|}
block|}
elseif|else
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|compactionThreshold
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Not compacting "
operator|+
name|this
operator|.
name|storeName
operator|+
literal|" because number of stores "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|"< compaction threshold "
operator|+
name|compactionThreshold
argument_list|)
expr_stmt|;
block|}
return|return
name|checkSplit
argument_list|()
return|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|compactionDir
argument_list|)
operator|&&
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|compactionDir
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mkdir on "
operator|+
name|compactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" failed"
argument_list|)
expr_stmt|;
return|return
name|checkSplit
argument_list|()
return|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"started compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" files using "
operator|+
name|compactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
comment|// Storefiles are keyed by sequence id. The oldest file comes first.
comment|// We need to return out of here a List that has the newest file first.
name|Collections
operator|.
name|reverse
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
comment|// The max-sequenceID in any of the to-be-compacted TreeMaps is the
comment|// last key of storefiles.
name|maxId
operator|=
name|this
operator|.
name|storefiles
operator|.
name|lastKey
argument_list|()
expr_stmt|;
block|}
comment|// Step through them, writing to the brand-new MapFile
name|HStoreFile
name|compactedOutputFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|this
operator|.
name|compactionDir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|,
operator|-
literal|1L
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|MapFile
operator|.
name|Writer
name|compactedOut
init|=
name|compactedOutputFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
decl_stmt|;
try|try
block|{
name|compactHStoreFiles
argument_list|(
name|compactedOut
argument_list|,
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|compactedOut
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|maxId
argument_list|)
expr_stmt|;
comment|// Move the compaction into place.
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|compactedOutputFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Completed compaction of "
operator|+
name|this
operator|.
name|storeName
operator|+
literal|" store size is "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|storeSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|checkSplit
argument_list|()
return|;
block|}
comment|/*    * Compact passed<code>toCompactFiles</code> into<code>compactedOut</code>.    * We create a new set of MapFile.Reader objects so we don't screw up the    * caching associated with the currently-loaded ones. Our iteration-based    * access pattern is practically designed to ruin the cache.    *     * We work by opening a single MapFile.Reader for each file, and iterating    * through them in parallel. We always increment the lowest-ranked one.    * Updates to a single row/column will appear ranked by timestamp. This allows    * us to throw out deleted values or obsolete versions. @param compactedOut    * @param toCompactFiles @throws IOException    */
specifier|private
name|void
name|compactHStoreFiles
parameter_list|(
specifier|final
name|MapFile
operator|.
name|Writer
name|compactedOut
parameter_list|,
specifier|final
name|List
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|size
init|=
name|toCompactFiles
operator|.
name|size
argument_list|()
decl_stmt|;
name|CompactionReader
index|[]
name|rdrs
init|=
operator|new
name|CompactionReader
index|[
name|size
index|]
decl_stmt|;
name|int
name|index
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|toCompactFiles
control|)
block|{
try|try
block|{
name|rdrs
index|[
name|index
operator|++
index|]
operator|=
operator|new
name|MapFileCompactionReader
argument_list|(
name|hsf
operator|.
name|getReader
argument_list|(
name|fs
argument_list|,
name|bloomFilter
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// Add info about which file threw exception. It may not be in the
comment|// exception message so output a message here where we know the
comment|// culprit.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed with "
operator|+
name|e
operator|.
name|toString
argument_list|()
operator|+
literal|": HStoreFile="
operator|+
name|hsf
operator|.
name|toString
argument_list|()
operator|+
operator|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|?
literal|", Reference="
operator|+
name|hsf
operator|.
name|getReference
argument_list|()
operator|.
name|toString
argument_list|()
else|:
literal|""
operator|)
operator|+
literal|" for Store="
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
name|closeCompactionReaders
argument_list|(
name|rdrs
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
block|}
try|try
block|{
name|HStoreKey
index|[]
name|keys
init|=
operator|new
name|HStoreKey
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
name|ImmutableBytesWritable
index|[]
name|vals
init|=
operator|new
name|ImmutableBytesWritable
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
name|boolean
index|[]
name|done
init|=
operator|new
name|boolean
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|keys
index|[
name|i
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
name|vals
index|[
name|i
index|]
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
literal|false
expr_stmt|;
block|}
comment|// Now, advance through the readers in order.  This will have the
comment|// effect of a run-time sort of the entire dataset.
name|int
name|numDone
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|rdrs
index|[
name|i
index|]
operator|.
name|reset
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
operator|!
name|rdrs
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|vals
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
name|numDone
operator|++
expr_stmt|;
block|}
block|}
name|int
name|timesSeen
init|=
literal|0
decl_stmt|;
name|Text
name|lastRow
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
name|Text
name|lastColumn
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
comment|// Map of a row deletes keyed by column with a list of timestamps for value
name|Map
argument_list|<
name|Text
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
name|deletes
init|=
literal|null
decl_stmt|;
while|while
condition|(
name|numDone
operator|<
name|done
operator|.
name|length
condition|)
block|{
comment|// Find the reader with the smallest key.  If two files have same key
comment|// but different values -- i.e. one is delete and other is non-delete
comment|// value -- we will find the first, the one that was written later and
comment|// therefore the one whose value should make it out to the compacted
comment|// store file.
name|int
name|smallestKey
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|smallestKey
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|keys
index|[
name|i
index|]
operator|.
name|compareTo
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|)
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
block|}
block|}
comment|// Reflect the current key/val in the output
name|HStoreKey
name|sk
init|=
name|keys
index|[
name|smallestKey
index|]
decl_stmt|;
if|if
condition|(
name|lastRow
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
operator|&&
name|lastColumn
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
condition|)
block|{
name|timesSeen
operator|++
expr_stmt|;
block|}
else|else
block|{
name|timesSeen
operator|=
literal|1
expr_stmt|;
comment|// We are on to a new row.  Create a new deletes list.
name|deletes
operator|=
operator|new
name|HashMap
argument_list|<
name|Text
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
argument_list|()
expr_stmt|;
block|}
name|byte
index|[]
name|value
init|=
operator|(
name|vals
index|[
name|smallestKey
index|]
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|vals
index|[
name|smallestKey
index|]
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|isDeleted
argument_list|(
name|sk
argument_list|,
name|value
argument_list|,
literal|false
argument_list|,
name|deletes
argument_list|)
operator|&&
name|timesSeen
operator|<=
name|family
operator|.
name|getMaxVersions
argument_list|()
condition|)
block|{
comment|// Keep old versions until we have maxVersions worth.
comment|// Then just skip them.
if|if
condition|(
name|sk
operator|.
name|getRow
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
operator|&&
name|sk
operator|.
name|getColumn
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
condition|)
block|{
comment|// Only write out objects which have a non-zero length key and
comment|// value
name|compactedOut
operator|.
name|append
argument_list|(
name|sk
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Update last-seen items
name|lastRow
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|lastColumn
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
comment|// Advance the smallest key.  If that reader's all finished, then
comment|// mark it as done.
if|if
condition|(
operator|!
name|rdrs
index|[
name|smallestKey
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
condition|)
block|{
name|done
index|[
name|smallestKey
index|]
operator|=
literal|true
expr_stmt|;
name|rdrs
index|[
name|smallestKey
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
name|rdrs
index|[
name|smallestKey
index|]
operator|=
literal|null
expr_stmt|;
name|numDone
operator|++
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|closeCompactionReaders
argument_list|(
name|rdrs
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|closeCompactionReaders
parameter_list|(
specifier|final
name|CompactionReader
index|[]
name|rdrs
parameter_list|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|rdrs
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|rdrs
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception closing reader for "
operator|+
name|this
operator|.
name|storeName
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|/*    * Check if this is cell is deleted.    * If a memcache and a deletes, check key does not have an entry filled.    * Otherwise, check value is not the<code>HGlobals.deleteBytes</code> value.    * If passed value IS deleteBytes, then it is added to the passed    * deletes map.    * @param hsk    * @param value    * @param checkMemcache true if the memcache should be consulted    * @param deletes Map keyed by column with a value of timestamp. Can be null.    * If non-null and passed value is HGlobals.deleteBytes, then we add to this    * map.    * @return True if this is a deleted cell.  Adds the passed deletes map if    * passed value is HGlobals.deleteBytes.   */
specifier|private
name|boolean
name|isDeleted
parameter_list|(
specifier|final
name|HStoreKey
name|hsk
parameter_list|,
specifier|final
name|byte
index|[]
name|value
parameter_list|,
specifier|final
name|boolean
name|checkMemcache
parameter_list|,
specifier|final
name|Map
argument_list|<
name|Text
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
name|deletes
parameter_list|)
block|{
if|if
condition|(
name|checkMemcache
operator|&&
name|memcache
operator|.
name|isDeleted
argument_list|(
name|hsk
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
name|List
argument_list|<
name|Long
argument_list|>
name|timestamps
init|=
operator|(
name|deletes
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|deletes
operator|.
name|get
argument_list|(
name|hsk
operator|.
name|getColumn
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|timestamps
operator|!=
literal|null
operator|&&
name|timestamps
operator|.
name|contains
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|hsk
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|value
operator|==
literal|null
condition|)
block|{
comment|// If a null value, shouldn't be in here.  Mark it as deleted cell.
return|return
literal|true
return|;
block|}
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|value
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Cell has delete value.  Save it into deletes.
if|if
condition|(
name|deletes
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|timestamps
operator|==
literal|null
condition|)
block|{
name|timestamps
operator|=
operator|new
name|ArrayList
argument_list|<
name|Long
argument_list|>
argument_list|()
expr_stmt|;
name|deletes
operator|.
name|put
argument_list|(
name|hsk
operator|.
name|getColumn
argument_list|()
argument_list|,
name|timestamps
argument_list|)
expr_stmt|;
block|}
comment|// We know its not already in the deletes array else we'd have returned
comment|// earlier so no need to test if timestamps already has this value.
name|timestamps
operator|.
name|add
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|hsk
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
literal|true
return|;
block|}
comment|/*    * It's assumed that the compactLock  will be acquired prior to calling this     * method!  Otherwise, it is not thread-safe!    *    * It works by processing a compaction that's been written to disk.    *     *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *     *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Wait for active scanners to exit    * 2) Acquiring the write-lock    * 3) Moving the new compacted MapFile into place    * 4) Unloading all the replaced MapFiles and close.    * 5) Deleting all the replaced MapFile files.    * 6) Loading the new TreeMap.    * 7) Compute new store size    * 8) Releasing the write-lock    * 9) Allow new scanners to proceed.    *</pre>    *     * @param compactedFiles list of files that were compacted    * @param compactedFile HStoreFile that is the result of the compaction    * @throws IOException    */
specifier|private
name|void
name|completeCompaction
parameter_list|(
name|List
argument_list|<
name|HStoreFile
argument_list|>
name|compactedFiles
parameter_list|,
name|HStoreFile
name|compactedFile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// 1. Wait for active scanners to exit
name|newScannerLock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
comment|// prevent new scanners
try|try
block|{
synchronized|synchronized
init|(
name|activeScanners
init|)
block|{
while|while
condition|(
name|activeScanners
operator|.
name|get
argument_list|()
operator|!=
literal|0
condition|)
block|{
try|try
block|{
name|activeScanners
operator|.
name|wait
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
comment|// continue
block|}
block|}
comment|// 2. Acquiring the HStore write-lock
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
block|}
try|try
block|{
comment|// 3. Moving the new MapFile into place.
name|HStoreFile
name|finalCompactedFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getFamilyName
argument_list|()
argument_list|,
operator|-
literal|1
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"moving "
operator|+
name|compactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|" in "
operator|+
name|this
operator|.
name|compactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" to "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|" in "
operator|+
name|basedir
operator|.
name|toString
argument_list|()
operator|+
literal|" for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|compactedFile
operator|.
name|rename
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|finalCompactedFile
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|" for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// 4. and 5. Unload all the replaced MapFiles, close and delete.
synchronized|synchronized
init|(
name|storefiles
init|)
block|{
name|List
argument_list|<
name|Long
argument_list|>
name|toDelete
init|=
operator|new
name|ArrayList
argument_list|<
name|Long
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|compactedFiles
operator|.
name|contains
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Long
name|key
init|=
name|e
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|MapFile
operator|.
name|Reader
name|reader
init|=
name|this
operator|.
name|readers
operator|.
name|remove
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|toDelete
operator|.
name|add
argument_list|(
name|key
argument_list|)
expr_stmt|;
block|}
try|try
block|{
for|for
control|(
name|Long
name|key
range|:
name|toDelete
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|this
operator|.
name|storefiles
operator|.
name|remove
argument_list|(
name|key
argument_list|)
decl_stmt|;
name|hsf
operator|.
name|delete
argument_list|()
expr_stmt|;
block|}
comment|// 6. Loading the new TreeMap.
name|Long
name|orderVal
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|finalCompactedFile
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
comment|// Use a block cache (if configured) for this reader since
comment|// it is the only one.
name|finalCompactedFile
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|,
name|family
operator|.
name|isBlockCacheEnabled
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|finalCompactedFile
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|=
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files for "
operator|+
name|this
operator|.
name|storeName
operator|+
literal|". Compacted file is "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|".  Files replaced are "
operator|+
name|compactedFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// 7. Compute new store size
name|storeSize
operator|=
literal|0L
expr_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
name|storeSize
operator|+=
name|hsf
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
comment|// 8. Releasing the write-lock
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
comment|// 9. Allow new scanners to proceed.
name|newScannerLock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return all the available columns for the given key.  The key indicates a     * row and timestamp, but not a column name.    *    * The returned object should map column names to Cells.    */
name|void
name|getFull
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
specifier|final
name|Set
argument_list|<
name|Text
argument_list|>
name|columns
parameter_list|,
name|Map
argument_list|<
name|Text
argument_list|,
name|Cell
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
name|Map
argument_list|<
name|Text
argument_list|,
name|Long
argument_list|>
name|deletes
init|=
operator|new
name|HashMap
argument_list|<
name|Text
argument_list|,
name|Long
argument_list|>
argument_list|()
decl_stmt|;
comment|// if the key is null, we're not even looking for anything. return.
if|if
condition|(
name|key
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
comment|// get from the memcache first.
name|memcache
operator|.
name|getFull
argument_list|(
name|key
argument_list|,
name|columns
argument_list|,
name|deletes
argument_list|,
name|results
argument_list|)
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
comment|// examine each mapfile
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
comment|// synchronize on the map so that no one else iterates it at the same
comment|// time
name|getFullFromMapFile
argument_list|(
name|map
argument_list|,
name|key
argument_list|,
name|columns
argument_list|,
name|deletes
argument_list|,
name|results
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|getFullFromMapFile
parameter_list|(
name|MapFile
operator|.
name|Reader
name|map
parameter_list|,
name|HStoreKey
name|key
parameter_list|,
name|Set
argument_list|<
name|Text
argument_list|>
name|columns
parameter_list|,
name|Map
argument_list|<
name|Text
argument_list|,
name|Long
argument_list|>
name|deletes
parameter_list|,
name|Map
argument_list|<
name|Text
argument_list|,
name|Cell
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|map
init|)
block|{
comment|// seek back to the beginning
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
comment|// seek to the closest key that should match the row we're looking for
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
return|return;
block|}
do|do
block|{
name|Text
name|readcol
init|=
name|readkey
operator|.
name|getColumn
argument_list|()
decl_stmt|;
comment|// if we're looking for this column (or all of them), and there isn't
comment|// already a value for this column in the results map, and the key we
comment|// just read matches, then we'll consider it
if|if
condition|(
operator|(
name|columns
operator|==
literal|null
operator|||
name|columns
operator|.
name|contains
argument_list|(
name|readcol
argument_list|)
operator|)
operator|&&
operator|!
name|results
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|&&
name|key
operator|.
name|matchesWithoutColumn
argument_list|(
name|readkey
argument_list|)
condition|)
block|{
comment|// if the value of the cell we're looking at right now is a delete,
comment|// we need to treat it differently
if|if
condition|(
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
comment|// if it's not already recorded as a delete or recorded with a more
comment|// recent delete timestamp, record it for later
if|if
condition|(
operator|!
name|deletes
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|||
name|deletes
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|.
name|longValue
argument_list|()
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|deletes
operator|.
name|put
argument_list|(
operator|new
name|Text
argument_list|(
name|readcol
argument_list|)
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
operator|!
operator|(
name|deletes
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|&&
name|deletes
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|.
name|longValue
argument_list|()
operator|>=
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|)
condition|)
block|{
comment|// So the cell itself isn't a delete, but there may be a delete
comment|// pending from earlier in our search. Only record this result if
comment|// there aren't any pending deletes.
if|if
condition|(
operator|!
operator|(
name|deletes
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|&&
name|deletes
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|.
name|longValue
argument_list|()
operator|>=
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|)
condition|)
block|{
name|results
operator|.
name|put
argument_list|(
operator|new
name|Text
argument_list|(
name|readcol
argument_list|)
argument_list|,
operator|new
name|Cell
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// need to reinstantiate the readval so we can reuse it,
comment|// otherwise next iteration will destroy our result
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|key
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|<
literal|0
condition|)
block|{
comment|// if we've crossed into the next row, then we can just stop
comment|// iterating
break|break;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
block|}
block|}
name|MapFile
operator|.
name|Reader
index|[]
name|getReaders
parameter_list|()
block|{
return|return
name|this
operator|.
name|readers
operator|.
name|values
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|this
operator|.
name|readers
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
comment|/**    * Get the value for the indicated HStoreKey.  Grab the target value and the     * previous 'numVersions-1' values, as well.    *    * If 'numVersions' is negative, the method returns all available versions.    * @param key    * @param numVersions Number of versions to fetch.  Must be> 0.    * @return values for the specified versions    * @throws IOException    */
name|Cell
index|[]
name|get
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|numVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Check the memcache
name|List
argument_list|<
name|Cell
argument_list|>
name|results
init|=
name|this
operator|.
name|memcache
operator|.
name|get
argument_list|(
name|key
argument_list|,
name|numVersions
argument_list|)
decl_stmt|;
comment|// If we got sufficient versions from memcache, return.
if|if
condition|(
name|results
operator|.
name|size
argument_list|()
operator|==
name|numVersions
condition|)
block|{
return|return
name|results
operator|.
name|toArray
argument_list|(
operator|new
name|Cell
index|[
name|results
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
comment|// Keep a list of deleted cell keys.  We need this because as we go through
comment|// the store files, the cell with the delete marker may be in one file and
comment|// the old non-delete cell value in a later store file. If we don't keep
comment|// around the fact that the cell was deleted in a newer record, we end up
comment|// returning the old value if user is asking for more than one version.
comment|// This List of deletes should not large since we are only keeping rows
comment|// and columns that match those set on the scanner and which have delete
comment|// values.  If memory usage becomes an issue, could redo as bloom filter.
name|Map
argument_list|<
name|Text
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
name|deletes
init|=
operator|new
name|HashMap
argument_list|<
name|Text
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
comment|// This code below is very close to the body of the getKeys method.
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// map.getClosest returns null if the passed key is> than the
comment|// last key in the map file.  getClosest is a bit of a misnomer
comment|// since it returns exact match or the next closest key AFTER not
comment|// BEFORE.
continue|continue;
block|}
if|if
condition|(
operator|!
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
operator|!
name|isDeleted
argument_list|(
name|readkey
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|,
literal|true
argument_list|,
name|deletes
argument_list|)
condition|)
block|{
name|results
operator|.
name|add
argument_list|(
operator|new
name|Cell
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// Perhaps only one version is wanted.  I could let this
comment|// test happen later in the for loop test but it would cost
comment|// the allocation of an ImmutableBytesWritable.
if|if
condition|(
name|hasEnoughVersions
argument_list|(
name|numVersions
argument_list|,
name|results
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
for|for
control|(
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
init|;
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
operator|&&
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
operator|&&
operator|!
name|hasEnoughVersions
argument_list|(
name|numVersions
argument_list|,
name|results
argument_list|)
condition|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|isDeleted
argument_list|(
name|readkey
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|,
literal|true
argument_list|,
name|deletes
argument_list|)
condition|)
block|{
name|results
operator|.
name|add
argument_list|(
operator|new
name|Cell
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|hasEnoughVersions
argument_list|(
name|numVersions
argument_list|,
name|results
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
return|return
name|results
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|?
literal|null
else|:
name|results
operator|.
name|toArray
argument_list|(
operator|new
name|Cell
index|[
name|results
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|boolean
name|hasEnoughVersions
parameter_list|(
specifier|final
name|int
name|numVersions
parameter_list|,
specifier|final
name|List
argument_list|<
name|Cell
argument_list|>
name|results
parameter_list|)
block|{
return|return
name|numVersions
operator|>
literal|0
operator|&&
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
return|;
block|}
comment|/**    * Get<code>versions</code> keys matching the origin key's    * row/column/timestamp and those of an older vintage    * Default access so can be accessed out of {@link HRegionServer}.    * @param origin Where to start searching.    * @param versions How many versions to return. Pass    * {@link HConstants.ALL_VERSIONS} to retrieve all. Versions will include    * size of passed<code>allKeys</code> in its count.    * @param allKeys List of keys prepopulated by keys we found in memcache.    * This method returns this passed list with all matching keys found in    * stores appended.    * @return The passed<code>allKeys</code> with<code>versions</code> of    * matching keys found in store files appended.    * @throws IOException    */
name|List
argument_list|<
name|HStoreKey
argument_list|>
name|getKeys
parameter_list|(
specifier|final
name|HStoreKey
name|origin
parameter_list|,
specifier|final
name|int
name|versions
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|HStoreKey
argument_list|>
name|keys
init|=
name|this
operator|.
name|memcache
operator|.
name|getKeys
argument_list|(
name|origin
argument_list|,
name|versions
argument_list|)
decl_stmt|;
if|if
condition|(
name|versions
operator|!=
name|ALL_VERSIONS
operator|&&
name|keys
operator|.
name|size
argument_list|()
operator|>=
name|versions
condition|)
block|{
return|return
name|keys
return|;
block|}
comment|// This code below is very close to the body of the get method.
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
comment|// do the priming read
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|origin
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// map.getClosest returns null if the passed key is> than the
comment|// last key in the map file.  getClosest is a bit of a misnomer
comment|// since it returns exact match or the next closest key AFTER not
comment|// BEFORE.
continue|continue;
block|}
do|do
block|{
comment|// if the row matches, we might want this one.
if|if
condition|(
name|rowMatches
argument_list|(
name|origin
argument_list|,
name|readkey
argument_list|)
condition|)
block|{
comment|// if the cell matches, then we definitely want this key.
if|if
condition|(
name|cellMatches
argument_list|(
name|origin
argument_list|,
name|readkey
argument_list|)
condition|)
block|{
comment|// store the key if it isn't deleted or superceeded by what's
comment|// in the memcache
if|if
condition|(
operator|!
name|isDeleted
argument_list|(
name|readkey
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|,
literal|false
argument_list|,
literal|null
argument_list|)
operator|&&
operator|!
name|keys
operator|.
name|contains
argument_list|(
name|readkey
argument_list|)
condition|)
block|{
name|keys
operator|.
name|add
argument_list|(
operator|new
name|HStoreKey
argument_list|(
name|readkey
argument_list|)
argument_list|)
expr_stmt|;
comment|// if we've collected enough versions, then exit the loop.
if|if
condition|(
name|versions
operator|!=
name|ALL_VERSIONS
operator|&&
name|keys
operator|.
name|size
argument_list|()
operator|>=
name|versions
condition|)
block|{
break|break;
block|}
block|}
block|}
else|else
block|{
comment|// the cell doesn't match, but there might be more with different
comment|// timestamps, so move to the next key
continue|continue;
block|}
block|}
else|else
block|{
comment|// the row doesn't match, so we've gone too far.
break|break;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
comment|// advance to the next key
block|}
block|}
return|return
name|keys
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Find the key that matches<i>row</i> exactly, or the one that immediately    * preceeds it. WARNING: Only use this method on a table where writes occur     * with stricly increasing timestamps. This method assumes this pattern of     * writes in order to make it reasonably performant.     */
name|Text
name|getRowKeyAtOrBefore
parameter_list|(
specifier|final
name|Text
name|row
parameter_list|)
throws|throws
name|IOException
block|{
comment|// map of HStoreKeys that are candidates for holding the row key that
comment|// most closely matches what we're looking for. we'll have to update it
comment|// deletes found all over the place as we go along before finally reading
comment|// the best key out of it at the end.
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
name|candidateKeys
init|=
operator|new
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
argument_list|()
decl_stmt|;
comment|// obtain read lock
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
comment|// process each store file
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
comment|// update the candidate keys from the current map file
name|rowAtOrBeforeFromMapFile
argument_list|(
name|maparray
index|[
name|i
index|]
argument_list|,
name|row
argument_list|,
name|candidateKeys
argument_list|)
expr_stmt|;
block|}
comment|// finally, check the memcache
name|memcache
operator|.
name|getRowKeyAtOrBefore
argument_list|(
name|row
argument_list|,
name|candidateKeys
argument_list|)
expr_stmt|;
comment|// return the best key from candidateKeys
if|if
condition|(
operator|!
name|candidateKeys
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
name|candidateKeys
operator|.
name|lastKey
argument_list|()
operator|.
name|getRow
argument_list|()
return|;
block|}
return|return
literal|null
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Check an individual MapFile for the row at or before a given key     * and timestamp    */
specifier|private
name|void
name|rowAtOrBeforeFromMapFile
parameter_list|(
name|MapFile
operator|.
name|Reader
name|map
parameter_list|,
name|Text
name|row
parameter_list|,
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
name|candidateKeys
parameter_list|)
throws|throws
name|IOException
block|{
name|HStoreKey
name|searchKey
init|=
literal|null
decl_stmt|;
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|HStoreKey
name|strippedKey
init|=
literal|null
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
comment|// don't bother with the rest of this if the file is empty
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
block|{
return|return;
block|}
comment|// if there aren't any candidate keys yet, we'll do some things slightly
comment|// different
if|if
condition|(
name|candidateKeys
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|searchKey
operator|=
operator|new
name|HStoreKey
argument_list|(
name|row
argument_list|)
expr_stmt|;
comment|// if the row we're looking for is past the end of this mapfile, just
comment|// save time and add the last key to the candidates.
name|HStoreKey
name|finalKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|map
operator|.
name|finalKey
argument_list|(
name|finalKey
argument_list|)
expr_stmt|;
if|if
condition|(
name|finalKey
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|row
argument_list|)
operator|<
literal|0
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|stripTimestamp
argument_list|(
name|finalKey
argument_list|)
argument_list|,
operator|new
name|Long
argument_list|(
name|finalKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// seek to the exact row, or the one that would be immediately before it
name|readkey
operator|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|searchKey
argument_list|,
name|readval
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// didn't find anything that would match, so return
return|return;
block|}
do|do
block|{
comment|// if we have an exact match on row, and it's not a delete, save this
comment|// as a candidate key
if|if
condition|(
name|readkey
operator|.
name|getRow
argument_list|()
operator|.
name|equals
argument_list|(
name|row
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
argument_list|,
operator|new
name|Long
argument_list|(
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|readkey
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|row
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the row key we just read is beyond the key we're searching for,
comment|// then we're done. return.
return|return;
block|}
else|else
block|{
comment|// so, the row key doesn't match, but we haven't gone past the row
comment|// we're seeking yet, so this row is a candidate for closest
comment|// (assuming that it isn't a delete).
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
argument_list|,
operator|new
name|Long
argument_list|(
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
comment|// arriving here just means that we consumed the whole rest of the map
comment|// without going "past" the key we're searching for. we can just fall
comment|// through here.
block|}
else|else
block|{
comment|// if there are already candidate keys, we need to start our search
comment|// at the earliest possible key so that we can discover any possible
comment|// deletes for keys between the start and the search key.
name|searchKey
operator|=
operator|new
name|HStoreKey
argument_list|(
name|candidateKeys
operator|.
name|firstKey
argument_list|()
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
comment|// if the row we're looking for is past the end of this mapfile, just
comment|// save time and add the last key to the candidates.
name|HStoreKey
name|finalKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|map
operator|.
name|finalKey
argument_list|(
name|finalKey
argument_list|)
expr_stmt|;
if|if
condition|(
name|finalKey
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|searchKey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|<
literal|0
condition|)
block|{
name|strippedKey
operator|=
name|stripTimestamp
argument_list|(
name|finalKey
argument_list|)
expr_stmt|;
comment|// if the candidate keys has a cell like this one already,
comment|// then we might want to update the timestamp we're using on it
if|if
condition|(
name|candidateKeys
operator|.
name|containsKey
argument_list|(
name|strippedKey
argument_list|)
condition|)
block|{
name|long
name|bestCandidateTs
init|=
name|candidateKeys
operator|.
name|get
argument_list|(
name|strippedKey
argument_list|)
operator|.
name|longValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|bestCandidateTs
operator|<
name|finalKey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
operator|new
name|Long
argument_list|(
name|finalKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// otherwise, this is a new key, so put it up as a candidate
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
operator|new
name|Long
argument_list|(
name|finalKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return;
block|}
comment|// seek to the exact row, or the one that would be immediately before it
name|readkey
operator|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|searchKey
argument_list|,
name|readval
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// didn't find anything that would match, so return
return|return;
block|}
do|do
block|{
comment|// if we have an exact match on row, and it's not a delete, save this
comment|// as a candidate key
if|if
condition|(
name|readkey
operator|.
name|getRow
argument_list|()
operator|.
name|equals
argument_list|(
name|row
argument_list|)
condition|)
block|{
name|strippedKey
operator|=
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
operator|new
name|Long
argument_list|(
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// if the candidate keys contain any that might match by timestamp,
comment|// then check for a match and remove it if it's too young to
comment|// survive the delete
if|if
condition|(
name|candidateKeys
operator|.
name|containsKey
argument_list|(
name|strippedKey
argument_list|)
condition|)
block|{
name|long
name|bestCandidateTs
init|=
name|candidateKeys
operator|.
name|get
argument_list|(
name|strippedKey
argument_list|)
operator|.
name|longValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|bestCandidateTs
operator|<=
name|readkey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|candidateKeys
operator|.
name|remove
argument_list|(
name|strippedKey
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|readkey
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|row
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the row key we just read is beyond the key we're searching for,
comment|// then we're done. return.
return|return;
block|}
else|else
block|{
name|strippedKey
operator|=
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
expr_stmt|;
comment|// so, the row key doesn't match, but we haven't gone past the row
comment|// we're seeking yet, so this row is a candidate for closest
comment|// (assuming that it isn't a delete).
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// if the candidate keys contain any that might match by timestamp,
comment|// then check for a match and remove it if it's too young to
comment|// survive the delete
if|if
condition|(
name|candidateKeys
operator|.
name|containsKey
argument_list|(
name|strippedKey
argument_list|)
condition|)
block|{
name|long
name|bestCandidateTs
init|=
name|candidateKeys
operator|.
name|get
argument_list|(
name|strippedKey
argument_list|)
operator|.
name|longValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|bestCandidateTs
operator|<=
name|readkey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|candidateKeys
operator|.
name|remove
argument_list|(
name|strippedKey
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
block|}
block|}
specifier|static
name|HStoreKey
name|stripTimestamp
parameter_list|(
name|HStoreKey
name|key
parameter_list|)
block|{
return|return
operator|new
name|HStoreKey
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|key
operator|.
name|getColumn
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Test that the<i>target</i> matches the<i>origin</i>. If the     *<i>origin</i> has an empty column, then it's assumed to mean any column     * matches and only match on row and timestamp. Otherwise, it compares the    * keys with HStoreKey.matchesRowCol().    * @param origin The key we're testing against    * @param target The key we're testing    */
specifier|private
name|boolean
name|cellMatches
parameter_list|(
name|HStoreKey
name|origin
parameter_list|,
name|HStoreKey
name|target
parameter_list|)
block|{
comment|// if the origin's column is empty, then we're matching any column
if|if
condition|(
name|origin
operator|.
name|getColumn
argument_list|()
operator|.
name|equals
argument_list|(
operator|new
name|Text
argument_list|()
argument_list|)
condition|)
block|{
comment|// if the row matches, then...
if|if
condition|(
name|target
operator|.
name|getRow
argument_list|()
operator|.
name|equals
argument_list|(
name|origin
operator|.
name|getRow
argument_list|()
argument_list|)
condition|)
block|{
comment|// check the timestamp
return|return
name|target
operator|.
name|getTimestamp
argument_list|()
operator|<=
name|origin
operator|.
name|getTimestamp
argument_list|()
return|;
block|}
return|return
literal|false
return|;
block|}
comment|// otherwise, we want to match on row and column
return|return
name|target
operator|.
name|matchesRowCol
argument_list|(
name|origin
argument_list|)
return|;
block|}
comment|/**    * Test that the<i>target</i> matches the<i>origin</i>. If the<i>origin</i>    * has an empty column, then it just tests row equivalence. Otherwise, it uses    * HStoreKey.matchesRowCol().    * @param origin Key we're testing against    * @param target Key we're testing    */
specifier|private
name|boolean
name|rowMatches
parameter_list|(
name|HStoreKey
name|origin
parameter_list|,
name|HStoreKey
name|target
parameter_list|)
block|{
comment|// if the origin's column is empty, then we're matching any column
if|if
condition|(
name|origin
operator|.
name|getColumn
argument_list|()
operator|.
name|equals
argument_list|(
operator|new
name|Text
argument_list|()
argument_list|)
condition|)
block|{
comment|// if the row matches, then...
return|return
name|target
operator|.
name|getRow
argument_list|()
operator|.
name|equals
argument_list|(
name|origin
operator|.
name|getRow
argument_list|()
argument_list|)
return|;
block|}
comment|// otherwise, we want to match on row and column
return|return
name|target
operator|.
name|matchesRowCol
argument_list|(
name|origin
argument_list|)
return|;
block|}
comment|/**    * Determines if HStore can be split    *     * @return midKey if store can be split, null otherwise    */
name|Text
name|checkSplit
parameter_list|()
block|{
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
name|storeSize
operator|<
name|this
operator|.
name|desiredMaxFileSize
condition|)
block|{
return|return
literal|null
return|;
block|}
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Not splitable if we find a reference store file present in the store.
name|boolean
name|splitable
init|=
literal|true
decl_stmt|;
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|Long
name|mapIndex
init|=
name|Long
operator|.
name|valueOf
argument_list|(
literal|0L
argument_list|)
decl_stmt|;
comment|// Iterate through all the MapFiles
synchronized|synchronized
init|(
name|storefiles
init|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreFile
name|curHSF
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|long
name|size
init|=
name|curHSF
operator|.
name|length
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|mapIndex
operator|=
name|e
operator|.
name|getKey
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|splitable
condition|)
block|{
name|splitable
operator|=
operator|!
name|curHSF
operator|.
name|isReference
argument_list|()
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
operator|!
name|splitable
condition|)
block|{
return|return
literal|null
return|;
block|}
name|MapFile
operator|.
name|Reader
name|r
init|=
name|this
operator|.
name|readers
operator|.
name|get
argument_list|(
name|mapIndex
argument_list|)
decl_stmt|;
comment|// seek back to the beginning of mapfile
name|r
operator|.
name|reset
argument_list|()
expr_stmt|;
comment|// get the first and last keys
name|HStoreKey
name|firstKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|HStoreKey
name|lastKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|Writable
name|value
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|r
operator|.
name|next
argument_list|(
name|firstKey
argument_list|,
name|value
argument_list|)
expr_stmt|;
name|r
operator|.
name|finalKey
argument_list|(
name|lastKey
argument_list|)
expr_stmt|;
comment|// get the midkey
name|HStoreKey
name|mk
init|=
operator|(
name|HStoreKey
operator|)
name|r
operator|.
name|midKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|mk
operator|!=
literal|null
condition|)
block|{
comment|// if the midkey is the same as the first and last keys, then we cannot
comment|// (ever) split this region.
if|if
condition|(
name|mk
operator|.
name|getRow
argument_list|()
operator|.
name|equals
argument_list|(
name|firstKey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|&&
name|mk
operator|.
name|getRow
argument_list|()
operator|.
name|equals
argument_list|(
name|lastKey
operator|.
name|getRow
argument_list|()
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|mk
operator|.
name|getRow
argument_list|()
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed getting store size for "
operator|+
name|this
operator|.
name|storeName
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** @return aggregate size of HStore */
specifier|public
name|long
name|getSize
parameter_list|()
block|{
return|return
name|storeSize
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return a scanner for both the memcache and the HStore files    */
name|HInternalScannerInterface
name|getScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|Text
name|targetCols
index|[]
parameter_list|,
name|Text
name|firstRow
parameter_list|,
name|RowFilterInterface
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|newScannerLock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
comment|// ability to create a new
comment|// scanner during a compaction
try|try
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
comment|// lock HStore
try|try
block|{
return|return
operator|new
name|HStoreScanner
argument_list|(
name|this
argument_list|,
name|targetCols
argument_list|,
name|firstRow
argument_list|,
name|timestamp
argument_list|,
name|filter
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|newScannerLock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/** {@inheritDoc} */
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|this
operator|.
name|storeName
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/*    * @see writeSplitInfo(Path p, HStoreFile hsf, FileSystem fs)    */
specifier|static
name|HStoreFile
operator|.
name|Reference
name|readSplitInfo
parameter_list|(
specifier|final
name|Path
name|p
parameter_list|,
specifier|final
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
name|FSDataInputStream
name|in
init|=
name|fs
operator|.
name|open
argument_list|(
name|p
argument_list|)
decl_stmt|;
try|try
block|{
name|HStoreFile
operator|.
name|Reference
name|r
init|=
operator|new
name|HStoreFile
operator|.
name|Reference
argument_list|()
decl_stmt|;
name|r
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
return|return
name|r
return|;
block|}
finally|finally
block|{
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @param p Path to check.    * @return True if the path has format of a HStoreFile reference.    */
specifier|public
specifier|static
name|boolean
name|isReference
parameter_list|(
specifier|final
name|Path
name|p
parameter_list|)
block|{
return|return
name|isReference
argument_list|(
name|p
argument_list|,
name|REF_NAME_PARSER
operator|.
name|matcher
argument_list|(
name|p
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isReference
parameter_list|(
specifier|final
name|Path
name|p
parameter_list|,
specifier|final
name|Matcher
name|m
parameter_list|)
block|{
if|if
condition|(
name|m
operator|==
literal|null
operator|||
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed match of store file name "
operator|+
name|p
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Failed match of store file name "
operator|+
name|p
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
return|return
name|m
operator|.
name|groupCount
argument_list|()
operator|>
literal|1
operator|&&
name|m
operator|.
name|group
argument_list|(
literal|2
argument_list|)
operator|!=
literal|null
return|;
block|}
specifier|protected
name|void
name|updateActiveScanners
parameter_list|()
block|{
synchronized|synchronized
init|(
name|activeScanners
init|)
block|{
name|int
name|numberOfScanners
init|=
name|activeScanners
operator|.
name|decrementAndGet
argument_list|()
decl_stmt|;
if|if
condition|(
name|numberOfScanners
operator|<
literal|0
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|storeName
operator|+
literal|" number of active scanners less than zero: "
operator|+
name|numberOfScanners
operator|+
literal|" resetting to zero"
argument_list|)
expr_stmt|;
name|activeScanners
operator|.
name|set
argument_list|(
literal|0
argument_list|)
expr_stmt|;
name|numberOfScanners
operator|=
literal|0
expr_stmt|;
block|}
if|if
condition|(
name|numberOfScanners
operator|==
literal|0
condition|)
block|{
name|activeScanners
operator|.
name|notifyAll
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
end_class

end_unit

