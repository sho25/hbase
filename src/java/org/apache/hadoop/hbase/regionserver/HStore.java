begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2007 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|EOFException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|locks
operator|.
name|ReentrantReadWriteLock
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Matcher
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Pattern
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HBaseConfiguration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HRegionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HStoreKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|RemoteExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|filter
operator|.
name|RowFilterInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|Cell
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|ImmutableBytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|FSUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|MapFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Progressable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_comment
comment|/**  * HStore maintains a bunch of data files.  It is responsible for maintaining   * the memory/file hierarchy and for periodic flushes to disk and compacting   * edits to the file.  *  * Locking and transactions are handled at a higher level.  This API should not   * be called directly by any writer, but rather by an HRegion manager.  */
end_comment

begin_class
specifier|public
class|class
name|HStore
implements|implements
name|HConstants
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HStore
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/*    * Regex that will work for straight filenames and for reference names.    * If reference, then the regex has more than just one group.  Group 1 is    * this files id.  Group 2 the referenced region name, etc.    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|REF_NAME_PARSER
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^(\\d+)(?:\\.(.+))?$"
argument_list|)
decl_stmt|;
specifier|protected
specifier|final
name|Memcache
name|memcache
decl_stmt|;
specifier|private
specifier|final
name|Path
name|basedir
decl_stmt|;
specifier|private
specifier|final
name|HRegionInfo
name|info
decl_stmt|;
specifier|private
specifier|final
name|HColumnDescriptor
name|family
decl_stmt|;
specifier|private
specifier|final
name|SequenceFile
operator|.
name|CompressionType
name|compression
decl_stmt|;
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|HBaseConfiguration
name|conf
decl_stmt|;
specifier|protected
name|long
name|ttl
decl_stmt|;
specifier|private
specifier|final
name|long
name|desiredMaxFileSize
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|storeSize
decl_stmt|;
specifier|private
specifier|final
name|Integer
name|flushLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|final
name|ReentrantReadWriteLock
name|lock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
specifier|final
name|byte
index|[]
name|storeName
decl_stmt|;
specifier|private
specifier|final
name|String
name|storeNameStr
decl_stmt|;
comment|/*    * Sorted Map of readers keyed by sequence id (Most recent should be last in    * in list).    */
specifier|private
specifier|final
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|storefiles
init|=
name|Collections
operator|.
name|synchronizedSortedMap
argument_list|(
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
comment|/*    * Sorted Map of readers keyed by sequence id (Most recent should be last in    * in list).    */
specifier|private
specifier|final
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
name|readers
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
argument_list|()
decl_stmt|;
comment|// The most-recent log-seq-ID that's present.  The most-recent such ID means
comment|// we can ignore all log messages up to and including that ID (because they're
comment|// already reflected in the TreeMaps).
specifier|private
specifier|volatile
name|long
name|maxSeqId
decl_stmt|;
specifier|private
specifier|final
name|Path
name|compactionDir
decl_stmt|;
specifier|private
specifier|final
name|Integer
name|compactLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|int
name|compactionThreshold
decl_stmt|;
specifier|private
specifier|final
name|Set
argument_list|<
name|ChangedReadersObserver
argument_list|>
name|changedReaderObservers
init|=
name|Collections
operator|.
name|synchronizedSet
argument_list|(
operator|new
name|HashSet
argument_list|<
name|ChangedReadersObserver
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
comment|/**    * An HStore is a set of zero or more MapFiles, which stretch backwards over     * time.  A given HStore is responsible for a certain set of columns for a    * row in the HRegion.    *    *<p>The HRegion starts writing to its set of HStores when the HRegion's     * memcache is flushed.  This results in a round of new MapFiles, one for    * each HStore.    *    *<p>There's no reason to consider append-logging at this level; all logging     * and locking is handled at the HRegion level.  HStore just provides    * services to manage sets of MapFiles.  One of the most important of those    * services is MapFile-compaction services.    *    *<p>The only thing having to do with logs that HStore needs to deal with is    * the reconstructionLog.  This is a segment of an HRegion's log that might    * NOT be present upon startup.  If the param is NULL, there's nothing to do.    * If the param is non-NULL, we need to process the log to reconstruct    * a TreeMap that might not have been written to disk before the process    * died.    *    *<p>It's assumed that after this constructor returns, the reconstructionLog    * file will be deleted (by whoever has instantiated the HStore).    *    * @param basedir qualified path under which the region directory lives    * @param info HRegionInfo for this region    * @param family HColumnDescriptor for this column    * @param fs file system object    * @param reconstructionLog existing log file to apply if any    * @param conf configuration object    * @param reporter Call on a period so hosting server can report we're    * making progress to master -- otherwise master might think region deploy    * failed.  Can be null.    * @throws IOException    */
specifier|protected
name|HStore
parameter_list|(
name|Path
name|basedir
parameter_list|,
name|HRegionInfo
name|info
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|reconstructionLog
parameter_list|,
name|HBaseConfiguration
name|conf
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|basedir
operator|=
name|basedir
expr_stmt|;
name|this
operator|.
name|info
operator|=
name|info
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|ttl
operator|=
name|family
operator|.
name|getTimeToLive
argument_list|()
expr_stmt|;
if|if
condition|(
name|ttl
operator|!=
name|HConstants
operator|.
name|FOREVER
condition|)
name|this
operator|.
name|ttl
operator|*=
literal|1000
expr_stmt|;
name|this
operator|.
name|memcache
operator|=
operator|new
name|Memcache
argument_list|(
name|this
operator|.
name|ttl
argument_list|)
expr_stmt|;
name|this
operator|.
name|compactionDir
operator|=
name|HRegion
operator|.
name|getCompactionDir
argument_list|(
name|basedir
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeName
operator|=
name|Bytes
operator|.
name|toBytes
argument_list|(
name|this
operator|.
name|info
operator|.
name|getEncodedName
argument_list|()
operator|+
literal|"/"
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeNameStr
operator|=
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
comment|// By default, we compact if an HStore has more than
comment|// MIN_COMMITS_FOR_COMPACTION map files
name|this
operator|.
name|compactionThreshold
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
expr_stmt|;
comment|// By default we split region if a file> DEFAULT_MAX_FILE_SIZE.
name|long
name|maxFileSize
init|=
name|info
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getMaxFileSize
argument_list|()
decl_stmt|;
if|if
condition|(
name|maxFileSize
operator|==
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
condition|)
block|{
name|maxFileSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hregion.max.filesize"
argument_list|,
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|desiredMaxFileSize
operator|=
name|maxFileSize
expr_stmt|;
name|this
operator|.
name|storeSize
operator|=
literal|0L
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|BLOCK
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|BLOCK
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|RECORD
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|RECORD
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|NONE
expr_stmt|;
block|}
name|Path
name|mapdir
init|=
name|HStoreFile
operator|.
name|getMapDir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|mapdir
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|mapdir
argument_list|)
expr_stmt|;
block|}
name|Path
name|infodir
init|=
name|HStoreFile
operator|.
name|getInfoDir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|infodir
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|infodir
argument_list|)
expr_stmt|;
block|}
comment|// Go through the 'mapdir' and 'infodir' together, make sure that all
comment|// MapFiles are in a reliable state.  Every entry in 'mapdir' must have a
comment|// corresponding one in 'loginfodir'. Without a corresponding log info
comment|// file, the entry in 'mapdir' must be deleted.
comment|// loadHStoreFiles also computes the max sequence id internally.
name|this
operator|.
name|maxSeqId
operator|=
operator|-
literal|1L
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|putAll
argument_list|(
name|loadHStoreFiles
argument_list|(
name|infodir
argument_list|,
name|mapdir
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
operator|&&
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Loaded "
operator|+
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in hstore "
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
operator|+
literal|", max sequence id "
operator|+
name|this
operator|.
name|maxSeqId
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|doReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|maxSeqId
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|EOFException
name|e
parameter_list|)
block|{
comment|// Presume we got here because of lack of HADOOP-1700; for now keep going
comment|// but this is probably not what we want long term.  If we got here there
comment|// has been data-loss
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception processing reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" opening "
operator|+
name|this
operator|.
name|storeName
operator|+
literal|" -- continuing.  Probably lack-of-HADOOP-1700 causing DATA LOSS!"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// Presume we got here because of some HDFS issue. Don't just keep going.
comment|// Fail to open the HStore.  Probably means we'll fail over and over
comment|// again until human intervention but alternative has us skipping logs
comment|// and losing edits: HBASE-642.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception processing reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" opening "
operator|+
name|this
operator|.
name|storeName
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
comment|// Finally, start up all the map readers! (There could be more than one
comment|// since we haven't compacted yet.)
name|boolean
name|first
init|=
literal|true
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|MapFile
operator|.
name|Reader
name|r
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|first
condition|)
block|{
comment|// Use a block cache (if configured) for the first reader only
comment|// so as to control memory usage.
name|r
operator|=
name|e
operator|.
name|getValue
argument_list|()
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBloomfilter
argument_list|()
argument_list|,
name|family
operator|.
name|isBlockCacheEnabled
argument_list|()
argument_list|)
expr_stmt|;
name|first
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|r
operator|=
name|e
operator|.
name|getValue
argument_list|()
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBloomfilter
argument_list|()
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|,
name|r
argument_list|)
expr_stmt|;
block|}
block|}
name|HColumnDescriptor
name|getFamily
parameter_list|()
block|{
return|return
name|this
operator|.
name|family
return|;
block|}
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|this
operator|.
name|maxSeqId
return|;
block|}
comment|/*    * Read the reconstructionLog to see whether we need to build a brand-new     * MapFile out of non-flushed log entries.      *    * We can ignore any log message that has a sequence ID that's equal to or     * lower than maxSeqID.  (Because we know such log messages are already     * reflected in the MapFiles.)    */
specifier|private
name|void
name|doReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|maxSeqID
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|UnsupportedEncodingException
throws|,
name|IOException
block|{
if|if
condition|(
name|reconstructionLog
operator|==
literal|null
operator|||
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
condition|)
block|{
comment|// Nothing to do.
return|return;
block|}
comment|// Check its not empty.
name|FileStatus
index|[]
name|stats
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|reconstructionLog
argument_list|)
decl_stmt|;
if|if
condition|(
name|stats
operator|==
literal|null
operator|||
name|stats
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Passed reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" is zero-length"
argument_list|)
expr_stmt|;
return|return;
block|}
name|long
name|maxSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|reconstructedCache
init|=
operator|new
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|()
decl_stmt|;
name|SequenceFile
operator|.
name|Reader
name|logReader
init|=
operator|new
name|SequenceFile
operator|.
name|Reader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|reconstructionLog
argument_list|,
name|this
operator|.
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|HLogKey
name|key
init|=
operator|new
name|HLogKey
argument_list|()
decl_stmt|;
name|HLogEdit
name|val
init|=
operator|new
name|HLogEdit
argument_list|()
decl_stmt|;
name|long
name|skippedEdits
init|=
literal|0
decl_stmt|;
name|long
name|editsCount
init|=
literal|0
decl_stmt|;
comment|// How many edits to apply before we send a progress report.
name|int
name|reportInterval
init|=
name|this
operator|.
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.report.interval.edits"
argument_list|,
literal|2000
argument_list|)
decl_stmt|;
while|while
condition|(
name|logReader
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
condition|)
block|{
name|maxSeqIdInLog
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSeqIdInLog
argument_list|,
name|key
operator|.
name|getLogSeqNum
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|<=
name|maxSeqID
condition|)
block|{
name|skippedEdits
operator|++
expr_stmt|;
continue|continue;
block|}
comment|// Check this edit is for me. Also, guard against writing
comment|// METACOLUMN info such as HBASE::CACHEFLUSH entries
name|byte
index|[]
name|column
init|=
name|val
operator|.
name|getColumn
argument_list|()
decl_stmt|;
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|column
argument_list|,
name|HLog
operator|.
name|METACOLUMN
argument_list|)
operator|||
operator|!
name|Bytes
operator|.
name|equals
argument_list|(
name|key
operator|.
name|getRegionName
argument_list|()
argument_list|,
name|info
operator|.
name|getRegionName
argument_list|()
argument_list|)
operator|||
operator|!
name|HStoreKey
operator|.
name|matchingFamily
argument_list|(
name|family
operator|.
name|getName
argument_list|()
argument_list|,
name|column
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|HStoreKey
name|k
init|=
operator|new
name|HStoreKey
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|column
argument_list|,
name|val
operator|.
name|getTimestamp
argument_list|()
argument_list|)
decl_stmt|;
name|reconstructedCache
operator|.
name|put
argument_list|(
name|k
argument_list|,
name|val
operator|.
name|getVal
argument_list|()
argument_list|)
expr_stmt|;
name|editsCount
operator|++
expr_stmt|;
comment|// Every 2k edits, tell the reporter we're making progress.
comment|// Have seen 60k edits taking 3minutes to complete.
if|if
condition|(
name|reporter
operator|!=
literal|null
operator|&&
operator|(
name|editsCount
operator|%
name|reportInterval
operator|)
operator|==
literal|0
condition|)
block|{
name|reporter
operator|.
name|progress
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Applied "
operator|+
name|editsCount
operator|+
literal|", skipped "
operator|+
name|skippedEdits
operator|+
literal|" because sequence id<= "
operator|+
name|maxSeqID
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|logReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|reconstructedCache
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// We create a "virtual flush" at maxSeqIdInLog+1.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing reconstructionCache"
argument_list|)
expr_stmt|;
block|}
name|internalFlushCache
argument_list|(
name|reconstructedCache
argument_list|,
name|maxSeqIdInLog
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
block|}
comment|/*    * Creates a series of HStoreFiles loaded from the given directory.    * There must be a matching 'mapdir' and 'loginfo' pair of files.    * If only one exists, we'll delete it.  Does other consistency tests    * checking files are not zero, etc.    *    * @param infodir qualified path for info file directory    * @param mapdir qualified path for map file directory    * @throws IOException    */
specifier|private
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|loadHStoreFiles
parameter_list|(
name|Path
name|infodir
parameter_list|,
name|Path
name|mapdir
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Look first at info files.  If a reference, these contain info we need
comment|// to create the HStoreFile.
name|FileStatus
name|infofiles
index|[]
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|infodir
argument_list|)
decl_stmt|;
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|results
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|Path
argument_list|>
name|mapfiles
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|(
name|infofiles
operator|.
name|length
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|infofiles
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|Path
name|p
init|=
name|infofiles
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Check for empty info file.  Should never be the case but can happen
comment|// after data loss in hdfs for whatever reason (upgrade, etc.): HBASE-646
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|p
argument_list|)
operator|.
name|getLen
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Skipping "
operator|+
name|p
operator|+
literal|" because its empty.  DATA LOSS?  Can "
operator|+
literal|"this scenario be repaired?  HBASE-646"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|Matcher
name|m
init|=
name|REF_NAME_PARSER
operator|.
name|matcher
argument_list|(
name|p
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
comment|/*        *  *  *  *  *  N O T E  *  *  *  *  *        *          *  We call isReference(Path, Matcher) here because it calls        *  Matcher.matches() which must be called before Matcher.group(int)        *  and we don't want to call Matcher.matches() twice.        *          *  *  *  *  *  N O T E  *  *  *  *  *        */
name|boolean
name|isReference
init|=
name|isReference
argument_list|(
name|p
argument_list|,
name|m
argument_list|)
decl_stmt|;
name|long
name|fid
init|=
name|Long
operator|.
name|parseLong
argument_list|(
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
argument_list|)
decl_stmt|;
name|HStoreFile
name|curfile
init|=
literal|null
decl_stmt|;
name|HStoreFile
operator|.
name|Reference
name|reference
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isReference
condition|)
block|{
name|reference
operator|=
name|HStoreFile
operator|.
name|readSplitInfo
argument_list|(
name|p
argument_list|,
name|fs
argument_list|)
expr_stmt|;
block|}
name|curfile
operator|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|,
name|fid
argument_list|,
name|reference
argument_list|)
expr_stmt|;
name|long
name|storeSeqId
init|=
operator|-
literal|1
decl_stmt|;
try|try
block|{
name|storeSeqId
operator|=
name|curfile
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|storeSeqId
operator|>
name|this
operator|.
name|maxSeqId
condition|)
block|{
name|this
operator|.
name|maxSeqId
operator|=
name|storeSeqId
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// If the HSTORE_LOGINFOFILE doesn't contain a number, just ignore it.
comment|// That means it was built prior to the previous run of HStore, and so
comment|// it cannot contain any updates also contained in the log.
name|LOG
operator|.
name|info
argument_list|(
literal|"HSTORE_LOGINFOFILE "
operator|+
name|curfile
operator|+
literal|" does not contain a sequence number - ignoring"
argument_list|)
expr_stmt|;
block|}
name|Path
name|mapfile
init|=
name|curfile
operator|.
name|getMapFilePath
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|mapfile
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|curfile
operator|.
name|getInfoFilePath
argument_list|()
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mapfile "
operator|+
name|mapfile
operator|.
name|toString
argument_list|()
operator|+
literal|" does not exist. "
operator|+
literal|"Cleaned up info file.  Continuing...Probable DATA LOSS!!!"
argument_list|)
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|isEmptyDataFile
argument_list|(
name|mapfile
argument_list|)
condition|)
block|{
name|curfile
operator|.
name|delete
argument_list|()
expr_stmt|;
comment|// We can have empty data file if data loss in hdfs.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mapfile "
operator|+
name|mapfile
operator|.
name|toString
argument_list|()
operator|+
literal|" has empty data. "
operator|+
literal|"Deleting.  Continuing...Probable DATA LOSS!!!  See HBASE-646."
argument_list|)
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|isEmptyIndexFile
argument_list|(
name|mapfile
argument_list|)
condition|)
block|{
try|try
block|{
comment|// Try fixing this file.. if we can.  Use the hbase version of fix.
comment|// Need to remove the old index file first else fix won't go ahead.
name|this
operator|.
name|fs
operator|.
name|delete
argument_list|(
operator|new
name|Path
argument_list|(
name|mapfile
argument_list|,
name|MapFile
operator|.
name|INDEX_FILE_NAME
argument_list|)
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|long
name|count
init|=
name|MapFile
operator|.
name|fix
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|mapfile
argument_list|,
name|HStoreFile
operator|.
name|HbaseMapFile
operator|.
name|KEY_CLASS
argument_list|,
name|HStoreFile
operator|.
name|HbaseMapFile
operator|.
name|VALUE_CLASS
argument_list|,
literal|false
argument_list|,
name|this
operator|.
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Fixed index on "
operator|+
name|mapfile
operator|.
name|toString
argument_list|()
operator|+
literal|"; had "
operator|+
name|count
operator|+
literal|" entries"
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed fix of "
operator|+
name|mapfile
operator|.
name|toString
argument_list|()
operator|+
literal|"...continuing; Probable DATA LOSS!!!"
argument_list|,
name|e
argument_list|)
expr_stmt|;
continue|continue;
block|}
block|}
name|storeSize
operator|+=
name|curfile
operator|.
name|length
argument_list|()
expr_stmt|;
comment|// TODO: Confirm referent exists.
comment|// Found map and sympathetic info file.  Add this hstorefile to result.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|p
argument_list|)
operator|+
literal|", isReference="
operator|+
name|isReference
operator|+
literal|", sequence id="
operator|+
name|storeSeqId
argument_list|)
expr_stmt|;
block|}
name|results
operator|.
name|put
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|storeSeqId
argument_list|)
argument_list|,
name|curfile
argument_list|)
expr_stmt|;
comment|// Keep list of sympathetic data mapfiles for cleaning info dir in next
comment|// section.  Make sure path is fully qualified for compare.
name|mapfiles
operator|.
name|add
argument_list|(
name|mapfile
argument_list|)
expr_stmt|;
block|}
comment|// List paths by experience returns fully qualified names -- at least when
comment|// running on a mini hdfs cluster.
name|FileStatus
name|datfiles
index|[]
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|mapdir
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|datfiles
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|Path
name|p
init|=
name|datfiles
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// If does not have sympathetic info file, delete.
if|if
condition|(
operator|!
name|mapfiles
operator|.
name|contains
argument_list|(
name|fs
operator|.
name|makeQualified
argument_list|(
name|p
argument_list|)
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|p
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|results
return|;
block|}
comment|/*     * @param mapfile    * @return True if the passed mapfile has a zero-length data component (its    * broken).    * @throws IOException    */
specifier|private
name|boolean
name|isEmptyDataFile
parameter_list|(
specifier|final
name|Path
name|mapfile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Mapfiles are made of 'data' and 'index' files.  Confirm 'data' is
comment|// non-null if it exists (may not have been written to yet).
return|return
name|isEmptyFile
argument_list|(
operator|new
name|Path
argument_list|(
name|mapfile
argument_list|,
name|MapFile
operator|.
name|DATA_FILE_NAME
argument_list|)
argument_list|)
return|;
block|}
comment|/*     * @param mapfile    * @return True if the passed mapfile has a zero-length index component (its    * broken).    * @throws IOException    */
specifier|private
name|boolean
name|isEmptyIndexFile
parameter_list|(
specifier|final
name|Path
name|mapfile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Mapfiles are made of 'data' and 'index' files.  Confirm 'data' is
comment|// non-null if it exists (may not have been written to yet).
return|return
name|isEmptyFile
argument_list|(
operator|new
name|Path
argument_list|(
name|mapfile
argument_list|,
name|MapFile
operator|.
name|INDEX_FILE_NAME
argument_list|)
argument_list|)
return|;
block|}
comment|/*     * @param mapfile    * @return True if the passed mapfile has a zero-length index component (its    * broken).    * @throws IOException    */
specifier|private
name|boolean
name|isEmptyFile
parameter_list|(
specifier|final
name|Path
name|f
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|f
argument_list|)
operator|&&
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|f
argument_list|)
operator|.
name|getLen
argument_list|()
operator|==
literal|0
return|;
block|}
comment|/**    * Adds a value to the memcache    *     * @param key    * @param value    * @return memcache size delta    */
specifier|protected
name|long
name|add
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|byte
index|[]
name|value
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memcache
operator|.
name|add
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Close all the MapFile readers    *     * We don't need to worry about subsequent requests because the HRegion holds    * a write lock that will prevent any more reads or writes.    *     * @throws IOException    */
name|List
argument_list|<
name|HStoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|ArrayList
argument_list|<
name|HStoreFile
argument_list|>
name|result
init|=
literal|null
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
for|for
control|(
name|MapFile
operator|.
name|Reader
name|reader
range|:
name|this
operator|.
name|readers
operator|.
name|values
argument_list|()
control|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
synchronized|synchronized
init|(
name|this
operator|.
name|storefiles
init|)
block|{
name|result
operator|=
operator|new
name|ArrayList
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"closed "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Flush changes to disk
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Snapshot this stores memcache.  Call before running    * {@link #flushCache(long)} so it has some work to do.    */
name|void
name|snapshot
parameter_list|()
block|{
name|this
operator|.
name|memcache
operator|.
name|snapshot
argument_list|()
expr_stmt|;
block|}
comment|/**    * Write out current snapshot.  Presumes {@link #snapshot()} has been called    * previously.    * @param logCacheFlushId flush sequence number    * @return true if a compaction is needed    * @throws IOException    */
name|boolean
name|flushCache
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Get the snapshot to flush.  Presumes that a call to
comment|// this.memcache.snapshot() has happened earlier up in the chain.
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|cache
init|=
name|this
operator|.
name|memcache
operator|.
name|getSnapshot
argument_list|()
decl_stmt|;
name|boolean
name|compactionNeeded
init|=
name|internalFlushCache
argument_list|(
name|cache
argument_list|,
name|logCacheFlushId
argument_list|)
decl_stmt|;
comment|// If an exception happens flushing, we let it out without clearing
comment|// the memcache snapshot.  The old snapshot will be returned when we say
comment|// 'snapshot', the next time flush comes around.
name|this
operator|.
name|memcache
operator|.
name|clearSnapshot
argument_list|(
name|cache
argument_list|)
expr_stmt|;
return|return
name|compactionNeeded
return|;
block|}
specifier|private
name|boolean
name|internalFlushCache
parameter_list|(
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|cache
parameter_list|,
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|flushed
init|=
literal|0
decl_stmt|;
comment|// Don't flush if there are no entries.
if|if
condition|(
name|cache
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// TODO:  We can fail in the below block before we complete adding this
comment|// flush to list of store files.  Add cleanup of anything put on filesystem
comment|// if we fail.
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
comment|// A. Write the Maps out to the disk
name|HStoreFile
name|flushedFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|,
operator|-
literal|1L
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|MapFile
operator|.
name|Writer
name|out
init|=
name|flushedFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBloomfilter
argument_list|()
argument_list|,
name|cache
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
name|out
operator|.
name|setIndexInterval
argument_list|(
name|family
operator|.
name|getMapFileIndexInterval
argument_list|()
argument_list|)
expr_stmt|;
comment|// Here we tried picking up an existing HStoreFile from disk and
comment|// interlacing the memcache flush compacting as we go.  The notion was
comment|// that interlacing would take as long as a pure flush with the added
comment|// benefit of having one less file in the store.  Experiments showed that
comment|// it takes two to three times the amount of time flushing -- more column
comment|// families makes it so the two timings come closer together -- but it
comment|// also complicates the flush. The code was removed.  Needed work picking
comment|// which file to interlace (favor references first, etc.)
comment|//
comment|// Related, looks like 'merging compactions' in BigTable paper interlaces
comment|// a memcache flush.  We don't.
name|int
name|entries
init|=
literal|0
decl_stmt|;
try|try
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|es
range|:
name|cache
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreKey
name|curkey
init|=
name|es
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|byte
index|[]
name|bytes
init|=
name|es
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|HStoreKey
operator|.
name|matchingFamily
argument_list|(
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|,
name|curkey
operator|.
name|getColumn
argument_list|()
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|curkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|entries
operator|++
expr_stmt|;
name|out
operator|.
name|append
argument_list|(
name|curkey
argument_list|,
operator|new
name|ImmutableBytesWritable
argument_list|(
name|bytes
argument_list|)
argument_list|)
expr_stmt|;
name|flushed
operator|+=
name|curkey
operator|.
name|getSize
argument_list|()
operator|+
operator|(
name|bytes
operator|==
literal|null
condition|?
literal|0
else|:
name|bytes
operator|.
name|length
operator|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"internalFlushCache: "
operator|+
name|curkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|long
name|newStoreSize
init|=
name|flushedFile
operator|.
name|length
argument_list|()
decl_stmt|;
name|storeSize
operator|+=
name|newStoreSize
expr_stmt|;
comment|// B. Write out the log sequence number that corresponds to this output
comment|// MapFile.  The MapFile is current up to and including the log seq num.
name|flushedFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
comment|// C. Finally, make the new MapFile available.
name|updateReaders
argument_list|(
name|logCacheFlushId
argument_list|,
name|flushedFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Added "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|flushedFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
operator|+
literal|" with "
operator|+
name|entries
operator|+
literal|" entries, sequence id "
operator|+
name|logCacheFlushId
operator|+
literal|", data size "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|flushed
argument_list|)
operator|+
literal|", file size "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|newStoreSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|storefiles
operator|.
name|size
argument_list|()
operator|>=
name|compactionThreshold
return|;
block|}
comment|/*    * Change readers adding into place the Reader produced by this new flush.    * @param logCacheFlushId    * @param flushedFile    * @throws IOException    */
specifier|private
name|void
name|updateReaders
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
specifier|final
name|HStoreFile
name|flushedFile
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|Long
name|flushid
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|logCacheFlushId
argument_list|)
decl_stmt|;
comment|// Open the map file reader.
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|flushedFile
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBloomfilter
argument_list|()
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBlockCacheEnabled
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|flushedFile
argument_list|)
expr_stmt|;
comment|// Tell listeners of the change in readers.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Notify all observers that set of Readers has changed.    * @throws IOException    */
specifier|private
name|void
name|notifyChangedReadersObservers
parameter_list|()
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|this
operator|.
name|changedReaderObservers
init|)
block|{
for|for
control|(
name|ChangedReadersObserver
name|o
range|:
name|this
operator|.
name|changedReaderObservers
control|)
block|{
name|o
operator|.
name|updateReaders
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/*    * @param o Observer who wants to know about changes in set of Readers    */
name|void
name|addChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
name|this
operator|.
name|changedReaderObservers
operator|.
name|add
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|/*    * @param o Observer no longer interested in changes in set of Readers.    */
name|void
name|deleteChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|changedReaderObservers
operator|.
name|remove
argument_list|(
name|o
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Not in set"
operator|+
name|o
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/*    * @param files    * @return True if any of the files in<code>files</code> are References.    */
specifier|private
name|boolean
name|hasReferences
parameter_list|(
name|Collection
argument_list|<
name|HStoreFile
argument_list|>
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|files
control|)
block|{
if|if
condition|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/**    * Compact the back-HStores.  This method may take some time, so the calling     * thread must be able to block for long periods.    *     *<p>During this time, the HStore can work as usual, getting values from    * MapFiles and writing new MapFiles from the Memcache.    *     * Existing MapFiles are not destroyed until the new compacted TreeMap is     * completely written-out to disk.    *    * The compactLock prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *     * We don't want to hold the structureLock for the whole time, as a compact()     * can be lengthy and we want to allow cache-flushes during this period.    *     * @param force True to force a compaction regardless of thresholds (Needed    * by merge).    * @return mid key if a split is needed, null otherwise    * @throws IOException    */
name|StoreSize
name|compact
parameter_list|(
specifier|final
name|boolean
name|force
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
name|long
name|maxId
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|nrows
init|=
operator|-
literal|1
decl_stmt|;
name|List
argument_list|<
name|HStoreFile
argument_list|>
name|filesToCompact
init|=
literal|null
decl_stmt|;
synchronized|synchronized
init|(
name|storefiles
init|)
block|{
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
comment|// The max-sequenceID in any of the to-be-compacted TreeMaps is the
comment|// last key of storefiles.
name|maxId
operator|=
name|this
operator|.
name|storefiles
operator|.
name|lastKey
argument_list|()
operator|.
name|longValue
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|force
operator|&&
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|compactionThreshold
condition|)
block|{
return|return
name|checkSplit
argument_list|()
return|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|compactionDir
argument_list|)
operator|&&
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|compactionDir
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mkdir on "
operator|+
name|compactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" failed"
argument_list|)
expr_stmt|;
return|return
name|checkSplit
argument_list|()
return|;
block|}
comment|// HBASE-745, preparing all store file size for incremental compacting selection.
name|int
name|countOfFiles
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
name|totalSize
init|=
literal|0
decl_stmt|;
name|long
index|[]
name|fileSizes
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
name|long
name|skipped
init|=
literal|0
decl_stmt|;
name|int
name|point
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|countOfFiles
condition|;
name|i
operator|++
control|)
block|{
name|HStoreFile
name|file
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|Path
name|path
init|=
name|file
operator|.
name|getMapFilePath
argument_list|()
decl_stmt|;
name|int
name|len
init|=
literal|0
decl_stmt|;
for|for
control|(
name|FileStatus
name|fstatus
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|)
control|)
block|{
name|len
operator|+=
name|fstatus
operator|.
name|getLen
argument_list|()
expr_stmt|;
block|}
name|fileSizes
index|[
name|i
index|]
operator|=
name|len
expr_stmt|;
name|totalSize
operator|+=
name|len
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|force
operator|&&
operator|!
name|hasReferences
argument_list|(
name|filesToCompact
argument_list|)
condition|)
block|{
comment|// Here we select files for incremental compaction.
comment|// The rule is: if the largest(oldest) one is more than twice the
comment|// size of the second, skip the largest, and continue to next...,
comment|// until we meet the compactionThreshold limit.
for|for
control|(
name|point
operator|=
literal|0
init|;
name|point
operator|<
name|compactionThreshold
operator|-
literal|1
condition|;
name|point
operator|++
control|)
block|{
if|if
condition|(
name|fileSizes
index|[
name|point
index|]
operator|<
name|fileSizes
index|[
name|point
operator|+
literal|1
index|]
operator|*
literal|2
condition|)
block|{
break|break;
block|}
name|skipped
operator|+=
name|fileSizes
index|[
name|point
index|]
expr_stmt|;
block|}
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|filesToCompact
operator|.
name|subList
argument_list|(
name|point
argument_list|,
name|countOfFiles
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Compaction size "
operator|+
name|totalSize
operator|+
literal|", skipped "
operator|+
name|point
operator|+
literal|", "
operator|+
name|skipped
argument_list|)
expr_stmt|;
block|}
comment|/*        * We create a new list of MapFile.Reader objects so we don't screw up        * the caching associated with the currently-loaded ones. Our iteration-        * based access pattern is practically designed to ruin the cache.        */
name|List
argument_list|<
name|MapFile
operator|.
name|Reader
argument_list|>
name|readers
init|=
operator|new
name|ArrayList
argument_list|<
name|MapFile
operator|.
name|Reader
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|HStoreFile
name|file
range|:
name|filesToCompact
control|)
block|{
try|try
block|{
name|HStoreFile
operator|.
name|BloomFilterMapFile
operator|.
name|Reader
name|reader
init|=
name|file
operator|.
name|getReader
argument_list|(
name|fs
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|readers
operator|.
name|add
argument_list|(
name|reader
argument_list|)
expr_stmt|;
comment|// Compute the size of the new bloomfilter if needed
if|if
condition|(
name|this
operator|.
name|family
operator|.
name|isBloomfilter
argument_list|()
condition|)
block|{
name|nrows
operator|+=
name|reader
operator|.
name|getBloomFilterSize
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// Add info about which file threw exception. It may not be in the
comment|// exception message so output a message here where we know the
comment|// culprit.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed with "
operator|+
name|e
operator|.
name|toString
argument_list|()
operator|+
literal|": "
operator|+
name|file
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|closeCompactionReaders
argument_list|(
name|readers
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
block|}
comment|// Storefiles are keyed by sequence id. The oldest file comes first.
comment|// We need to return out of here a List that has the newest file first.
name|Collections
operator|.
name|reverse
argument_list|(
name|readers
argument_list|)
expr_stmt|;
comment|// Step through them, writing to the brand-new MapFile
name|HStoreFile
name|compactedOutputFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|this
operator|.
name|compactionDir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|,
operator|-
literal|1L
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"started compaction of "
operator|+
name|readers
operator|.
name|size
argument_list|()
operator|+
literal|" files into "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|compactedOutputFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|MapFile
operator|.
name|Writer
name|writer
init|=
name|compactedOutputFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBloomfilter
argument_list|()
argument_list|,
name|nrows
argument_list|)
decl_stmt|;
name|writer
operator|.
name|setIndexInterval
argument_list|(
name|family
operator|.
name|getMapFileIndexInterval
argument_list|()
argument_list|)
expr_stmt|;
try|try
block|{
name|compactHStoreFiles
argument_list|(
name|writer
argument_list|,
name|readers
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|maxId
argument_list|)
expr_stmt|;
comment|// Move the compaction into place.
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|compactedOutputFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Completed compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" store size is "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|storeSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|checkSplit
argument_list|()
return|;
block|}
comment|/*    * Compact a list of MapFile.Readers into MapFile.Writer.    *     * We work by iterating through the readers in parallel. We always increment    * the lowest-ranked one.    * Updates to a single row/column will appear ranked by timestamp. This allows    * us to throw out deleted values or obsolete versions.    */
specifier|private
name|void
name|compactHStoreFiles
parameter_list|(
specifier|final
name|MapFile
operator|.
name|Writer
name|compactedOut
parameter_list|,
specifier|final
name|List
argument_list|<
name|MapFile
operator|.
name|Reader
argument_list|>
name|readers
parameter_list|)
throws|throws
name|IOException
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|rdrs
init|=
name|readers
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|readers
operator|.
name|size
argument_list|()
index|]
argument_list|)
decl_stmt|;
try|try
block|{
name|HStoreKey
index|[]
name|keys
init|=
operator|new
name|HStoreKey
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
name|ImmutableBytesWritable
index|[]
name|vals
init|=
operator|new
name|ImmutableBytesWritable
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
name|boolean
index|[]
name|done
init|=
operator|new
name|boolean
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|keys
index|[
name|i
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
name|vals
index|[
name|i
index|]
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
literal|false
expr_stmt|;
block|}
comment|// Now, advance through the readers in order.  This will have the
comment|// effect of a run-time sort of the entire dataset.
name|int
name|numDone
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|rdrs
index|[
name|i
index|]
operator|.
name|reset
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
operator|!
name|rdrs
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|vals
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
name|numDone
operator|++
expr_stmt|;
block|}
block|}
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|int
name|timesSeen
init|=
literal|0
decl_stmt|;
name|byte
index|[]
name|lastRow
init|=
literal|null
decl_stmt|;
name|byte
index|[]
name|lastColumn
init|=
literal|null
decl_stmt|;
while|while
condition|(
name|numDone
operator|<
name|done
operator|.
name|length
condition|)
block|{
comment|// Find the reader with the smallest key.  If two files have same key
comment|// but different values -- i.e. one is delete and other is non-delete
comment|// value -- we will find the first, the one that was written later and
comment|// therefore the one whose value should make it out to the compacted
comment|// store file.
name|int
name|smallestKey
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|smallestKey
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|keys
index|[
name|i
index|]
operator|.
name|compareTo
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|)
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
block|}
block|}
comment|// Reflect the current key/val in the output
name|HStoreKey
name|sk
init|=
name|keys
index|[
name|smallestKey
index|]
decl_stmt|;
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|lastRow
argument_list|,
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
operator|&&
name|Bytes
operator|.
name|equals
argument_list|(
name|lastColumn
argument_list|,
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
condition|)
block|{
name|timesSeen
operator|++
expr_stmt|;
block|}
else|else
block|{
name|timesSeen
operator|=
literal|0
expr_stmt|;
block|}
if|if
condition|(
name|timesSeen
operator|<=
name|family
operator|.
name|getMaxVersions
argument_list|()
condition|)
block|{
comment|// Keep old versions until we have maxVersions worth.
comment|// Then just skip them.
if|if
condition|(
name|sk
operator|.
name|getRow
argument_list|()
operator|.
name|length
operator|!=
literal|0
operator|&&
name|sk
operator|.
name|getColumn
argument_list|()
operator|.
name|length
operator|!=
literal|0
condition|)
block|{
comment|// Only write out objects which have a non-zero length key and
comment|// value
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|sk
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|compactedOut
operator|.
name|append
argument_list|(
name|sk
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"compactHStoreFiles: "
operator|+
name|sk
operator|+
literal|": expired, deleted"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// Update last-seen items
name|lastRow
operator|=
name|sk
operator|.
name|getRow
argument_list|()
expr_stmt|;
name|lastColumn
operator|=
name|sk
operator|.
name|getColumn
argument_list|()
expr_stmt|;
comment|// Advance the smallest key.  If that reader's all finished, then
comment|// mark it as done.
if|if
condition|(
operator|!
name|rdrs
index|[
name|smallestKey
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
condition|)
block|{
name|done
index|[
name|smallestKey
index|]
operator|=
literal|true
expr_stmt|;
name|rdrs
index|[
name|smallestKey
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
name|rdrs
index|[
name|smallestKey
index|]
operator|=
literal|null
expr_stmt|;
name|numDone
operator|++
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|closeCompactionReaders
argument_list|(
name|Arrays
operator|.
name|asList
argument_list|(
name|rdrs
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|closeCompactionReaders
parameter_list|(
specifier|final
name|List
argument_list|<
name|MapFile
operator|.
name|Reader
argument_list|>
name|rdrs
parameter_list|)
block|{
for|for
control|(
name|MapFile
operator|.
name|Reader
name|r
range|:
name|rdrs
control|)
block|{
try|try
block|{
if|if
condition|(
name|r
operator|!=
literal|null
condition|)
block|{
name|r
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception closing reader for "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/*    * Check if this is cell is deleted.    * If a memcache and a deletes, check key does not have an entry filled.    * Otherwise, check value is not the<code>HGlobals.deleteBytes</code> value.    * If passed value IS deleteBytes, then it is added to the passed    * deletes map.    * @param hsk    * @param value    * @param checkMemcache true if the memcache should be consulted    * @param deletes Map keyed by column with a value of timestamp. Can be null.    * If non-null and passed value is HGlobals.deleteBytes, then we add to this    * map.    * @return True if this is a deleted cell.  Adds the passed deletes map if    * passed value is HGlobals.deleteBytes.   */
specifier|private
name|boolean
name|isDeleted
parameter_list|(
specifier|final
name|HStoreKey
name|hsk
parameter_list|,
specifier|final
name|byte
index|[]
name|value
parameter_list|,
specifier|final
name|boolean
name|checkMemcache
parameter_list|,
specifier|final
name|Map
argument_list|<
name|byte
index|[]
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
name|deletes
parameter_list|)
block|{
if|if
condition|(
name|checkMemcache
operator|&&
name|memcache
operator|.
name|isDeleted
argument_list|(
name|hsk
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
name|List
argument_list|<
name|Long
argument_list|>
name|timestamps
init|=
operator|(
name|deletes
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|deletes
operator|.
name|get
argument_list|(
name|hsk
operator|.
name|getColumn
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|timestamps
operator|!=
literal|null
operator|&&
name|timestamps
operator|.
name|contains
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|hsk
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|value
operator|==
literal|null
condition|)
block|{
comment|// If a null value, shouldn't be in here.  Mark it as deleted cell.
return|return
literal|true
return|;
block|}
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|value
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Cell has delete value.  Save it into deletes.
if|if
condition|(
name|deletes
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|timestamps
operator|==
literal|null
condition|)
block|{
name|timestamps
operator|=
operator|new
name|ArrayList
argument_list|<
name|Long
argument_list|>
argument_list|()
expr_stmt|;
name|deletes
operator|.
name|put
argument_list|(
name|hsk
operator|.
name|getColumn
argument_list|()
argument_list|,
name|timestamps
argument_list|)
expr_stmt|;
block|}
comment|// We know its not already in the deletes array else we'd have returned
comment|// earlier so no need to test if timestamps already has this value.
name|timestamps
operator|.
name|add
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|hsk
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
literal|true
return|;
block|}
comment|/*    * It's assumed that the compactLock  will be acquired prior to calling this     * method!  Otherwise, it is not thread-safe!    *    * It works by processing a compaction that's been written to disk.    *     *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *     *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Moving the new compacted MapFile into place    * 2) Unload all replaced MapFiles, close and collect list to delete.    * 3) Loading the new TreeMap.    * 4) Compute new store size    *</pre>    *     * @param compactedFiles list of files that were compacted    * @param compactedFile HStoreFile that is the result of the compaction    * @throws IOException    */
specifier|private
name|void
name|completeCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|HStoreFile
argument_list|>
name|compactedFiles
parameter_list|,
specifier|final
name|HStoreFile
name|compactedFile
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// 1. Moving the new MapFile into place.
name|HStoreFile
name|finalCompactedFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|,
operator|-
literal|1
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"moving "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|compactedFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
operator|+
literal|" to "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|finalCompactedFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|compactedFile
operator|.
name|rename
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|finalCompactedFile
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|finalCompactedFile
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// 2. Unload all replaced MapFiles, close and collect list to delete.
synchronized|synchronized
init|(
name|storefiles
init|)
block|{
name|Map
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|toDelete
init|=
operator|new
name|HashMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|compactedFiles
operator|.
name|contains
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Long
name|key
init|=
name|e
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|MapFile
operator|.
name|Reader
name|reader
init|=
name|this
operator|.
name|readers
operator|.
name|remove
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|toDelete
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
try|try
block|{
comment|// 3. Loading the new TreeMap.
comment|// Change this.storefiles so it reflects new state but do not
comment|// delete old store files until we have sent out notification of
comment|// change in case old files are still being accessed by outstanding
comment|// scanners.
for|for
control|(
name|Long
name|key
range|:
name|toDelete
operator|.
name|keySet
argument_list|()
control|)
block|{
name|this
operator|.
name|storefiles
operator|.
name|remove
argument_list|(
name|key
argument_list|)
expr_stmt|;
block|}
comment|// Add new compacted Reader and store file.
name|Long
name|orderVal
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|finalCompactedFile
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
comment|// Use a block cache (if configured) for this reader since
comment|// it is the only one.
name|finalCompactedFile
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBloomfilter
argument_list|()
argument_list|,
name|this
operator|.
name|family
operator|.
name|isBlockCacheEnabled
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|finalCompactedFile
argument_list|)
expr_stmt|;
comment|// Tell observers that list of Readers has changed.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
comment|// Finally, delete old store files.
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|toDelete
operator|.
name|values
argument_list|()
control|)
block|{
name|hsf
operator|.
name|delete
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|=
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files for "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|". Compacted file is "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|".  Files replaced are "
operator|+
name|compactedFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// 4. Compute new store size
name|storeSize
operator|=
literal|0L
expr_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
name|storeSize
operator|+=
name|hsf
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|// ////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return all the available columns for the given key.  The key indicates a     * row and timestamp, but not a column name.    *    * The returned object should map column names to Cells.    */
name|void
name|getFull
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
specifier|final
name|Set
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
name|Map
argument_list|<
name|byte
index|[]
argument_list|,
name|Cell
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
name|Map
argument_list|<
name|byte
index|[]
argument_list|,
name|Long
argument_list|>
name|deletes
init|=
operator|new
name|TreeMap
argument_list|<
name|byte
index|[]
argument_list|,
name|Long
argument_list|>
argument_list|(
name|Bytes
operator|.
name|BYTES_COMPARATOR
argument_list|)
decl_stmt|;
comment|// if the key is null, we're not even looking for anything. return.
if|if
condition|(
name|key
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
comment|// get from the memcache first.
name|memcache
operator|.
name|getFull
argument_list|(
name|key
argument_list|,
name|columns
argument_list|,
name|deletes
argument_list|,
name|results
argument_list|)
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
comment|// examine each mapfile
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
comment|// synchronize on the map so that no one else iterates it at the same
comment|// time
name|getFullFromMapFile
argument_list|(
name|map
argument_list|,
name|key
argument_list|,
name|columns
argument_list|,
name|deletes
argument_list|,
name|results
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|getFullFromMapFile
parameter_list|(
name|MapFile
operator|.
name|Reader
name|map
parameter_list|,
name|HStoreKey
name|key
parameter_list|,
name|Set
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
name|Map
argument_list|<
name|byte
index|[]
argument_list|,
name|Long
argument_list|>
name|deletes
parameter_list|,
name|Map
argument_list|<
name|byte
index|[]
argument_list|,
name|Cell
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|map
init|)
block|{
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
comment|// seek back to the beginning
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
comment|// seek to the closest key that should match the row we're looking for
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
return|return;
block|}
do|do
block|{
name|byte
index|[]
name|readcol
init|=
name|readkey
operator|.
name|getColumn
argument_list|()
decl_stmt|;
comment|// if we're looking for this column (or all of them), and there isn't
comment|// already a value for this column in the results map, and the key we
comment|// just read matches, then we'll consider it
if|if
condition|(
operator|(
name|columns
operator|==
literal|null
operator|||
name|columns
operator|.
name|contains
argument_list|(
name|readcol
argument_list|)
operator|)
operator|&&
operator|!
name|results
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|&&
name|key
operator|.
name|matchesWithoutColumn
argument_list|(
name|readkey
argument_list|)
condition|)
block|{
comment|// if the value of the cell we're looking at right now is a delete,
comment|// we need to treat it differently
if|if
condition|(
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
comment|// if it's not already recorded as a delete or recorded with a more
comment|// recent delete timestamp, record it for later
if|if
condition|(
operator|!
name|deletes
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|||
name|deletes
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|.
name|longValue
argument_list|()
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|deletes
operator|.
name|put
argument_list|(
name|readcol
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
operator|!
operator|(
name|deletes
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|&&
name|deletes
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|.
name|longValue
argument_list|()
operator|>=
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|)
condition|)
block|{
comment|// So the cell itself isn't a delete, but there may be a delete
comment|// pending from earlier in our search. Only record this result if
comment|// there aren't any pending deletes.
if|if
condition|(
operator|!
operator|(
name|deletes
operator|.
name|containsKey
argument_list|(
name|readcol
argument_list|)
operator|&&
name|deletes
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|.
name|longValue
argument_list|()
operator|>=
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|results
operator|.
name|put
argument_list|(
name|readcol
argument_list|,
operator|new
name|Cell
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// need to reinstantiate the readval so we can reuse it,
comment|// otherwise next iteration will destroy our result
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"getFullFromMapFile: "
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|Bytes
operator|.
name|compareTo
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|<
literal|0
condition|)
block|{
comment|// if we've crossed into the next row, then we can just stop
comment|// iterating
break|break;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
block|}
block|}
name|MapFile
operator|.
name|Reader
index|[]
name|getReaders
parameter_list|()
block|{
return|return
name|this
operator|.
name|readers
operator|.
name|values
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|this
operator|.
name|readers
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
comment|/**    * Get the value for the indicated HStoreKey.  Grab the target value and the     * previous<code>numVersions - 1</code> values, as well.    *    * Use {@link HConstants.ALL_VERSIONS} to retrieve all versions.    * @param key    * @param numVersions Number of versions to fetch.  Must be> 0.    * @return values for the specified versions    * @throws IOException    */
name|Cell
index|[]
name|get
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|numVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
try|try
block|{
comment|// Check the memcache
name|List
argument_list|<
name|Cell
argument_list|>
name|results
init|=
name|this
operator|.
name|memcache
operator|.
name|get
argument_list|(
name|key
argument_list|,
name|numVersions
argument_list|)
decl_stmt|;
comment|// If we got sufficient versions from memcache, return.
if|if
condition|(
name|results
operator|.
name|size
argument_list|()
operator|==
name|numVersions
condition|)
block|{
return|return
name|results
operator|.
name|toArray
argument_list|(
operator|new
name|Cell
index|[
name|results
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
comment|// Keep a list of deleted cell keys.  We need this because as we go through
comment|// the store files, the cell with the delete marker may be in one file and
comment|// the old non-delete cell value in a later store file. If we don't keep
comment|// around the fact that the cell was deleted in a newer record, we end up
comment|// returning the old value if user is asking for more than one version.
comment|// This List of deletes should not large since we are only keeping rows
comment|// and columns that match those set on the scanner and which have delete
comment|// values.  If memory usage becomes an issue, could redo as bloom filter.
name|Map
argument_list|<
name|byte
index|[]
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
name|deletes
init|=
operator|new
name|TreeMap
argument_list|<
name|byte
index|[]
argument_list|,
name|List
argument_list|<
name|Long
argument_list|>
argument_list|>
argument_list|(
name|Bytes
operator|.
name|BYTES_COMPARATOR
argument_list|)
decl_stmt|;
comment|// This code below is very close to the body of the getKeys method.
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// map.getClosest returns null if the passed key is> than the
comment|// last key in the map file.  getClosest is a bit of a misnomer
comment|// since it returns exact match or the next closest key AFTER not
comment|// BEFORE.
continue|continue;
block|}
if|if
condition|(
operator|!
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
operator|!
name|isDeleted
argument_list|(
name|readkey
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|,
literal|true
argument_list|,
name|deletes
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|results
operator|.
name|add
argument_list|(
operator|new
name|Cell
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"get: "
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Perhaps only one version is wanted.  I could let this
comment|// test happen later in the for loop test but it would cost
comment|// the allocation of an ImmutableBytesWritable.
if|if
condition|(
name|hasEnoughVersions
argument_list|(
name|numVersions
argument_list|,
name|results
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
for|for
control|(
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
init|;
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
operator|&&
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
operator|&&
operator|!
name|hasEnoughVersions
argument_list|(
name|numVersions
argument_list|,
name|results
argument_list|)
condition|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|isDeleted
argument_list|(
name|readkey
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|,
literal|true
argument_list|,
name|deletes
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|results
operator|.
name|add
argument_list|(
operator|new
name|Cell
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"get: "
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
if|if
condition|(
name|hasEnoughVersions
argument_list|(
name|numVersions
argument_list|,
name|results
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
return|return
name|results
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|?
literal|null
else|:
name|results
operator|.
name|toArray
argument_list|(
operator|new
name|Cell
index|[
name|results
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Small method to check if we are over the max number of versions    * or we acheived this family max versions.     * The later happens when we have the situation described in HBASE-621.    * @param numVersions    * @param results    * @return     */
specifier|private
name|boolean
name|hasEnoughVersions
parameter_list|(
specifier|final
name|int
name|numVersions
parameter_list|,
specifier|final
name|List
argument_list|<
name|Cell
argument_list|>
name|results
parameter_list|)
block|{
return|return
operator|(
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
operator|||
name|results
operator|.
name|size
argument_list|()
operator|>=
name|family
operator|.
name|getMaxVersions
argument_list|()
operator|)
return|;
block|}
comment|/**    * Get<code>versions</code> keys matching the origin key's    * row/column/timestamp and those of an older vintage    * Default access so can be accessed out of {@link HRegionServer}.    * @param origin Where to start searching.    * @param versions How many versions to return. Pass    * {@link HConstants.ALL_VERSIONS} to retrieve all. Versions will include    * size of passed<code>allKeys</code> in its count.    * @param allKeys List of keys prepopulated by keys we found in memcache.    * This method returns this passed list with all matching keys found in    * stores appended.    * @return The passed<code>allKeys</code> with<code>versions</code> of    * matching keys found in store files appended.    * @throws IOException    */
name|List
argument_list|<
name|HStoreKey
argument_list|>
name|getKeys
parameter_list|(
specifier|final
name|HStoreKey
name|origin
parameter_list|,
specifier|final
name|int
name|versions
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|HStoreKey
argument_list|>
name|keys
init|=
name|this
operator|.
name|memcache
operator|.
name|getKeys
argument_list|(
name|origin
argument_list|,
name|versions
argument_list|)
decl_stmt|;
if|if
condition|(
name|keys
operator|.
name|size
argument_list|()
operator|>=
name|versions
condition|)
block|{
return|return
name|keys
return|;
block|}
comment|// This code below is very close to the body of the get method.
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
comment|// do the priming read
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|origin
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// map.getClosest returns null if the passed key is> than the
comment|// last key in the map file.  getClosest is a bit of a misnomer
comment|// since it returns exact match or the next closest key AFTER not
comment|// BEFORE.
continue|continue;
block|}
do|do
block|{
comment|// if the row matches, we might want this one.
if|if
condition|(
name|rowMatches
argument_list|(
name|origin
argument_list|,
name|readkey
argument_list|)
condition|)
block|{
comment|// if the cell matches, then we definitely want this key.
if|if
condition|(
name|cellMatches
argument_list|(
name|origin
argument_list|,
name|readkey
argument_list|)
condition|)
block|{
comment|// store the key if it isn't deleted or superceeded by what's
comment|// in the memcache
if|if
condition|(
operator|!
name|isDeleted
argument_list|(
name|readkey
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|,
literal|false
argument_list|,
literal|null
argument_list|)
operator|&&
operator|!
name|keys
operator|.
name|contains
argument_list|(
name|readkey
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|keys
operator|.
name|add
argument_list|(
operator|new
name|HStoreKey
argument_list|(
name|readkey
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"getKeys: "
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
comment|// if we've collected enough versions, then exit the loop.
if|if
condition|(
name|keys
operator|.
name|size
argument_list|()
operator|>=
name|versions
condition|)
block|{
break|break;
block|}
block|}
block|}
else|else
block|{
comment|// the cell doesn't match, but there might be more with different
comment|// timestamps, so move to the next key
continue|continue;
block|}
block|}
else|else
block|{
comment|// the row doesn't match, so we've gone too far.
break|break;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
comment|// advance to the next key
block|}
block|}
return|return
name|keys
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Find the key that matches<i>row</i> exactly, or the one that immediately    * preceeds it. WARNING: Only use this method on a table where writes occur     * with stricly increasing timestamps. This method assumes this pattern of     * writes in order to make it reasonably performant.     */
name|byte
index|[]
name|getRowKeyAtOrBefore
parameter_list|(
specifier|final
name|byte
index|[]
name|row
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Map of HStoreKeys that are candidates for holding the row key that
comment|// most closely matches what we're looking for. We'll have to update it
comment|// deletes found all over the place as we go along before finally reading
comment|// the best key out of it at the end.
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
name|candidateKeys
init|=
operator|new
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
argument_list|()
decl_stmt|;
comment|// Obtain read lock
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Process each store file
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
comment|// update the candidate keys from the current map file
name|rowAtOrBeforeFromMapFile
argument_list|(
name|maparray
index|[
name|i
index|]
argument_list|,
name|row
argument_list|,
name|candidateKeys
argument_list|)
expr_stmt|;
block|}
comment|// Finally, check the memcache
name|this
operator|.
name|memcache
operator|.
name|getRowKeyAtOrBefore
argument_list|(
name|row
argument_list|,
name|candidateKeys
argument_list|)
expr_stmt|;
comment|// Return the best key from candidateKeys
return|return
name|candidateKeys
operator|.
name|isEmpty
argument_list|()
condition|?
literal|null
else|:
name|candidateKeys
operator|.
name|lastKey
argument_list|()
operator|.
name|getRow
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Check an individual MapFile for the row at or before a given key     * and timestamp    */
specifier|private
name|void
name|rowAtOrBeforeFromMapFile
parameter_list|(
name|MapFile
operator|.
name|Reader
name|map
parameter_list|,
specifier|final
name|byte
index|[]
name|row
parameter_list|,
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
name|candidateKeys
parameter_list|)
throws|throws
name|IOException
block|{
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
comment|// don't bother with the rest of this if the file is empty
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
block|{
return|return;
block|}
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
comment|// if there aren't any candidate keys yet, we'll do some things slightly
comment|// different
if|if
condition|(
name|candidateKeys
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|rowKeyFromMapFileEmptyKeys
argument_list|(
name|map
argument_list|,
name|row
argument_list|,
name|candidateKeys
argument_list|,
name|now
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|rowKeyAtOrBeforeExistingCandKeys
argument_list|(
name|map
argument_list|,
name|row
argument_list|,
name|candidateKeys
argument_list|,
name|now
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|void
name|rowKeyFromMapFileEmptyKeys
parameter_list|(
name|MapFile
operator|.
name|Reader
name|map
parameter_list|,
name|byte
index|[]
name|row
parameter_list|,
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
name|candidateKeys
parameter_list|,
name|long
name|now
parameter_list|)
throws|throws
name|IOException
block|{
name|HStoreKey
name|searchKey
init|=
operator|new
name|HStoreKey
argument_list|(
name|row
argument_list|)
decl_stmt|;
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
comment|// if the row we're looking for is past the end of this mapfile, just
comment|// save time and add the last key to the candidates.
name|HStoreKey
name|finalKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|map
operator|.
name|finalKey
argument_list|(
name|finalKey
argument_list|)
expr_stmt|;
if|if
condition|(
name|Bytes
operator|.
name|compareTo
argument_list|(
name|finalKey
operator|.
name|getRow
argument_list|()
argument_list|,
name|row
argument_list|)
operator|<
literal|0
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|stripTimestamp
argument_list|(
name|finalKey
argument_list|)
argument_list|,
operator|new
name|Long
argument_list|(
name|finalKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
return|return;
block|}
name|HStoreKey
name|deletedOrExpiredRow
init|=
literal|null
decl_stmt|;
name|boolean
name|foundCandidate
init|=
literal|false
decl_stmt|;
while|while
condition|(
operator|!
name|foundCandidate
condition|)
block|{
comment|// seek to the exact row, or the one that would be immediately before it
name|readkey
operator|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|searchKey
argument_list|,
name|readval
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// didn't find anything that would match, so return
return|return;
block|}
do|do
block|{
comment|// if we have an exact match on row, and it's not a delete, save this
comment|// as a candidate key
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|,
name|row
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
argument_list|,
operator|new
name|Long
argument_list|(
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|foundCandidate
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"rowAtOrBeforeFromMapFile:"
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
name|deletedOrExpiredRow
operator|=
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|Bytes
operator|.
name|compareTo
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|,
name|row
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the row key we just read is beyond the key we're searching for,
comment|// then we're done. return.
break|break;
block|}
else|else
block|{
comment|// so, the row key doesn't match, but we haven't gone past the row
comment|// we're seeking yet, so this row is a candidate for closest
comment|// (assuming that it isn't a delete).
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
argument_list|,
operator|new
name|Long
argument_list|(
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|foundCandidate
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"rowAtOrBeforeFromMapFile:"
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
name|deletedOrExpiredRow
operator|=
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
expr_stmt|;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
comment|// If we get here and have no candidates but we did find a deleted or
comment|// expired candidate, we need to look at the key before that
if|if
condition|(
operator|!
name|foundCandidate
operator|&&
name|deletedOrExpiredRow
operator|!=
literal|null
condition|)
block|{
name|searchKey
operator|=
name|deletedOrExpiredRow
expr_stmt|;
name|deletedOrExpiredRow
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
comment|// No candidates and no deleted or expired candidates. Give up.
break|break;
block|}
block|}
comment|// arriving here just means that we consumed the whole rest of the map
comment|// without going "past" the key we're searching for. we can just fall
comment|// through here.
block|}
specifier|private
name|void
name|rowKeyAtOrBeforeExistingCandKeys
parameter_list|(
name|MapFile
operator|.
name|Reader
name|map
parameter_list|,
name|byte
index|[]
name|row
parameter_list|,
name|SortedMap
argument_list|<
name|HStoreKey
argument_list|,
name|Long
argument_list|>
name|candidateKeys
parameter_list|,
name|long
name|now
parameter_list|)
throws|throws
name|IOException
block|{
name|HStoreKey
name|strippedKey
init|=
literal|null
decl_stmt|;
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
comment|// if there are already candidate keys, we need to start our search
comment|// at the earliest possible key so that we can discover any possible
comment|// deletes for keys between the start and the search key.
name|HStoreKey
name|searchKey
init|=
operator|new
name|HStoreKey
argument_list|(
name|candidateKeys
operator|.
name|firstKey
argument_list|()
operator|.
name|getRow
argument_list|()
argument_list|)
decl_stmt|;
comment|// if the row we're looking for is past the end of this mapfile, just
comment|// save time and add the last key to the candidates.
name|HStoreKey
name|finalKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|map
operator|.
name|finalKey
argument_list|(
name|finalKey
argument_list|)
expr_stmt|;
if|if
condition|(
name|Bytes
operator|.
name|compareTo
argument_list|(
name|finalKey
operator|.
name|getRow
argument_list|()
argument_list|,
name|searchKey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|<
literal|0
condition|)
block|{
name|strippedKey
operator|=
name|stripTimestamp
argument_list|(
name|finalKey
argument_list|)
expr_stmt|;
comment|// if the candidate keys has a cell like this one already,
comment|// then we might want to update the timestamp we're using on it
if|if
condition|(
name|candidateKeys
operator|.
name|containsKey
argument_list|(
name|strippedKey
argument_list|)
condition|)
block|{
name|long
name|bestCandidateTs
init|=
name|candidateKeys
operator|.
name|get
argument_list|(
name|strippedKey
argument_list|)
operator|.
name|longValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|bestCandidateTs
operator|<
name|finalKey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
operator|new
name|Long
argument_list|(
name|finalKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// otherwise, this is a new key, so put it up as a candidate
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
operator|new
name|Long
argument_list|(
name|finalKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
comment|// seek to the exact row, or the one that would be immediately before it
name|readkey
operator|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|searchKey
argument_list|,
name|readval
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// didn't find anything that would match, so return
return|return;
block|}
do|do
block|{
comment|// if we have an exact match on row, and it's not a delete, save this
comment|// as a candidate key
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|,
name|row
argument_list|)
condition|)
block|{
name|strippedKey
operator|=
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
operator|new
name|Long
argument_list|(
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"rowAtOrBeforeFromMapFile: "
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|// if the candidate keys contain any that might match by timestamp,
comment|// then check for a match and remove it if it's too young to
comment|// survive the delete
if|if
condition|(
name|candidateKeys
operator|.
name|containsKey
argument_list|(
name|strippedKey
argument_list|)
condition|)
block|{
name|long
name|bestCandidateTs
init|=
name|candidateKeys
operator|.
name|get
argument_list|(
name|strippedKey
argument_list|)
operator|.
name|longValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|bestCandidateTs
operator|<=
name|readkey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|candidateKeys
operator|.
name|remove
argument_list|(
name|strippedKey
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|Bytes
operator|.
name|compareTo
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|,
name|row
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the row key we just read is beyond the key we're searching for,
comment|// then we're done. return.
return|return;
block|}
else|else
block|{
name|strippedKey
operator|=
name|stripTimestamp
argument_list|(
name|readkey
argument_list|)
expr_stmt|;
comment|// so, the row key doesn't match, but we haven't gone past the row
comment|// we're seeking yet, so this row is a candidate for closest
comment|// (assuming that it isn't a delete).
if|if
condition|(
operator|!
name|HLogEdit
operator|.
name|isDeleted
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
condition|)
block|{
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|now
operator|<
name|readkey
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
condition|)
block|{
name|candidateKeys
operator|.
name|put
argument_list|(
name|strippedKey
argument_list|,
name|readkey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"rowAtOrBeforeFromMapFile: "
operator|+
name|readkey
operator|+
literal|": expired, skipped"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|// if the candidate keys contain any that might match by timestamp,
comment|// then check for a match and remove it if it's too young to
comment|// survive the delete
if|if
condition|(
name|candidateKeys
operator|.
name|containsKey
argument_list|(
name|strippedKey
argument_list|)
condition|)
block|{
name|long
name|bestCandidateTs
init|=
name|candidateKeys
operator|.
name|get
argument_list|(
name|strippedKey
argument_list|)
operator|.
name|longValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|bestCandidateTs
operator|<=
name|readkey
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|candidateKeys
operator|.
name|remove
argument_list|(
name|strippedKey
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
block|}
specifier|static
name|HStoreKey
name|stripTimestamp
parameter_list|(
name|HStoreKey
name|key
parameter_list|)
block|{
return|return
operator|new
name|HStoreKey
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|key
operator|.
name|getColumn
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Test that the<i>target</i> matches the<i>origin</i>. If the     *<i>origin</i> has an empty column, then it's assumed to mean any column     * matches and only match on row and timestamp. Otherwise, it compares the    * keys with HStoreKey.matchesRowCol().    * @param origin The key we're testing against    * @param target The key we're testing    */
specifier|private
name|boolean
name|cellMatches
parameter_list|(
name|HStoreKey
name|origin
parameter_list|,
name|HStoreKey
name|target
parameter_list|)
block|{
comment|// if the origin's column is empty, then we're matching any column
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|origin
operator|.
name|getColumn
argument_list|()
argument_list|,
name|HConstants
operator|.
name|EMPTY_BYTE_ARRAY
argument_list|)
condition|)
block|{
comment|// if the row matches, then...
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|target
operator|.
name|getRow
argument_list|()
argument_list|,
name|origin
operator|.
name|getRow
argument_list|()
argument_list|)
condition|)
block|{
comment|// check the timestamp
return|return
name|target
operator|.
name|getTimestamp
argument_list|()
operator|<=
name|origin
operator|.
name|getTimestamp
argument_list|()
return|;
block|}
return|return
literal|false
return|;
block|}
comment|// otherwise, we want to match on row and column
return|return
name|target
operator|.
name|matchesRowCol
argument_list|(
name|origin
argument_list|)
return|;
block|}
comment|/**    * Test that the<i>target</i> matches the<i>origin</i>. If the<i>origin</i>    * has an empty column, then it just tests row equivalence. Otherwise, it uses    * HStoreKey.matchesRowCol().    * @param origin Key we're testing against    * @param target Key we're testing    */
specifier|private
name|boolean
name|rowMatches
parameter_list|(
name|HStoreKey
name|origin
parameter_list|,
name|HStoreKey
name|target
parameter_list|)
block|{
comment|// if the origin's column is empty, then we're matching any column
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|origin
operator|.
name|getColumn
argument_list|()
argument_list|,
name|HConstants
operator|.
name|EMPTY_BYTE_ARRAY
argument_list|)
condition|)
block|{
comment|// if the row matches, then...
return|return
name|Bytes
operator|.
name|equals
argument_list|(
name|target
operator|.
name|getRow
argument_list|()
argument_list|,
name|origin
operator|.
name|getRow
argument_list|()
argument_list|)
return|;
block|}
comment|// otherwise, we want to match on row and column
return|return
name|target
operator|.
name|matchesRowCol
argument_list|(
name|origin
argument_list|)
return|;
block|}
comment|/**    * Determines if HStore can be split    *     * @return a StoreSize if store can be split, null otherwise    */
name|StoreSize
name|checkSplit
parameter_list|()
block|{
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
name|storeSize
operator|<
name|this
operator|.
name|desiredMaxFileSize
condition|)
block|{
return|return
literal|null
return|;
block|}
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Not splitable if we find a reference store file present in the store.
name|boolean
name|splitable
init|=
literal|true
decl_stmt|;
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|Long
name|mapIndex
init|=
name|Long
operator|.
name|valueOf
argument_list|(
literal|0L
argument_list|)
decl_stmt|;
comment|// Iterate through all the MapFiles
synchronized|synchronized
init|(
name|storefiles
init|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreFile
name|curHSF
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|long
name|size
init|=
name|curHSF
operator|.
name|length
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|mapIndex
operator|=
name|e
operator|.
name|getKey
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|splitable
condition|)
block|{
name|splitable
operator|=
operator|!
name|curHSF
operator|.
name|isReference
argument_list|()
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
operator|!
name|splitable
condition|)
block|{
return|return
literal|null
return|;
block|}
name|MapFile
operator|.
name|Reader
name|r
init|=
name|this
operator|.
name|readers
operator|.
name|get
argument_list|(
name|mapIndex
argument_list|)
decl_stmt|;
comment|// seek back to the beginning of mapfile
name|r
operator|.
name|reset
argument_list|()
expr_stmt|;
comment|// get the first and last keys
name|HStoreKey
name|firstKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|HStoreKey
name|lastKey
init|=
operator|new
name|HStoreKey
argument_list|()
decl_stmt|;
name|Writable
name|value
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|r
operator|.
name|next
argument_list|(
name|firstKey
argument_list|,
name|value
argument_list|)
expr_stmt|;
name|r
operator|.
name|finalKey
argument_list|(
name|lastKey
argument_list|)
expr_stmt|;
comment|// get the midkey
name|HStoreKey
name|mk
init|=
operator|(
name|HStoreKey
operator|)
name|r
operator|.
name|midKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|mk
operator|!=
literal|null
condition|)
block|{
comment|// if the midkey is the same as the first and last keys, then we cannot
comment|// (ever) split this region.
if|if
condition|(
name|Bytes
operator|.
name|equals
argument_list|(
name|mk
operator|.
name|getRow
argument_list|()
argument_list|,
name|firstKey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|&&
name|Bytes
operator|.
name|equals
argument_list|(
name|mk
operator|.
name|getRow
argument_list|()
argument_list|,
name|lastKey
operator|.
name|getRow
argument_list|()
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
operator|new
name|StoreSize
argument_list|(
name|maxSize
argument_list|,
name|mk
operator|.
name|getRow
argument_list|()
argument_list|)
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed getting store size for "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** @return aggregate size of HStore */
specifier|public
name|long
name|getSize
parameter_list|()
block|{
return|return
name|storeSize
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return a scanner for both the memcache and the HStore files    */
specifier|protected
name|InternalScanner
name|getScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|byte
index|[]
index|[]
name|targetCols
parameter_list|,
name|byte
index|[]
name|firstRow
parameter_list|,
name|RowFilterInterface
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|HStoreScanner
argument_list|(
name|this
argument_list|,
name|targetCols
argument_list|,
name|firstRow
argument_list|,
name|timestamp
argument_list|,
name|filter
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/** {@inheritDoc} */
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|this
operator|.
name|storeNameStr
return|;
block|}
comment|/**    * @param p Path to check.    * @return True if the path has format of a HStoreFile reference.    */
specifier|public
specifier|static
name|boolean
name|isReference
parameter_list|(
specifier|final
name|Path
name|p
parameter_list|)
block|{
return|return
name|isReference
argument_list|(
name|p
argument_list|,
name|REF_NAME_PARSER
operator|.
name|matcher
argument_list|(
name|p
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isReference
parameter_list|(
specifier|final
name|Path
name|p
parameter_list|,
specifier|final
name|Matcher
name|m
parameter_list|)
block|{
if|if
condition|(
name|m
operator|==
literal|null
operator|||
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed match of store file name "
operator|+
name|p
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Failed match of store file name "
operator|+
name|p
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
return|return
name|m
operator|.
name|groupCount
argument_list|()
operator|>
literal|1
operator|&&
name|m
operator|.
name|group
argument_list|(
literal|2
argument_list|)
operator|!=
literal|null
return|;
block|}
comment|/**    * @return Current list of store files.    */
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|getStorefiles
parameter_list|()
block|{
synchronized|synchronized
init|(
name|this
operator|.
name|storefiles
init|)
block|{
name|SortedMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|copy
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
argument_list|)
decl_stmt|;
return|return
name|copy
return|;
block|}
block|}
class|class
name|StoreSize
block|{
specifier|private
specifier|final
name|long
name|size
decl_stmt|;
specifier|private
specifier|final
name|byte
index|[]
name|key
decl_stmt|;
name|StoreSize
parameter_list|(
name|long
name|size
parameter_list|,
name|byte
index|[]
name|key
parameter_list|)
block|{
name|this
operator|.
name|size
operator|=
name|size
expr_stmt|;
name|this
operator|.
name|key
operator|=
operator|new
name|byte
index|[
name|key
operator|.
name|length
index|]
expr_stmt|;
name|System
operator|.
name|arraycopy
argument_list|(
name|key
argument_list|,
literal|0
argument_list|,
name|this
operator|.
name|key
argument_list|,
literal|0
argument_list|,
name|key
operator|.
name|length
argument_list|)
expr_stmt|;
block|}
comment|/* @return the size */
name|long
name|getSize
parameter_list|()
block|{
return|return
name|size
return|;
block|}
comment|/* @return the key */
name|byte
index|[]
name|getKey
parameter_list|()
block|{
return|return
name|key
return|;
block|}
block|}
block|}
end_class

end_unit

