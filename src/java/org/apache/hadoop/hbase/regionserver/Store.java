begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2009 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|EOFException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentSkipListMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentSkipListSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CopyOnWriteArraySet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|locks
operator|.
name|ReentrantReadWriteLock
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Pattern
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HBaseConfiguration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HRegionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|RemoteExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|filter
operator|.
name|RowFilterInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|Compression
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFileScanner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|HRegion
operator|.
name|Counter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|FSUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Progressable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_comment
comment|/**   * A Store holds a column family in a Region.  Its a memcache and a set of zero   * or more StoreFiles, which stretch backwards over time.   *   *<p>There's no reason to consider append-logging at this level; all logging    * and locking is handled at the HRegion level.  Store just provides   * services to manage sets of StoreFiles.  One of the most important of those   * services is compaction services where files are aggregated once they pass   * a configurable threshold.   *   *<p>The only thing having to do with logs that Store needs to deal with is   * the reconstructionLog.  This is a segment of an HRegion's log that might   * NOT be present upon startup.  If the param is NULL, there's nothing to do.   * If the param is non-NULL, we need to process the log to reconstruct   * a TreeMap that might not have been written to disk before the process   * died.   *   *<p>It's assumed that after this constructor returns, the reconstructionLog   * file will be deleted (by whoever has instantiated the Store).  *  *<p>Locking and transactions are handled at a higher level.  This API should  * not be called directly but by an HRegion manager.  */
end_comment

begin_class
specifier|public
class|class
name|Store
implements|implements
name|HConstants
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|Store
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/**    * Comparator that looks at columns and compares their family portions.    * Presumes columns have already been checked for presence of delimiter.    * If no delimiter present, presume the buffer holds a store name so no need    * of a delimiter.    */
specifier|protected
specifier|final
name|Memcache
name|memcache
decl_stmt|;
comment|// This stores directory in the filesystem.
specifier|private
specifier|final
name|Path
name|homedir
decl_stmt|;
specifier|private
specifier|final
name|HRegionInfo
name|regioninfo
decl_stmt|;
specifier|private
specifier|final
name|HColumnDescriptor
name|family
decl_stmt|;
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|HBaseConfiguration
name|conf
decl_stmt|;
comment|// ttl in milliseconds.
specifier|protected
name|long
name|ttl
decl_stmt|;
specifier|private
name|long
name|majorCompactionTime
decl_stmt|;
specifier|private
name|int
name|maxFilesToCompact
decl_stmt|;
specifier|private
specifier|final
name|long
name|desiredMaxFileSize
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|storeSize
init|=
literal|0L
decl_stmt|;
specifier|private
specifier|final
name|Object
name|flushLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|final
name|ReentrantReadWriteLock
name|lock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
specifier|final
name|byte
index|[]
name|storeName
decl_stmt|;
specifier|private
specifier|final
name|String
name|storeNameStr
decl_stmt|;
comment|/*    * Sorted Map of readers keyed by maximum edit sequence id (Most recent should    * be last in in list).  ConcurrentSkipListMap is lazily consistent so no    * need to lock it down when iterating; iterator view is that of when the    * iterator was taken out.    */
specifier|private
specifier|final
name|NavigableMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|storefiles
init|=
operator|new
name|ConcurrentSkipListMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
comment|// All access must be synchronized.
specifier|private
specifier|final
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
name|changedReaderObservers
init|=
operator|new
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
argument_list|()
decl_stmt|;
comment|// The most-recent log-seq-ID.  The most-recent such ID means we can ignore
comment|// all log messages up to and including that ID (because they're already
comment|// reflected in the TreeMaps).
specifier|private
specifier|volatile
name|long
name|maxSeqId
init|=
operator|-
literal|1
decl_stmt|;
specifier|private
specifier|final
name|Path
name|compactionDir
decl_stmt|;
specifier|private
specifier|final
name|Object
name|compactLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|int
name|compactionThreshold
decl_stmt|;
specifier|private
specifier|final
name|int
name|blocksize
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|bloomfilter
decl_stmt|;
specifier|private
specifier|final
name|Compression
operator|.
name|Algorithm
name|compression
decl_stmt|;
comment|// Comparing KeyValues
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|comparator
decl_stmt|;
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|comparatorIgnoringType
decl_stmt|;
comment|/**    * Constructor    * @param basedir qualified path under which the region directory lives;    * generally the table subdirectory    * @param info HRegionInfo for this region    * @param family HColumnDescriptor for this column    * @param fs file system object    * @param reconstructionLog existing log file to apply if any    * @param conf configuration object    * @param reporter Call on a period so hosting server can report we're    * making progress to master -- otherwise master might think region deploy    * failed.  Can be null.    * @throws IOException    */
specifier|protected
name|Store
parameter_list|(
name|Path
name|basedir
parameter_list|,
name|HRegionInfo
name|info
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|reconstructionLog
parameter_list|,
name|HBaseConfiguration
name|conf
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|homedir
operator|=
name|getStoreHomedir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|regioninfo
operator|=
name|info
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|bloomfilter
operator|=
name|family
operator|.
name|isBloomfilter
argument_list|()
expr_stmt|;
name|this
operator|.
name|blocksize
operator|=
name|family
operator|.
name|getBlocksize
argument_list|()
expr_stmt|;
name|this
operator|.
name|compression
operator|=
name|family
operator|.
name|getCompression
argument_list|()
expr_stmt|;
name|this
operator|.
name|comparator
operator|=
name|info
operator|.
name|getComparator
argument_list|()
expr_stmt|;
name|this
operator|.
name|comparatorIgnoringType
operator|=
name|this
operator|.
name|comparator
operator|.
name|getComparatorIgnoringType
argument_list|()
expr_stmt|;
comment|// getTimeToLive returns ttl in seconds.  Convert to milliseconds.
name|this
operator|.
name|ttl
operator|=
name|family
operator|.
name|getTimeToLive
argument_list|()
expr_stmt|;
if|if
condition|(
name|ttl
operator|!=
name|HConstants
operator|.
name|FOREVER
condition|)
block|{
name|this
operator|.
name|ttl
operator|*=
literal|1000
expr_stmt|;
block|}
name|this
operator|.
name|memcache
operator|=
operator|new
name|Memcache
argument_list|(
name|this
operator|.
name|ttl
argument_list|,
name|this
operator|.
name|comparator
argument_list|)
expr_stmt|;
name|this
operator|.
name|compactionDir
operator|=
name|HRegion
operator|.
name|getCompactionDir
argument_list|(
name|basedir
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeName
operator|=
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
expr_stmt|;
name|this
operator|.
name|storeNameStr
operator|=
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
comment|// By default, we compact if an HStore has more than
comment|// MIN_COMMITS_FOR_COMPACTION map files
name|this
operator|.
name|compactionThreshold
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
expr_stmt|;
comment|// By default we split region if a file> DEFAULT_MAX_FILE_SIZE.
name|long
name|maxFileSize
init|=
name|info
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getMaxFileSize
argument_list|()
decl_stmt|;
if|if
condition|(
name|maxFileSize
operator|==
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
condition|)
block|{
name|maxFileSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hregion.max.filesize"
argument_list|,
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|desiredMaxFileSize
operator|=
name|maxFileSize
expr_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|,
literal|86400000
argument_list|)
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|String
name|strCompactionTime
init|=
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
decl_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
operator|(
operator|new
name|Long
argument_list|(
name|strCompactionTime
argument_list|)
operator|)
operator|.
name|longValue
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|maxFilesToCompact
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.max"
argument_list|,
literal|10
argument_list|)
expr_stmt|;
comment|// loadStoreFiles calculates this.maxSeqId. as side-effect.
name|this
operator|.
name|storefiles
operator|.
name|putAll
argument_list|(
name|loadStoreFiles
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
operator|&&
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Loaded "
operator|+
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in Store "
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
operator|+
literal|", max sequence id "
operator|+
name|this
operator|.
name|maxSeqId
argument_list|)
expr_stmt|;
block|}
comment|// Do reconstruction log.
name|runReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|this
operator|.
name|maxSeqId
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
block|}
name|HColumnDescriptor
name|getFamily
parameter_list|()
block|{
return|return
name|this
operator|.
name|family
return|;
block|}
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|this
operator|.
name|maxSeqId
return|;
block|}
comment|/**    * @param tabledir    * @param encodedName Encoded region name.    * @param family    * @return Path to family/Store home directory.    */
specifier|public
specifier|static
name|Path
name|getStoreHomedir
parameter_list|(
specifier|final
name|Path
name|tabledir
parameter_list|,
specifier|final
name|int
name|encodedName
parameter_list|,
specifier|final
name|byte
index|[]
name|family
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|tabledir
argument_list|,
operator|new
name|Path
argument_list|(
name|Integer
operator|.
name|toString
argument_list|(
name|encodedName
argument_list|)
argument_list|,
operator|new
name|Path
argument_list|(
name|Bytes
operator|.
name|toString
argument_list|(
name|family
argument_list|)
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
comment|/*    * Run reconstruction log    * @param reconstructionLog    * @param msid    * @param reporter    * @throws IOException    */
specifier|private
name|void
name|runReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|msid
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
name|doReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|msid
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|EOFException
name|e
parameter_list|)
block|{
comment|// Presume we got here because of lack of HADOOP-1700; for now keep going
comment|// but this is probably not what we want long term.  If we got here there
comment|// has been data-loss
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception processing reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" opening "
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
operator|+
literal|" -- continuing.  Probably lack-of-HADOOP-1700 causing DATA LOSS!"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// Presume we got here because of some HDFS issue. Don't just keep going.
comment|// Fail to open the HStore.  Probably means we'll fail over and over
comment|// again until human intervention but alternative has us skipping logs
comment|// and losing edits: HBASE-642.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception processing reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" opening "
operator|+
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|storeName
argument_list|)
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
block|}
comment|/*    * Read the reconstructionLog to see whether we need to build a brand-new     * file out of non-flushed log entries.      *    * We can ignore any log message that has a sequence ID that's equal to or     * lower than maxSeqID.  (Because we know such log messages are already     * reflected in the MapFiles.)    */
specifier|private
name|void
name|doReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|maxSeqID
parameter_list|,
specifier|final
name|Progressable
name|reporter
parameter_list|)
throws|throws
name|UnsupportedEncodingException
throws|,
name|IOException
block|{
if|if
condition|(
name|reconstructionLog
operator|==
literal|null
operator|||
operator|!
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
condition|)
block|{
comment|// Nothing to do.
return|return;
block|}
comment|// Check its not empty.
name|FileStatus
index|[]
name|stats
init|=
name|this
operator|.
name|fs
operator|.
name|listStatus
argument_list|(
name|reconstructionLog
argument_list|)
decl_stmt|;
if|if
condition|(
name|stats
operator|==
literal|null
operator|||
name|stats
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Passed reconstruction log "
operator|+
name|reconstructionLog
operator|+
literal|" is zero-length"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// TODO: This could grow large and blow heap out.  Need to get it into
comment|// general memory usage accounting.
name|long
name|maxSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|ConcurrentSkipListSet
argument_list|<
name|KeyValue
argument_list|>
name|reconstructedCache
init|=
name|Memcache
operator|.
name|createSet
argument_list|(
name|this
operator|.
name|comparator
argument_list|)
decl_stmt|;
name|SequenceFile
operator|.
name|Reader
name|logReader
init|=
operator|new
name|SequenceFile
operator|.
name|Reader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|reconstructionLog
argument_list|,
name|this
operator|.
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|HLogKey
name|key
init|=
operator|new
name|HLogKey
argument_list|()
decl_stmt|;
name|HLogEdit
name|val
init|=
operator|new
name|HLogEdit
argument_list|()
decl_stmt|;
name|long
name|skippedEdits
init|=
literal|0
decl_stmt|;
name|long
name|editsCount
init|=
literal|0
decl_stmt|;
comment|// How many edits to apply before we send a progress report.
name|int
name|reportInterval
init|=
name|this
operator|.
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.report.interval.edits"
argument_list|,
literal|2000
argument_list|)
decl_stmt|;
while|while
condition|(
name|logReader
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
condition|)
block|{
name|maxSeqIdInLog
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSeqIdInLog
argument_list|,
name|key
operator|.
name|getLogSeqNum
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|<=
name|maxSeqID
condition|)
block|{
name|skippedEdits
operator|++
expr_stmt|;
continue|continue;
block|}
comment|// Check this edit is for me. Also, guard against writing the speical
comment|// METACOLUMN info such as HBASE::CACHEFLUSH entries
name|KeyValue
name|kv
init|=
name|val
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|val
operator|.
name|isTransactionEntry
argument_list|()
operator|||
name|kv
operator|.
name|matchingColumnNoDelimiter
argument_list|(
name|HLog
operator|.
name|METACOLUMN
argument_list|)
operator|||
operator|!
name|Bytes
operator|.
name|equals
argument_list|(
name|key
operator|.
name|getRegionName
argument_list|()
argument_list|,
name|regioninfo
operator|.
name|getRegionName
argument_list|()
argument_list|)
operator|||
operator|!
name|kv
operator|.
name|matchingFamily
argument_list|(
name|family
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|reconstructedCache
operator|.
name|add
argument_list|(
name|kv
argument_list|)
expr_stmt|;
name|editsCount
operator|++
expr_stmt|;
comment|// Every 2k edits, tell the reporter we're making progress.
comment|// Have seen 60k edits taking 3minutes to complete.
if|if
condition|(
name|reporter
operator|!=
literal|null
operator|&&
operator|(
name|editsCount
operator|%
name|reportInterval
operator|)
operator|==
literal|0
condition|)
block|{
name|reporter
operator|.
name|progress
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Applied "
operator|+
name|editsCount
operator|+
literal|", skipped "
operator|+
name|skippedEdits
operator|+
literal|" because sequence id<= "
operator|+
name|maxSeqID
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|logReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|reconstructedCache
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// We create a "virtual flush" at maxSeqIdInLog+1.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing reconstructionCache"
argument_list|)
expr_stmt|;
block|}
name|internalFlushCache
argument_list|(
name|reconstructedCache
argument_list|,
name|maxSeqIdInLog
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
block|}
comment|/*    * Creates a series of StoreFile loaded from the given directory.    * @throws IOException    */
specifier|private
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|loadStoreFiles
parameter_list|()
throws|throws
name|IOException
block|{
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|results
init|=
operator|new
name|HashMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|FileStatus
name|files
index|[]
init|=
name|this
operator|.
name|fs
operator|.
name|listStatus
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|files
operator|!=
literal|null
operator|&&
name|i
operator|<
name|files
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
comment|// Skip directories.
if|if
condition|(
name|files
index|[
name|i
index|]
operator|.
name|isDir
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|Path
name|p
init|=
name|files
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Check for empty file.  Should never be the case but can happen
comment|// after data loss in hdfs for whatever reason (upgrade, etc.): HBASE-646
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|p
argument_list|)
operator|.
name|getLen
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Skipping "
operator|+
name|p
operator|+
literal|" because its empty. HBASE-646 DATA LOSS?"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|StoreFile
name|curfile
init|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|p
argument_list|)
decl_stmt|;
name|long
name|storeSeqId
init|=
name|curfile
operator|.
name|getMaxSequenceId
argument_list|()
decl_stmt|;
if|if
condition|(
name|storeSeqId
operator|>
name|this
operator|.
name|maxSeqId
condition|)
block|{
name|this
operator|.
name|maxSeqId
operator|=
name|storeSeqId
expr_stmt|;
block|}
name|long
name|length
init|=
name|curfile
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|length
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|p
argument_list|)
operator|+
literal|", isReference="
operator|+
name|curfile
operator|.
name|isReference
argument_list|()
operator|+
literal|", sequence id="
operator|+
name|storeSeqId
operator|+
literal|", length="
operator|+
name|length
operator|+
literal|", majorCompaction="
operator|+
name|curfile
operator|.
name|isMajorCompaction
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|results
operator|.
name|put
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|storeSeqId
argument_list|)
argument_list|,
name|curfile
argument_list|)
expr_stmt|;
block|}
return|return
name|results
return|;
block|}
comment|/**    * Adds a value to the memcache    *     * @param kv    * @return memcache size delta    */
specifier|protected
name|long
name|add
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memcache
operator|.
name|add
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return All store files.    */
name|NavigableMap
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|getStorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
return|;
block|}
comment|/**    * Close all the readers    *     * We don't need to worry about subsequent requests because the HRegion holds    * a write lock that will prevent any more reads or writes.    *     * @throws IOException    */
name|List
argument_list|<
name|StoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
decl_stmt|;
comment|// Clear so metrics doesn't find them.
name|this
operator|.
name|storefiles
operator|.
name|clear
argument_list|()
expr_stmt|;
for|for
control|(
name|StoreFile
name|f
range|:
name|result
control|)
block|{
name|f
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"closed "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Snapshot this stores memcache.  Call before running    * {@link #flushCache(long)} so it has some work to do.    */
name|void
name|snapshot
parameter_list|()
block|{
name|this
operator|.
name|memcache
operator|.
name|snapshot
argument_list|()
expr_stmt|;
block|}
comment|/**    * Write out current snapshot.  Presumes {@link #snapshot()} has been called    * previously.    * @param logCacheFlushId flush sequence number    * @return true if a compaction is needed    * @throws IOException    */
name|boolean
name|flushCache
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Get the snapshot to flush.  Presumes that a call to
comment|// this.memcache.snapshot() has happened earlier up in the chain.
name|ConcurrentSkipListSet
argument_list|<
name|KeyValue
argument_list|>
name|cache
init|=
name|this
operator|.
name|memcache
operator|.
name|getSnapshot
argument_list|()
decl_stmt|;
comment|// If an exception happens flushing, we let it out without clearing
comment|// the memcache snapshot.  The old snapshot will be returned when we say
comment|// 'snapshot', the next time flush comes around.
name|StoreFile
name|sf
init|=
name|internalFlushCache
argument_list|(
name|cache
argument_list|,
name|logCacheFlushId
argument_list|)
decl_stmt|;
if|if
condition|(
name|sf
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Add new file to store files.  Clear snapshot too while we have the
comment|// Store write lock.
name|int
name|size
init|=
name|updateStorefiles
argument_list|(
name|logCacheFlushId
argument_list|,
name|sf
argument_list|,
name|cache
argument_list|)
decl_stmt|;
return|return
name|size
operator|>=
name|this
operator|.
name|compactionThreshold
return|;
block|}
comment|/*    * @param cache    * @param logCacheFlushId    * @return StoreFile created.    * @throws IOException    */
specifier|private
name|StoreFile
name|internalFlushCache
parameter_list|(
specifier|final
name|ConcurrentSkipListSet
argument_list|<
name|KeyValue
argument_list|>
name|cache
parameter_list|,
specifier|final
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
name|HFile
operator|.
name|Writer
name|writer
init|=
literal|null
decl_stmt|;
name|long
name|flushed
init|=
literal|0
decl_stmt|;
comment|// Don't flush if there are no entries.
if|if
condition|(
name|cache
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
comment|// TODO:  We can fail in the below block before we complete adding this
comment|// flush to list of store files.  Add cleanup of anything put on filesystem
comment|// if we fail.
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
comment|// A. Write the map out to the disk
name|writer
operator|=
name|getWriter
argument_list|()
expr_stmt|;
name|int
name|entries
init|=
literal|0
decl_stmt|;
try|try
block|{
for|for
control|(
name|KeyValue
name|kv
range|:
name|cache
control|)
block|{
if|if
condition|(
operator|!
name|isExpired
argument_list|(
name|kv
argument_list|,
name|ttl
argument_list|,
name|now
argument_list|)
condition|)
block|{
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
name|entries
operator|++
expr_stmt|;
name|flushed
operator|+=
name|this
operator|.
name|memcache
operator|.
name|heapSize
argument_list|(
name|kv
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
comment|// B. Write out the log sequence number that corresponds to this output
comment|// MapFile.  The MapFile is current up to and including logCacheFlushId.
name|StoreFile
operator|.
name|appendMetadata
argument_list|(
name|writer
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|writer
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Added "
operator|+
name|sf
operator|+
literal|", entries="
operator|+
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|getEntries
argument_list|()
operator|+
literal|", sequenceid="
operator|+
name|logCacheFlushId
operator|+
literal|", memsize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|flushed
argument_list|)
operator|+
literal|", filesize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
argument_list|)
operator|+
literal|" to "
operator|+
name|this
operator|.
name|regioninfo
operator|.
name|getRegionNameAsString
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|sf
return|;
block|}
comment|/**    * @return Writer for this store.    * @throws IOException    */
name|HFile
operator|.
name|Writer
name|getWriter
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|getWriter
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
return|;
block|}
comment|/*    * @return Writer for this store.    * @param basedir Directory to put writer in.    * @throws IOException    */
specifier|private
name|HFile
operator|.
name|Writer
name|getWriter
parameter_list|(
specifier|final
name|Path
name|basedir
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|StoreFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|basedir
argument_list|,
name|this
operator|.
name|blocksize
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|comparator
operator|.
name|getRawComparator
argument_list|()
argument_list|,
name|this
operator|.
name|bloomfilter
argument_list|)
return|;
block|}
comment|/*    * Change storefiles adding into place the Reader produced by this new flush.    * @param logCacheFlushId    * @param sf    * @param cache That was used to make the passed file<code>p</code>.    * @throws IOException    * @return Count of store files.    */
specifier|private
name|int
name|updateStorefiles
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
specifier|final
name|StoreFile
name|sf
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|cache
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|count
init|=
literal|0
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|logCacheFlushId
argument_list|)
argument_list|,
name|sf
argument_list|)
expr_stmt|;
name|count
operator|=
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
expr_stmt|;
comment|// Tell listeners of the change in readers.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
name|this
operator|.
name|memcache
operator|.
name|clearSnapshot
argument_list|(
name|cache
argument_list|)
expr_stmt|;
return|return
name|count
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Notify all observers that set of Readers has changed.    * @throws IOException    */
specifier|private
name|void
name|notifyChangedReadersObservers
parameter_list|()
throws|throws
name|IOException
block|{
for|for
control|(
name|ChangedReadersObserver
name|o
range|:
name|this
operator|.
name|changedReaderObservers
control|)
block|{
name|o
operator|.
name|updateReaders
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * @param o Observer who wants to know about changes in set of Readers    */
name|void
name|addChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
name|this
operator|.
name|changedReaderObservers
operator|.
name|add
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|/*    * @param o Observer no longer interested in changes in set of Readers.    */
name|void
name|deleteChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|changedReaderObservers
operator|.
name|remove
argument_list|(
name|o
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Not in set"
operator|+
name|o
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the StoreFiles.  This method may take some time, so the calling     * thread must be able to block for long periods.    *     *<p>During this time, the Store can work as usual, getting values from    * MapFiles and writing new MapFiles from the Memcache.    *     * Existing MapFiles are not destroyed until the new compacted TreeMap is     * completely written-out to disk.    *    * The compactLock prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *     * We don't want to hold the structureLock for the whole time, as a compact()     * can be lengthy and we want to allow cache-flushes during this period.    *     * @param mc True to force a major compaction regardless of    * thresholds    * @return row to split around if a split is needed, null otherwise    * @throws IOException    */
name|StoreSize
name|compact
parameter_list|(
specifier|final
name|boolean
name|mc
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|forceSplit
init|=
name|this
operator|.
name|regioninfo
operator|.
name|shouldSplit
argument_list|(
literal|false
argument_list|)
decl_stmt|;
name|boolean
name|majorcompaction
init|=
name|mc
decl_stmt|;
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
name|long
name|maxId
init|=
operator|-
literal|1
decl_stmt|;
comment|// filesToCompact are sorted oldest to newest.
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
literal|null
decl_stmt|;
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|this
operator|.
name|storeNameStr
operator|+
literal|": no store files to compact"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// The max-sequenceID in any of the to-be-compacted TreeMaps is the
comment|// last key of storefiles.
name|maxId
operator|=
name|this
operator|.
name|storefiles
operator|.
name|lastKey
argument_list|()
operator|.
name|longValue
argument_list|()
expr_stmt|;
comment|// Check to see if we need to do a major compaction on this region.
comment|// If so, change doMajorCompaction to true to skip the incremental
comment|// compacting below. Only check if doMajorCompaction is not true.
if|if
condition|(
operator|!
name|majorcompaction
condition|)
block|{
name|majorcompaction
operator|=
name|isMajorCompaction
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
name|boolean
name|references
init|=
name|hasReferences
argument_list|(
name|filesToCompact
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|references
operator|&&
operator|(
name|forceSplit
operator|||
operator|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|compactionThreshold
operator|)
operator|)
condition|)
block|{
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|this
operator|.
name|compactionDir
argument_list|)
operator|&&
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|this
operator|.
name|compactionDir
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mkdir on "
operator|+
name|this
operator|.
name|compactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" failed"
argument_list|)
expr_stmt|;
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
comment|// HBASE-745, preparing all store file sizes for incremental compacting
comment|// selection.
name|int
name|countOfFiles
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
name|totalSize
init|=
literal|0
decl_stmt|;
name|long
index|[]
name|fileSizes
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
name|long
name|skipped
init|=
literal|0
decl_stmt|;
name|int
name|point
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|countOfFiles
condition|;
name|i
operator|++
control|)
block|{
name|StoreFile
name|file
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|Path
name|path
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Path is null for "
operator|+
name|file
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|long
name|len
init|=
name|file
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|fileSizes
index|[
name|i
index|]
operator|=
name|len
expr_stmt|;
name|totalSize
operator|+=
name|len
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|references
condition|)
block|{
comment|// Here we select files for incremental compaction.
comment|// The rule is: if the largest(oldest) one is more than twice the
comment|// size of the second, skip the largest, and continue to next...,
comment|// until we meet the compactionThreshold limit.
for|for
control|(
name|point
operator|=
literal|0
init|;
name|point
operator|<
name|countOfFiles
operator|-
literal|1
condition|;
name|point
operator|++
control|)
block|{
if|if
condition|(
operator|(
name|fileSizes
index|[
name|point
index|]
operator|<
name|fileSizes
index|[
name|point
operator|+
literal|1
index|]
operator|*
literal|2
operator|)
operator|&&
operator|(
name|countOfFiles
operator|-
name|point
operator|)
operator|<=
name|maxFilesToCompact
condition|)
block|{
break|break;
block|}
name|skipped
operator|+=
name|fileSizes
index|[
name|point
index|]
expr_stmt|;
block|}
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|filesToCompact
operator|.
name|subList
argument_list|(
name|point
argument_list|,
name|countOfFiles
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<=
literal|1
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipped compaction of 1 file; compaction size of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|": "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|"; Skipped "
operator|+
name|point
operator|+
literal|" files, size: "
operator|+
name|skipped
argument_list|)
expr_stmt|;
block|}
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Compaction size of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|": "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|"; Skipped "
operator|+
name|point
operator|+
literal|" file(s), size: "
operator|+
name|skipped
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Step through them, writing to the brand-new file
name|HFile
operator|.
name|Writer
name|writer
init|=
name|getWriter
argument_list|(
name|this
operator|.
name|compactionDir
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Started compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s)"
operator|+
operator|(
name|references
condition|?
literal|", hasReferences=true,"
else|:
literal|" "
operator|)
operator|+
literal|" into "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|writer
operator|.
name|getPath
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|compact
argument_list|(
name|writer
argument_list|,
name|filesToCompact
argument_list|,
name|majorcompaction
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.
name|StoreFile
operator|.
name|appendMetadata
argument_list|(
name|writer
argument_list|,
name|maxId
argument_list|,
name|majorcompaction
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Move the compaction into place.
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|writer
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Completed "
operator|+
operator|(
name|majorcompaction
condition|?
literal|"major"
else|:
literal|""
operator|)
operator|+
literal|" compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" store size is "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|storeSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
comment|/*    * @param files    * @return True if any of the files in<code>files</code> are References.    */
specifier|private
name|boolean
name|hasReferences
parameter_list|(
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|StoreFile
name|hsf
range|:
name|files
control|)
block|{
if|if
condition|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/*    * Gets lowest timestamp from files in a dir    *     * @param fs    * @param dir    * @throws IOException    */
specifier|private
specifier|static
name|long
name|getLowestTimestamp
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
name|FileStatus
index|[]
name|stats
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|)
decl_stmt|;
if|if
condition|(
name|stats
operator|==
literal|null
operator|||
name|stats
operator|.
name|length
operator|==
literal|0
condition|)
block|{
return|return
literal|0l
return|;
block|}
name|long
name|lowTimestamp
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|stats
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|long
name|timestamp
init|=
name|stats
index|[
name|i
index|]
operator|.
name|getModificationTime
argument_list|()
decl_stmt|;
if|if
condition|(
name|timestamp
operator|<
name|lowTimestamp
condition|)
block|{
name|lowTimestamp
operator|=
name|timestamp
expr_stmt|;
block|}
block|}
return|return
name|lowTimestamp
return|;
block|}
comment|/*    * @return True if we should run a major compaction.    */
name|boolean
name|isMajorCompaction
parameter_list|()
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
literal|null
decl_stmt|;
comment|// filesToCompact are sorted oldest to newest.
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|isMajorCompaction
argument_list|(
name|filesToCompact
argument_list|)
return|;
block|}
comment|/*    * @param filesToCompact Files to compact. Can be null.    * @return True if we should run a major compaction.    */
specifier|private
name|boolean
name|isMajorCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|==
literal|null
operator|||
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
name|result
return|;
block|}
name|long
name|lowTimestamp
init|=
name|getLowestTimestamp
argument_list|(
name|fs
argument_list|,
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
argument_list|)
decl_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
if|if
condition|(
name|lowTimestamp
operator|>
literal|0l
operator|&&
name|lowTimestamp
operator|<
operator|(
name|now
operator|-
name|this
operator|.
name|majorCompactionTime
operator|)
condition|)
block|{
comment|// Major compaction time has elapsed.
name|long
name|elapsedTime
init|=
name|now
operator|-
name|lowTimestamp
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
literal|1
operator|&&
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|isMajorCompaction
argument_list|()
operator|&&
operator|(
name|this
operator|.
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|elapsedTime
operator|<
name|this
operator|.
name|ttl
operator|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping major compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" because one (major) compacted file only and elapsedTime "
operator|+
name|elapsedTime
operator|+
literal|"ms is< ttl="
operator|+
name|this
operator|.
name|ttl
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Major compaction triggered on store "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|"; time since last major compaction "
operator|+
operator|(
name|now
operator|-
name|lowTimestamp
operator|)
operator|+
literal|"ms"
argument_list|)
expr_stmt|;
block|}
name|result
operator|=
literal|true
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
comment|/*    * @param r StoreFile list to reverse    * @return A new array of content of<code>readers</code>, reversed.    */
specifier|private
name|StoreFile
index|[]
name|reverse
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|r
parameter_list|)
block|{
name|List
argument_list|<
name|StoreFile
argument_list|>
name|copy
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|r
argument_list|)
decl_stmt|;
name|Collections
operator|.
name|reverse
argument_list|(
name|copy
argument_list|)
expr_stmt|;
comment|// MapFile.Reader is instance of StoreFileReader so this should be ok.
return|return
name|copy
operator|.
name|toArray
argument_list|(
operator|new
name|StoreFile
index|[
literal|0
index|]
argument_list|)
return|;
block|}
comment|/*    * @param rdrs List of StoreFiles    * @param keys Current keys    * @param done Which readers are done    * @return The lowest current key in passed<code>rdrs</code>    */
specifier|private
name|int
name|getLowestKey
parameter_list|(
specifier|final
name|HFileScanner
index|[]
name|rdrs
parameter_list|,
specifier|final
name|KeyValue
index|[]
name|keys
parameter_list|,
specifier|final
name|boolean
index|[]
name|done
parameter_list|)
block|{
name|int
name|lowestKey
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|lowestKey
operator|<
literal|0
condition|)
block|{
name|lowestKey
operator|=
name|i
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compare
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|keys
index|[
name|lowestKey
index|]
argument_list|)
operator|<
literal|0
condition|)
block|{
name|lowestKey
operator|=
name|i
expr_stmt|;
block|}
block|}
block|}
return|return
name|lowestKey
return|;
block|}
comment|/*    * Compact a list of StoreFiles.    *     * We work by iterating through the readers in parallel looking at newest    * store file first. We always increment the lowest-ranked one. Updates to a    * single row/column will appear ranked by timestamp.    * @param compactedOut Where to write compaction.    * @param pReaders List of readers sorted oldest to newest.    * @param majorCompaction True to force a major compaction regardless of    * thresholds    * @throws IOException    */
specifier|private
name|void
name|compact
parameter_list|(
specifier|final
name|HFile
operator|.
name|Writer
name|compactedOut
parameter_list|,
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|pReaders
parameter_list|,
specifier|final
name|boolean
name|majorCompaction
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Reverse order so newest store file is first.
name|StoreFile
index|[]
name|files
init|=
name|reverse
argument_list|(
name|pReaders
argument_list|)
decl_stmt|;
name|HFileScanner
index|[]
name|rdrs
init|=
operator|new
name|HFileScanner
index|[
name|files
operator|.
name|length
index|]
decl_stmt|;
name|KeyValue
index|[]
name|kvs
init|=
operator|new
name|KeyValue
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
name|boolean
index|[]
name|done
init|=
operator|new
name|boolean
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
comment|// Now, advance through the readers in order. This will have the
comment|// effect of a run-time sort of the entire dataset.
name|int
name|numDone
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|rdrs
index|[
name|i
index|]
operator|=
name|files
index|[
name|i
index|]
operator|.
name|getReader
argument_list|()
operator|.
name|getScanner
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
operator|!
name|rdrs
index|[
name|i
index|]
operator|.
name|seekTo
argument_list|()
expr_stmt|;
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
name|numDone
operator|++
expr_stmt|;
block|}
else|else
block|{
name|kvs
index|[
name|i
index|]
operator|=
name|rdrs
index|[
name|i
index|]
operator|.
name|getKeyValue
argument_list|()
expr_stmt|;
block|}
block|}
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|int
name|timesSeen
init|=
literal|0
decl_stmt|;
name|KeyValue
name|lastSeen
init|=
name|KeyValue
operator|.
name|LOWESTKEY
decl_stmt|;
name|KeyValue
name|lastDelete
init|=
literal|null
decl_stmt|;
name|int
name|maxVersions
init|=
name|family
operator|.
name|getMaxVersions
argument_list|()
decl_stmt|;
while|while
condition|(
name|numDone
operator|<
name|done
operator|.
name|length
condition|)
block|{
comment|// Get lowest key in all store files.
name|int
name|lowestKey
init|=
name|getLowestKey
argument_list|(
name|rdrs
argument_list|,
name|kvs
argument_list|,
name|done
argument_list|)
decl_stmt|;
name|KeyValue
name|kv
init|=
name|kvs
index|[
name|lowestKey
index|]
decl_stmt|;
comment|// If its same row and column as last key, increment times seen.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|matchingRowColumn
argument_list|(
name|lastSeen
argument_list|,
name|kv
argument_list|)
condition|)
block|{
name|timesSeen
operator|++
expr_stmt|;
comment|// Reset last delete if not exact timestamp -- lastDelete only stops
comment|// exactly the same key making it out to the compacted store file.
if|if
condition|(
name|lastDelete
operator|!=
literal|null
operator|&&
name|lastDelete
operator|.
name|getTimestamp
argument_list|()
operator|!=
name|kv
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|lastDelete
operator|=
literal|null
expr_stmt|;
block|}
block|}
else|else
block|{
name|timesSeen
operator|=
literal|1
expr_stmt|;
name|lastDelete
operator|=
literal|null
expr_stmt|;
block|}
comment|// Don't write empty rows or columns. Only remove cells on major
comment|// compaction. Remove if expired or> VERSIONS
if|if
condition|(
name|kv
operator|.
name|nonNullRowAndColumn
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|majorCompaction
condition|)
block|{
comment|// Write out all values if not a major compaction.
name|compactedOut
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|boolean
name|expired
init|=
literal|false
decl_stmt|;
name|boolean
name|deleted
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|timesSeen
operator|<=
name|maxVersions
operator|&&
operator|!
operator|(
name|expired
operator|=
name|isExpired
argument_list|(
name|kv
argument_list|,
name|ttl
argument_list|,
name|now
argument_list|)
operator|)
condition|)
block|{
comment|// If this value key is same as a deleted key, skip
if|if
condition|(
name|lastDelete
operator|!=
literal|null
operator|&&
name|this
operator|.
name|comparatorIgnoringType
operator|.
name|compare
argument_list|(
name|kv
argument_list|,
name|lastDelete
argument_list|)
operator|==
literal|0
condition|)
block|{
name|deleted
operator|=
literal|true
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|kv
operator|.
name|isDeleteType
argument_list|()
condition|)
block|{
comment|// If a deleted value, skip
name|deleted
operator|=
literal|true
expr_stmt|;
name|lastDelete
operator|=
name|kv
expr_stmt|;
block|}
else|else
block|{
name|compactedOut
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|expired
operator|||
name|deleted
condition|)
block|{
comment|// HBASE-855 remove one from timesSeen because it did not make it
comment|// past expired check -- don't count against max versions.
name|timesSeen
operator|--
expr_stmt|;
block|}
block|}
block|}
comment|// Update last-seen items
name|lastSeen
operator|=
name|kv
expr_stmt|;
comment|// Advance the smallest key. If that reader's all finished, then
comment|// mark it as done.
if|if
condition|(
operator|!
name|rdrs
index|[
name|lowestKey
index|]
operator|.
name|next
argument_list|()
condition|)
block|{
name|done
index|[
name|lowestKey
index|]
operator|=
literal|true
expr_stmt|;
name|rdrs
index|[
name|lowestKey
index|]
operator|=
literal|null
expr_stmt|;
name|numDone
operator|++
expr_stmt|;
block|}
else|else
block|{
name|kvs
index|[
name|lowestKey
index|]
operator|=
name|rdrs
index|[
name|lowestKey
index|]
operator|.
name|getKeyValue
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/*    * It's assumed that the compactLock  will be acquired prior to calling this     * method!  Otherwise, it is not thread-safe!    *    * It works by processing a compaction that's been written to disk.    *     *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *     *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Moving the new compacted MapFile into place    * 2) Unload all replaced MapFiles, close and collect list to delete.    * 3) Loading the new TreeMap.    * 4) Compute new store size    *</pre>    *     * @param compactedFiles list of files that were compacted    * @param compactedFile HStoreFile that is the result of the compaction    * @throws IOException    */
specifier|private
name|void
name|completeCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|compactedFiles
parameter_list|,
specifier|final
name|HFile
operator|.
name|Writer
name|compactedFile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// 1. Moving the new files into place.
name|Path
name|p
init|=
literal|null
decl_stmt|;
try|try
block|{
name|p
operator|=
name|StoreFile
operator|.
name|rename
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|this
operator|.
name|homedir
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return;
block|}
name|StoreFile
name|finalCompactedFile
init|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|p
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
try|try
block|{
comment|// 3. Loading the new TreeMap.
comment|// Change this.storefiles so it reflects new state but do not
comment|// delete old store files until we have sent out notification of
comment|// change in case old files are still being accessed by outstanding
comment|// scanners.
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
name|compactedFiles
operator|.
name|contains
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
condition|)
block|{
name|this
operator|.
name|storefiles
operator|.
name|remove
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Add new compacted Reader and store file.
name|Long
name|orderVal
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|finalCompactedFile
operator|.
name|getMaxSequenceId
argument_list|()
argument_list|)
decl_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|finalCompactedFile
argument_list|)
expr_stmt|;
comment|// Tell observers that list of Readers has changed.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
comment|// Finally, delete old store files.
for|for
control|(
name|StoreFile
name|hsf
range|:
name|compactedFiles
control|)
block|{
name|hsf
operator|.
name|delete
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|=
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files for "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|". Compacted file is "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|".  Files replaced are "
operator|+
name|compactedFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// 4. Compute new store size
name|this
operator|.
name|storeSize
operator|=
literal|0L
expr_stmt|;
for|for
control|(
name|StoreFile
name|hsf
range|:
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
name|this
operator|.
name|storeSize
operator|+=
name|hsf
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|// ////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return all the available columns for the given key.  The key indicates a     * row and timestamp, but not a column name.    *    * The returned object should map column names to Cells.    * @param key -  Where to start searching.  Specifies a row.    * Columns are specified in following arguments.    * @param columns Can be null which means get all    * @param columnPattern Can be null.    * @param numVersions    * @param versionsCounter Can be null.    * @param keyvalues    * @param now -  Where to start searching.  Specifies a timestamp.    * @throws IOException    */
specifier|public
name|void
name|getFull
parameter_list|(
name|KeyValue
name|key
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
specifier|final
name|Pattern
name|columnPattern
parameter_list|,
specifier|final
name|int
name|numVersions
parameter_list|,
name|Map
argument_list|<
name|KeyValue
argument_list|,
name|HRegion
operator|.
name|Counter
argument_list|>
name|versionsCounter
parameter_list|,
name|List
argument_list|<
name|KeyValue
argument_list|>
name|keyvalues
parameter_list|,
specifier|final
name|long
name|now
parameter_list|)
throws|throws
name|IOException
block|{
comment|// if the key is null, we're not even looking for anything. return.
if|if
condition|(
name|key
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|int
name|versions
init|=
name|versionsToReturn
argument_list|(
name|numVersions
argument_list|)
decl_stmt|;
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
init|=
operator|new
name|TreeSet
argument_list|<
name|KeyValue
argument_list|>
argument_list|(
name|this
operator|.
name|comparatorIgnoringType
argument_list|)
decl_stmt|;
comment|// Create a Map that has results by column so we can keep count of versions.
comment|// It duplicates columns but doing check of columns, we don't want to make
comment|// column set each time.
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// get from the memcache first.
if|if
condition|(
name|this
operator|.
name|memcache
operator|.
name|getFull
argument_list|(
name|key
argument_list|,
name|columns
argument_list|,
name|columnPattern
argument_list|,
name|versions
argument_list|,
name|versionsCounter
argument_list|,
name|deletes
argument_list|,
name|keyvalues
argument_list|,
name|now
argument_list|)
condition|)
block|{
comment|// May have gotten enough results, enough to return.
return|return;
block|}
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|m
init|=
name|this
operator|.
name|storefiles
operator|.
name|descendingMap
argument_list|()
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
argument_list|>
name|i
init|=
name|m
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|i
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
if|if
condition|(
name|getFullFromStoreFile
argument_list|(
name|i
operator|.
name|next
argument_list|()
operator|.
name|getValue
argument_list|()
argument_list|,
name|key
argument_list|,
name|columns
argument_list|,
name|columnPattern
argument_list|,
name|versions
argument_list|,
name|versionsCounter
argument_list|,
name|deletes
argument_list|,
name|keyvalues
argument_list|)
condition|)
block|{
return|return;
block|}
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * @param f    * @param key Where to start searching.  Specifies a row and timestamp.    * Columns are specified in following arguments.    * @param columns    * @param versions    * @param versionCounter    * @param deletes    * @param keyvalues    * @return True if we found enough results to satisfy the<code>versions</code>    * and<code>columns</code> passed.    * @throws IOException    */
specifier|private
name|boolean
name|getFullFromStoreFile
parameter_list|(
name|StoreFile
name|f
parameter_list|,
name|KeyValue
name|target
parameter_list|,
name|Set
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
specifier|final
name|Pattern
name|columnPattern
parameter_list|,
name|int
name|versions
parameter_list|,
name|Map
argument_list|<
name|KeyValue
argument_list|,
name|HRegion
operator|.
name|Counter
argument_list|>
name|versionCounter
parameter_list|,
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
name|List
argument_list|<
name|KeyValue
argument_list|>
name|keyvalues
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|HFileScanner
name|scanner
init|=
name|f
operator|.
name|getReader
argument_list|()
operator|.
name|getScanner
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|getClosest
argument_list|(
name|scanner
argument_list|,
name|target
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|boolean
name|hasEnough
init|=
literal|false
decl_stmt|;
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
comment|// Make sure we have not passed out the row.  If target key has a
comment|// column on it, then we are looking explicit key+column combination.  If
comment|// we've passed it out, also break.
if|if
condition|(
name|target
operator|.
name|isEmptyColumn
argument_list|()
condition|?
operator|!
name|this
operator|.
name|comparator
operator|.
name|matchingRows
argument_list|(
name|target
argument_list|,
name|kv
argument_list|)
else|:
operator|!
name|this
operator|.
name|comparator
operator|.
name|matchingRowColumn
argument_list|(
name|target
argument_list|,
name|kv
argument_list|)
condition|)
block|{
break|break;
block|}
if|if
condition|(
operator|!
name|Store
operator|.
name|getFullCheck
argument_list|(
name|this
operator|.
name|comparator
argument_list|,
name|target
argument_list|,
name|kv
argument_list|,
name|columns
argument_list|,
name|columnPattern
argument_list|)
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|Store
operator|.
name|doKeyValue
argument_list|(
name|kv
argument_list|,
name|versions
argument_list|,
name|versionCounter
argument_list|,
name|columns
argument_list|,
name|deletes
argument_list|,
name|now
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|keyvalues
argument_list|,
literal|null
argument_list|)
condition|)
block|{
name|hasEnough
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
return|return
name|hasEnough
return|;
block|}
comment|/**    * Code shared by {@link Memcache#getFull(KeyValue, NavigableSet, Pattern, int, Map, NavigableSet, List, long)}    * and {@link #getFullFromStoreFile(StoreFile, KeyValue, Set, Pattern, int, Map, NavigableSet, List)}    * @param c    * @param target    * @param candidate    * @param columns    * @param columnPattern    * @return True if<code>candidate</code> matches column and timestamp.    */
specifier|static
name|boolean
name|getFullCheck
parameter_list|(
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|c
parameter_list|,
specifier|final
name|KeyValue
name|target
parameter_list|,
specifier|final
name|KeyValue
name|candidate
parameter_list|,
specifier|final
name|Set
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
specifier|final
name|Pattern
name|columnPattern
parameter_list|)
block|{
comment|// Does column match?
if|if
condition|(
operator|!
name|Store
operator|.
name|matchingColumns
argument_list|(
name|candidate
argument_list|,
name|columns
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// if the column pattern is not null, we use it for column matching.
comment|// we will skip the keys whose column doesn't match the pattern.
if|if
condition|(
name|columnPattern
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
operator|(
name|columnPattern
operator|.
name|matcher
argument_list|(
name|candidate
operator|.
name|getColumnString
argument_list|()
argument_list|)
operator|.
name|matches
argument_list|()
operator|)
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
if|if
condition|(
name|c
operator|.
name|compareTimestamps
argument_list|(
name|target
argument_list|,
name|candidate
argument_list|)
operator|>
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
return|return
literal|true
return|;
block|}
comment|/*    * @param wantedVersions How many versions were asked for.    * @return wantedVersions or this families' VERSIONS.    */
specifier|private
name|int
name|versionsToReturn
parameter_list|(
specifier|final
name|int
name|wantedVersions
parameter_list|)
block|{
if|if
condition|(
name|wantedVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
comment|// Make sure we do not return more than maximum versions for this store.
name|int
name|maxVersions
init|=
name|this
operator|.
name|family
operator|.
name|getMaxVersions
argument_list|()
decl_stmt|;
return|return
name|wantedVersions
operator|>
name|maxVersions
operator|&&
name|wantedVersions
operator|!=
name|HConstants
operator|.
name|ALL_VERSIONS
condition|?
name|maxVersions
else|:
name|wantedVersions
return|;
block|}
comment|/**    * Get the value for the indicated HStoreKey.  Grab the target value and the     * previous<code>numVersions - 1</code> values, as well.    *    * Use {@link HConstants.ALL_VERSIONS} to retrieve all versions.    * @param key    * @param numVersions Number of versions to fetch.  Must be> 0.    * @return values for the specified versions    * @throws IOException    */
name|List
argument_list|<
name|KeyValue
argument_list|>
name|get
parameter_list|(
specifier|final
name|KeyValue
name|key
parameter_list|,
specifier|final
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
comment|// This code below is very close to the body of the getKeys method.  Any
comment|// changes in the flow below should also probably be done in getKeys.
comment|// TODO: Refactor so same code used.
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|int
name|versions
init|=
name|versionsToReturn
argument_list|(
name|numVersions
argument_list|)
decl_stmt|;
comment|// Keep a list of deleted cell keys.  We need this because as we go through
comment|// the memcache and store files, the cell with the delete marker may be
comment|// in one store and the old non-delete cell value in a later store.
comment|// If we don't keep around the fact that the cell was deleted in a newer
comment|// record, we end up returning the old value if user is asking for more
comment|// than one version. This List of deletes should not be large since we
comment|// are only keeping rows and columns that match those set on the get and
comment|// which have delete values.  If memory usage becomes an issue, could
comment|// redo as bloom filter.  Use sorted set because test for membership should
comment|// be faster than calculating a hash.  Use a comparator that ignores ts.
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
init|=
operator|new
name|TreeSet
argument_list|<
name|KeyValue
argument_list|>
argument_list|(
name|this
operator|.
name|comparatorIgnoringType
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|KeyValue
argument_list|>
name|keyvalues
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Check the memcache
if|if
condition|(
name|this
operator|.
name|memcache
operator|.
name|get
argument_list|(
name|key
argument_list|,
name|versions
argument_list|,
name|keyvalues
argument_list|,
name|deletes
argument_list|,
name|now
argument_list|)
condition|)
block|{
return|return
name|keyvalues
return|;
block|}
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|m
init|=
name|this
operator|.
name|storefiles
operator|.
name|descendingMap
argument_list|()
decl_stmt|;
name|boolean
name|hasEnough
init|=
literal|false
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|e
range|:
name|m
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|StoreFile
name|f
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|HFileScanner
name|scanner
init|=
name|f
operator|.
name|getReader
argument_list|()
operator|.
name|getScanner
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|getClosest
argument_list|(
name|scanner
argument_list|,
name|key
argument_list|)
condition|)
block|{
comment|// Move to next file.
continue|continue;
block|}
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
comment|// Make sure below matches what happens up in Memcache#get.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|matchingRowColumn
argument_list|(
name|kv
argument_list|,
name|key
argument_list|)
condition|)
block|{
if|if
condition|(
name|doKeyValue
argument_list|(
name|kv
argument_list|,
name|versions
argument_list|,
name|deletes
argument_list|,
name|now
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|keyvalues
argument_list|,
literal|null
argument_list|)
condition|)
block|{
name|hasEnough
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
else|else
block|{
comment|// Row and column don't match. Must have gone past. Move to next file.
break|break;
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
if|if
condition|(
name|hasEnough
condition|)
block|{
break|break;
comment|// Break out of files loop.
block|}
block|}
return|return
name|keyvalues
operator|.
name|isEmpty
argument_list|()
condition|?
literal|null
else|:
name|keyvalues
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Small method to check if we are over the max number of versions    * or we acheived this family max versions.     * The later happens when we have the situation described in HBASE-621.    * @param versions    * @param c    * @return     */
specifier|static
name|boolean
name|hasEnoughVersions
parameter_list|(
specifier|final
name|int
name|versions
parameter_list|,
specifier|final
name|List
argument_list|<
name|KeyValue
argument_list|>
name|c
parameter_list|)
block|{
return|return
name|versions
operator|>
literal|0
operator|&&
operator|!
name|c
operator|.
name|isEmpty
argument_list|()
operator|&&
name|c
operator|.
name|size
argument_list|()
operator|>=
name|versions
return|;
block|}
comment|/*    * Used when doing getFulls.    * @param kv    * @param versions    * @param versionCounter    * @param columns    * @param deletes    * @param now    * @param ttl    * @param keyvalues    * @param set    * @return True if enough versions.    */
specifier|static
name|boolean
name|doKeyValue
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|,
specifier|final
name|int
name|versions
parameter_list|,
specifier|final
name|Map
argument_list|<
name|KeyValue
argument_list|,
name|Counter
argument_list|>
name|versionCounter
parameter_list|,
specifier|final
name|Set
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
specifier|final
name|long
name|now
parameter_list|,
specifier|final
name|long
name|ttl
parameter_list|,
specifier|final
name|List
argument_list|<
name|KeyValue
argument_list|>
name|keyvalues
parameter_list|,
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|)
block|{
name|boolean
name|hasEnough
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|kv
operator|.
name|isDeleteType
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|deletes
operator|.
name|contains
argument_list|(
name|kv
argument_list|)
condition|)
block|{
name|deletes
operator|.
name|add
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
operator|!
name|deletes
operator|.
name|contains
argument_list|(
name|kv
argument_list|)
condition|)
block|{
comment|// Skip expired cells
if|if
condition|(
operator|!
name|isExpired
argument_list|(
name|kv
argument_list|,
name|ttl
argument_list|,
name|now
argument_list|)
condition|)
block|{
if|if
condition|(
name|HRegion
operator|.
name|okToAddResult
argument_list|(
name|kv
argument_list|,
name|versions
argument_list|,
name|versionCounter
argument_list|)
condition|)
block|{
name|HRegion
operator|.
name|addResult
argument_list|(
name|kv
argument_list|,
name|versionCounter
argument_list|,
name|keyvalues
argument_list|)
expr_stmt|;
if|if
condition|(
name|HRegion
operator|.
name|hasEnoughVersions
argument_list|(
name|versions
argument_list|,
name|versionCounter
argument_list|,
name|columns
argument_list|)
condition|)
block|{
name|hasEnough
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|// Remove the expired.
name|Store
operator|.
name|expiredOrDeleted
argument_list|(
name|set
argument_list|,
name|kv
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|hasEnough
return|;
block|}
comment|/*    * Used when doing get.    * @param kv    * @param versions    * @param deletes    * @param now    * @param ttl    * @param keyvalues    * @param set    * @return True if enough versions.    */
specifier|static
name|boolean
name|doKeyValue
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|,
specifier|final
name|int
name|versions
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
specifier|final
name|long
name|now
parameter_list|,
specifier|final
name|long
name|ttl
parameter_list|,
specifier|final
name|List
argument_list|<
name|KeyValue
argument_list|>
name|keyvalues
parameter_list|,
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|)
block|{
name|boolean
name|hasEnough
init|=
literal|false
decl_stmt|;
if|if
condition|(
operator|!
name|kv
operator|.
name|isDeleteType
argument_list|()
condition|)
block|{
comment|// Filter out expired results
if|if
condition|(
name|notExpiredAndNotInDeletes
argument_list|(
name|ttl
argument_list|,
name|kv
argument_list|,
name|now
argument_list|,
name|deletes
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|keyvalues
operator|.
name|contains
argument_list|(
name|kv
argument_list|)
condition|)
block|{
name|keyvalues
operator|.
name|add
argument_list|(
name|kv
argument_list|)
expr_stmt|;
if|if
condition|(
name|hasEnoughVersions
argument_list|(
name|versions
argument_list|,
name|keyvalues
argument_list|)
condition|)
block|{
name|hasEnough
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
if|if
condition|(
name|set
operator|!=
literal|null
condition|)
block|{
name|expiredOrDeleted
argument_list|(
name|set
argument_list|,
name|kv
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|// Cell holds a delete value.
name|deletes
operator|.
name|add
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
return|return
name|hasEnough
return|;
block|}
comment|/*    * Test that the<i>target</i> matches the<i>origin</i>. If the<i>origin</i>    * has an empty column, then it just tests row equivalence. Otherwise, it uses    * HStoreKey.matchesRowCol().    * @param c Comparator to use.    * @param origin Key we're testing against    * @param target Key we're testing    */
specifier|static
name|boolean
name|matchingRowColumn
parameter_list|(
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|c
parameter_list|,
specifier|final
name|KeyValue
name|origin
parameter_list|,
specifier|final
name|KeyValue
name|target
parameter_list|)
block|{
return|return
name|origin
operator|.
name|isEmptyColumn
argument_list|()
condition|?
name|c
operator|.
name|matchingRows
argument_list|(
name|target
argument_list|,
name|origin
argument_list|)
else|:
name|c
operator|.
name|matchingRowColumn
argument_list|(
name|target
argument_list|,
name|origin
argument_list|)
return|;
block|}
specifier|static
name|void
name|expiredOrDeleted
parameter_list|(
specifier|final
name|Set
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|,
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|boolean
name|b
init|=
name|set
operator|.
name|remove
argument_list|(
name|kv
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|kv
operator|.
name|toString
argument_list|()
operator|+
literal|" expired: "
operator|+
name|b
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Find the key that matches<i>row</i> exactly, or the one that immediately    * preceeds it. WARNING: Only use this method on a table where writes occur     * with stricly increasing timestamps. This method assumes this pattern of     * writes in order to make it reasonably performant.    * @param targetkey    * @return Found keyvalue    * @throws IOException    */
name|KeyValue
name|getRowKeyAtOrBefore
parameter_list|(
specifier|final
name|KeyValue
name|targetkey
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Map of keys that are candidates for holding the row key that
comment|// most closely matches what we're looking for. We'll have to update it as
comment|// deletes are found all over the place as we go along before finally
comment|// reading the best key out of it at the end.   Use a comparator that
comment|// ignores key types.  Otherwise, we can't remove deleted items doing
comment|// set.remove because of the differing type between insert and delete.
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|candidates
init|=
operator|new
name|TreeSet
argument_list|<
name|KeyValue
argument_list|>
argument_list|(
name|this
operator|.
name|comparator
operator|.
name|getComparatorIgnoringType
argument_list|()
argument_list|)
decl_stmt|;
comment|// Keep a list of deleted cell keys.  We need this because as we go through
comment|// the store files, the cell with the delete marker may be in one file and
comment|// the old non-delete cell value in a later store file. If we don't keep
comment|// around the fact that the cell was deleted in a newer record, we end up
comment|// returning the old value if user is asking for more than one version.
comment|// This List of deletes should not be large since we are only keeping rows
comment|// and columns that match those set on the scanner and which have delete
comment|// values.  If memory usage becomes an issue, could redo as bloom filter.
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
init|=
operator|new
name|TreeSet
argument_list|<
name|KeyValue
argument_list|>
argument_list|(
name|this
operator|.
name|comparatorIgnoringType
argument_list|)
decl_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// First go to the memcache.  Pick up deletes and candidates.
name|this
operator|.
name|memcache
operator|.
name|getRowKeyAtOrBefore
argument_list|(
name|targetkey
argument_list|,
name|candidates
argument_list|,
name|deletes
argument_list|,
name|now
argument_list|)
expr_stmt|;
comment|// Process each store file.  Run through from newest to oldest.
name|Map
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|m
init|=
name|this
operator|.
name|storefiles
operator|.
name|descendingMap
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|e
range|:
name|m
operator|.
name|entrySet
argument_list|()
control|)
block|{
comment|// Update the candidate keys from the current map file
name|rowAtOrBeforeFromStoreFile
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|,
name|targetkey
argument_list|,
name|candidates
argument_list|,
name|deletes
argument_list|,
name|now
argument_list|)
expr_stmt|;
block|}
comment|// Return the best key from candidateKeys
return|return
name|candidates
operator|.
name|isEmpty
argument_list|()
condition|?
literal|null
else|:
name|candidates
operator|.
name|last
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Check an individual MapFile for the row at or before a given key     * and timestamp    * @param f    * @param targetkey    * @param candidates Pass a Set with a Comparator that    * ignores key Type so we can do Set.remove using a delete, i.e. a KeyValue    * with a different Type to the candidate key.    * @throws IOException    */
specifier|private
name|void
name|rowAtOrBeforeFromStoreFile
parameter_list|(
specifier|final
name|StoreFile
name|f
parameter_list|,
specifier|final
name|KeyValue
name|targetkey
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|candidates
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
specifier|final
name|long
name|now
parameter_list|)
throws|throws
name|IOException
block|{
comment|// if there aren't any candidate keys yet, we'll do some things different
if|if
condition|(
name|candidates
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|rowAtOrBeforeCandidate
argument_list|(
name|f
argument_list|,
name|targetkey
argument_list|,
name|candidates
argument_list|,
name|deletes
argument_list|,
name|now
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|rowAtOrBeforeWithCandidates
argument_list|(
name|f
argument_list|,
name|targetkey
argument_list|,
name|candidates
argument_list|,
name|deletes
argument_list|,
name|now
argument_list|)
expr_stmt|;
block|}
block|}
comment|/*     * @param ttlSetting    * @param hsk    * @param now    * @param deletes A Set whose Comparator ignores Type.    * @return True if key has not expired and is not in passed set of deletes.    */
specifier|static
name|boolean
name|notExpiredAndNotInDeletes
parameter_list|(
specifier|final
name|long
name|ttl
parameter_list|,
specifier|final
name|KeyValue
name|key
parameter_list|,
specifier|final
name|long
name|now
parameter_list|,
specifier|final
name|Set
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|)
block|{
return|return
operator|!
name|isExpired
argument_list|(
name|key
argument_list|,
name|ttl
argument_list|,
name|now
argument_list|)
operator|&&
operator|(
name|deletes
operator|==
literal|null
operator|||
name|deletes
operator|.
name|isEmpty
argument_list|()
operator|||
operator|!
name|deletes
operator|.
name|contains
argument_list|(
name|key
argument_list|)
operator|)
return|;
block|}
specifier|static
name|boolean
name|isExpired
parameter_list|(
specifier|final
name|KeyValue
name|key
parameter_list|,
specifier|final
name|long
name|ttl
parameter_list|,
specifier|final
name|long
name|now
parameter_list|)
block|{
return|return
name|ttl
operator|!=
name|HConstants
operator|.
name|FOREVER
operator|&&
name|now
operator|>
name|key
operator|.
name|getTimestamp
argument_list|()
operator|+
name|ttl
return|;
block|}
comment|/* Find a candidate for row that is at or before passed key, searchkey, in hfile.    * @param f    * @param targetkey Key to go search the hfile with.    * @param candidates    * @param now    * @throws IOException    * @see {@link #rowAtOrBeforeCandidate(HStoreKey, org.apache.hadoop.io.MapFile.Reader, byte[], SortedMap, long)}    */
specifier|private
name|void
name|rowAtOrBeforeCandidate
parameter_list|(
specifier|final
name|StoreFile
name|f
parameter_list|,
specifier|final
name|KeyValue
name|targetkey
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|candidates
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
specifier|final
name|long
name|now
parameter_list|)
throws|throws
name|IOException
block|{
name|KeyValue
name|search
init|=
name|targetkey
decl_stmt|;
comment|// If the row we're looking for is past the end of this mapfile, set the
comment|// search key to be the last key.  If its a deleted key, then we'll back
comment|// up to the row before and return that.
comment|// TODO: Cache last key as KV over in the file.
name|byte
index|[]
name|lastkey
init|=
name|f
operator|.
name|getReader
argument_list|()
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKeyValue
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lastkey
argument_list|,
literal|0
argument_list|,
name|lastkey
operator|.
name|length
argument_list|)
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|lastKeyValue
argument_list|,
name|targetkey
argument_list|)
operator|<
literal|0
condition|)
block|{
name|search
operator|=
name|lastKeyValue
expr_stmt|;
block|}
name|KeyValue
name|knownNoGoodKey
init|=
literal|null
decl_stmt|;
name|HFileScanner
name|scanner
init|=
name|f
operator|.
name|getReader
argument_list|()
operator|.
name|getScanner
argument_list|()
decl_stmt|;
for|for
control|(
name|boolean
name|foundCandidate
init|=
literal|false
init|;
operator|!
name|foundCandidate
condition|;
control|)
block|{
comment|// Seek to the exact row, or the one that would be immediately before it
name|int
name|result
init|=
name|scanner
operator|.
name|seekTo
argument_list|(
name|search
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|search
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|search
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|result
operator|<
literal|0
condition|)
block|{
comment|// Not in file.
break|break;
block|}
name|KeyValue
name|deletedOrExpiredRow
init|=
literal|null
decl_stmt|;
name|KeyValue
name|kv
init|=
literal|null
decl_stmt|;
do|do
block|{
name|kv
operator|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
expr_stmt|;
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|kv
argument_list|,
name|search
argument_list|)
operator|<=
literal|0
condition|)
block|{
if|if
condition|(
operator|!
name|kv
operator|.
name|isDeleteType
argument_list|()
condition|)
block|{
if|if
condition|(
name|handleNonDelete
argument_list|(
name|kv
argument_list|,
name|now
argument_list|,
name|deletes
argument_list|,
name|candidates
argument_list|)
condition|)
block|{
name|foundCandidate
operator|=
literal|true
expr_stmt|;
comment|// NOTE! Continue.
continue|continue;
block|}
block|}
name|deletes
operator|.
name|add
argument_list|(
name|kv
argument_list|)
expr_stmt|;
if|if
condition|(
name|deletedOrExpiredRow
operator|==
literal|null
condition|)
block|{
name|deletedOrExpiredRow
operator|=
name|kv
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|kv
argument_list|,
name|search
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the row key we just read is beyond the key we're searching for,
comment|// then we're done.
break|break;
block|}
else|else
block|{
comment|// So, the row key doesn't match, but we haven't gone past the row
comment|// we're seeking yet, so this row is a candidate for closest
comment|// (assuming that it isn't a delete).
if|if
condition|(
operator|!
name|kv
operator|.
name|isDeleteType
argument_list|()
condition|)
block|{
if|if
condition|(
name|handleNonDelete
argument_list|(
name|kv
argument_list|,
name|now
argument_list|,
name|deletes
argument_list|,
name|candidates
argument_list|)
condition|)
block|{
name|foundCandidate
operator|=
literal|true
expr_stmt|;
comment|// NOTE: Continue
continue|continue;
block|}
block|}
name|deletes
operator|.
name|add
argument_list|(
name|kv
argument_list|)
expr_stmt|;
if|if
condition|(
name|deletedOrExpiredRow
operator|==
literal|null
condition|)
block|{
name|deletedOrExpiredRow
operator|=
name|kv
expr_stmt|;
block|}
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
operator|&&
operator|(
name|knownNoGoodKey
operator|==
literal|null
operator|||
name|this
operator|.
name|comparator
operator|.
name|compare
argument_list|(
name|kv
argument_list|,
name|knownNoGoodKey
argument_list|)
operator|<
literal|0
operator|)
condition|)
do|;
comment|// If we get here and have no candidates but we did find a deleted or
comment|// expired candidate, we need to look at the key before that
if|if
condition|(
operator|!
name|foundCandidate
operator|&&
name|deletedOrExpiredRow
operator|!=
literal|null
condition|)
block|{
name|knownNoGoodKey
operator|=
name|deletedOrExpiredRow
expr_stmt|;
if|if
condition|(
operator|!
name|scanner
operator|.
name|seekBefore
argument_list|(
name|deletedOrExpiredRow
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|deletedOrExpiredRow
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|deletedOrExpiredRow
operator|.
name|getKeyLength
argument_list|()
argument_list|)
condition|)
block|{
comment|// Not in file -- what can I do now but break?
break|break;
block|}
name|search
operator|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// No candidates and no deleted or expired candidates. Give up.
break|break;
block|}
block|}
comment|// Arriving here just means that we consumed the whole rest of the map
comment|// without going "past" the key we're searching for. we can just fall
comment|// through here.
block|}
specifier|private
name|void
name|rowAtOrBeforeWithCandidates
parameter_list|(
specifier|final
name|StoreFile
name|f
parameter_list|,
specifier|final
name|KeyValue
name|targetkey
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|candidates
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
specifier|final
name|long
name|now
parameter_list|)
throws|throws
name|IOException
block|{
comment|// if there are already candidate keys, we need to start our search
comment|// at the earliest possible key so that we can discover any possible
comment|// deletes for keys between the start and the search key.  Back up to start
comment|// of the row in case there are deletes for this candidate in this mapfile
comment|// BUT do not backup before the first key in the store file.
name|KeyValue
name|firstCandidateKey
init|=
name|candidates
operator|.
name|first
argument_list|()
decl_stmt|;
name|KeyValue
name|search
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|firstCandidateKey
argument_list|,
name|targetkey
argument_list|)
operator|<
literal|0
condition|)
block|{
name|search
operator|=
name|targetkey
expr_stmt|;
block|}
else|else
block|{
name|search
operator|=
name|firstCandidateKey
expr_stmt|;
block|}
comment|// Seek to the exact row, or the one that would be immediately before it
name|HFileScanner
name|scanner
init|=
name|f
operator|.
name|getReader
argument_list|()
operator|.
name|getScanner
argument_list|()
decl_stmt|;
name|int
name|result
init|=
name|scanner
operator|.
name|seekTo
argument_list|(
name|search
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|search
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|search
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|result
operator|<
literal|0
condition|)
block|{
comment|// Key is before start of this file.  Return.
return|return;
block|}
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
comment|// if we have an exact match on row, and it's not a delete, save this
comment|// as a candidate key
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|matchingRows
argument_list|(
name|kv
argument_list|,
name|targetkey
argument_list|)
condition|)
block|{
name|handleKey
argument_list|(
name|kv
argument_list|,
name|now
argument_list|,
name|deletes
argument_list|,
name|candidates
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|kv
argument_list|,
name|targetkey
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the row key we just read is beyond the key we're searching for,
comment|// then we're done.
break|break;
block|}
else|else
block|{
comment|// So, the row key doesn't match, but we haven't gone past the row
comment|// we're seeking yet, so this row is a candidate for closest
comment|// (assuming that it isn't a delete).
name|handleKey
argument_list|(
name|kv
argument_list|,
name|now
argument_list|,
name|deletes
argument_list|,
name|candidates
argument_list|)
expr_stmt|;
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
block|}
comment|/*    * Used calculating keys at or just before a passed key.    * @param readkey    * @param now    * @param deletes Set with Comparator that ignores key type.    * @param candidate Set with Comprator that ignores key type.    */
specifier|private
name|void
name|handleKey
parameter_list|(
specifier|final
name|KeyValue
name|readkey
parameter_list|,
specifier|final
name|long
name|now
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|candidates
parameter_list|)
block|{
if|if
condition|(
operator|!
name|readkey
operator|.
name|isDeleteType
argument_list|()
condition|)
block|{
name|handleNonDelete
argument_list|(
name|readkey
argument_list|,
name|now
argument_list|,
name|deletes
argument_list|,
name|candidates
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|handleDeletes
argument_list|(
name|readkey
argument_list|,
name|candidates
argument_list|,
name|deletes
argument_list|)
expr_stmt|;
block|}
block|}
comment|/*    * Used calculating keys at or just before a passed key.    * @param readkey    * @param now    * @param deletes Set with Comparator that ignores key type.    * @param candidates Set with Comparator that ignores key type.    * @return True if we added a candidate.    */
specifier|private
name|boolean
name|handleNonDelete
parameter_list|(
specifier|final
name|KeyValue
name|readkey
parameter_list|,
specifier|final
name|long
name|now
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|candidates
parameter_list|)
block|{
if|if
condition|(
name|notExpiredAndNotInDeletes
argument_list|(
name|this
operator|.
name|ttl
argument_list|,
name|readkey
argument_list|,
name|now
argument_list|,
name|deletes
argument_list|)
condition|)
block|{
name|candidates
operator|.
name|add
argument_list|(
name|readkey
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
comment|/**    * Handle keys whose values hold deletes.    * Add to the set of deletes and then if the candidate keys contain any that    * might match, then check for a match and remove it.  Implies candidates    * is made with a Comparator that ignores key type.    * @param k    * @param candidates    * @param deletes    * @return True if we removed<code>k</code> from<code>candidates</code>.    */
specifier|static
name|boolean
name|handleDeletes
parameter_list|(
specifier|final
name|KeyValue
name|k
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|candidates
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|KeyValue
argument_list|>
name|deletes
parameter_list|)
block|{
name|deletes
operator|.
name|add
argument_list|(
name|k
argument_list|)
expr_stmt|;
return|return
name|candidates
operator|.
name|remove
argument_list|(
name|k
argument_list|)
return|;
block|}
comment|/**    * Determines if HStore can be split    * @param force Whether to force a split or not.    * @return a StoreSize if store can be split, null otherwise.    */
name|StoreSize
name|checkSplit
parameter_list|(
specifier|final
name|boolean
name|force
parameter_list|)
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Iterate through all store files
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
operator|!
name|force
operator|&&
operator|(
name|storeSize
operator|<
name|this
operator|.
name|desiredMaxFileSize
operator|)
condition|)
block|{
return|return
literal|null
return|;
block|}
comment|// Not splitable if we find a reference store file present in the store.
name|boolean
name|splitable
init|=
literal|true
decl_stmt|;
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|Long
name|mapIndex
init|=
name|Long
operator|.
name|valueOf
argument_list|(
literal|0L
argument_list|)
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|StoreFile
argument_list|>
name|e
range|:
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|StoreFile
name|sf
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|splitable
condition|)
block|{
name|splitable
operator|=
operator|!
name|sf
operator|.
name|isReference
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|splitable
condition|)
block|{
comment|// RETURN IN MIDDLE OF FUNCTION!!! If not splitable, just return.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|sf
operator|+
literal|" is not splittable"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
block|}
name|long
name|size
init|=
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|mapIndex
operator|=
name|e
operator|.
name|getKey
argument_list|()
expr_stmt|;
block|}
block|}
name|HFile
operator|.
name|Reader
name|r
init|=
name|this
operator|.
name|storefiles
operator|.
name|get
argument_list|(
name|mapIndex
argument_list|)
operator|.
name|getReader
argument_list|()
decl_stmt|;
comment|// Get first, last, and mid keys.  Midkey is the key that starts block
comment|// in middle of hfile.  Has column and timestamp.  Need to return just
comment|// the row we want to split on as midkey.
name|byte
index|[]
name|midkey
init|=
name|r
operator|.
name|midkey
argument_list|()
decl_stmt|;
if|if
condition|(
name|midkey
operator|!=
literal|null
condition|)
block|{
name|KeyValue
name|mk
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|midkey
argument_list|,
literal|0
argument_list|,
name|midkey
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
comment|// if the midkey is the same as the first and last keys, then we cannot
comment|// (ever) split this region.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|firstKey
argument_list|)
operator|==
literal|0
operator|&&
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|lastKey
argument_list|)
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"cannot split because midkey is the same as first or "
operator|+
literal|"last row"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
return|return
operator|new
name|StoreSize
argument_list|(
name|maxSize
argument_list|,
name|mk
operator|.
name|getRow
argument_list|()
argument_list|)
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed getting store size for "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** @return aggregate size of HStore */
specifier|public
name|long
name|getSize
parameter_list|()
block|{
return|return
name|storeSize
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return a scanner for both the memcache and the HStore files    */
specifier|protected
name|InternalScanner
name|getScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|targetCols
parameter_list|,
name|byte
index|[]
name|firstRow
parameter_list|,
name|RowFilterInterface
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|targetCols
argument_list|,
name|firstRow
argument_list|,
name|timestamp
argument_list|,
name|filter
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|this
operator|.
name|storeNameStr
return|;
block|}
comment|/**    * @return Count of store files    */
name|int
name|getStorefilesCount
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/**    * @return The size of the store file indexes, in bytes.    * @throws IOException if there was a problem getting file sizes from the    * filesystem    */
name|long
name|getStorefilesIndexSize
parameter_list|()
throws|throws
name|IOException
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
operator|.
name|values
argument_list|()
control|)
name|size
operator|+=
name|s
operator|.
name|getReader
argument_list|()
operator|.
name|indexSize
argument_list|()
expr_stmt|;
return|return
name|size
return|;
block|}
comment|/*    * Datastructure that holds size and row to split a file around.    * TODO: Take a KeyValue rather than row.    */
specifier|static
class|class
name|StoreSize
block|{
specifier|private
specifier|final
name|long
name|size
decl_stmt|;
specifier|private
specifier|final
name|byte
index|[]
name|row
decl_stmt|;
name|StoreSize
parameter_list|(
name|long
name|size
parameter_list|,
name|byte
index|[]
name|row
parameter_list|)
block|{
name|this
operator|.
name|size
operator|=
name|size
expr_stmt|;
name|this
operator|.
name|row
operator|=
name|row
expr_stmt|;
block|}
comment|/* @return the size */
name|long
name|getSize
parameter_list|()
block|{
return|return
name|size
return|;
block|}
name|byte
index|[]
name|getSplitRow
parameter_list|()
block|{
return|return
name|this
operator|.
name|row
return|;
block|}
block|}
name|HRegionInfo
name|getHRegionInfo
parameter_list|()
block|{
return|return
name|this
operator|.
name|regioninfo
return|;
block|}
comment|/**    * Convenience method that implements the old MapFile.getClosest on top of    * HFile Scanners.  getClosest used seek to the asked-for key or just after    * (HFile seeks to the key or just before).    * @param s Scanner to use    * @param kv Key to find.    * @return True if we were able to seek the scanner to<code>b</code> or to    * the key just after.    * @throws IOException     */
specifier|static
name|boolean
name|getClosest
parameter_list|(
specifier|final
name|HFileScanner
name|s
parameter_list|,
specifier|final
name|KeyValue
name|kv
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Pass offsets to key content of a KeyValue; thats whats in the hfile index.
name|int
name|result
init|=
name|s
operator|.
name|seekTo
argument_list|(
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|result
operator|<
literal|0
condition|)
block|{
comment|// Not in file.  Will the first key do?
if|if
condition|(
operator|!
name|s
operator|.
name|seekTo
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
elseif|else
if|if
condition|(
name|result
operator|>
literal|0
condition|)
block|{
comment|// Less than what was asked for but maybe< because we're asking for
comment|// r/c/LATEST_TIMESTAMP -- what was returned was r/c-1/SOME_TS...
comment|// A next will get us a r/c/SOME_TS.
if|if
condition|(
operator|!
name|s
operator|.
name|next
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
comment|/**    * @param kv    * @param columns Can be null    * @return True if column matches.    */
specifier|static
name|boolean
name|matchingColumns
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|,
specifier|final
name|Set
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|)
block|{
if|if
condition|(
name|columns
operator|==
literal|null
condition|)
block|{
return|return
literal|true
return|;
block|}
comment|// Only instantiate columns if lots of columns to test.
if|if
condition|(
name|columns
operator|.
name|size
argument_list|()
operator|>
literal|100
condition|)
block|{
return|return
name|columns
operator|.
name|contains
argument_list|(
name|kv
operator|.
name|getColumn
argument_list|()
argument_list|)
return|;
block|}
for|for
control|(
name|byte
index|[]
name|column
range|:
name|columns
control|)
block|{
if|if
condition|(
name|kv
operator|.
name|matchingColumn
argument_list|(
name|column
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
block|}
end_class

end_unit

