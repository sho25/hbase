begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2006 The Apache Software Foundation  *  * Licensed under the Apache License, Version 2.0 (the "License");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|*
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|*
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|*
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|*
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|*
import|;
end_import

begin_comment
comment|/**  * HRegion stores data for a certain region of a table.  It stores all columns  * for each row. A given table consists of one or more HRegions.  *  *<p>We maintain multiple HStores for a single HRegion.  *   *<p>An HStore is a set of rows with some column data; together,  * they make up all the data for the rows.    *  *<p>Each HRegion has a 'startKey' and 'endKey'.  *     *<p>The first is inclusive, the second is exclusive (except for  * the final region)  The endKey of region 0 is the same as  * startKey for region 1 (if it exists).  The startKey for the  * first region is null. The endKey for the final region is null.  *  *<p>The HStores have no locking built-in.  All row-level locking  * and row-level atomicity is provided by the HRegion.  *   *<p>An HRegion is defined by its table and its key extent.  *   *<p>It consists of at least one HStore.  The number of HStores should be   * configurable, so that data which is accessed together is stored in the same  * HStore.  Right now, we approximate that by building a single HStore for   * each column family.  (This config info will be communicated via the   * tabledesc.)  *   * The HTableDescriptor contains metainfo about the HRegion's table.  * regionName is a unique identifier for this HRegion. (startKey, endKey]  * defines the keyspace for this HRegion.  */
end_comment

begin_class
class|class
name|HRegion
implements|implements
name|HConstants
block|{
specifier|static
name|String
name|SPLITDIR
init|=
literal|"splits"
decl_stmt|;
specifier|static
name|String
name|MERGEDIR
init|=
literal|"merges"
decl_stmt|;
specifier|static
name|String
name|TMPREGION_PREFIX
init|=
literal|"tmpregion_"
decl_stmt|;
specifier|static
name|int
name|MIN_COMMITS_FOR_COMPACTION
init|=
literal|10
decl_stmt|;
specifier|static
name|Random
name|rand
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HRegion
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/**    * Deletes all the files for a HRegion    *     * @param fs                  - the file system object    * @param baseDirectory       - base directory for HBase    * @param regionName          - name of the region to delete    * @throws IOException    */
specifier|static
name|void
name|deleteRegion
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|baseDirectory
parameter_list|,
name|Text
name|regionName
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Deleting region "
operator|+
name|regionName
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|HStoreFile
operator|.
name|getHRegionDir
argument_list|(
name|baseDirectory
argument_list|,
name|regionName
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**    * Merge two HRegions.  They must be available on the current    * HRegionServer. Returns a brand-new active HRegion, also    * running on the current HRegionServer.    */
specifier|static
name|HRegion
name|closeAndMerge
parameter_list|(
name|HRegion
name|srcA
parameter_list|,
name|HRegion
name|srcB
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Make sure that srcA comes first; important for key-ordering during
comment|// write of the merged file.
if|if
condition|(
name|srcA
operator|.
name|getStartKey
argument_list|()
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|srcB
operator|.
name|getStartKey
argument_list|()
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot merge two regions with null start key"
argument_list|)
throw|;
block|}
comment|// A's start key is null but B's isn't. Assume A comes before B
block|}
elseif|else
if|if
condition|(
operator|(
name|srcB
operator|.
name|getStartKey
argument_list|()
operator|==
literal|null
operator|)
comment|// A is not null but B is
operator|||
operator|(
name|srcA
operator|.
name|getStartKey
argument_list|()
operator|.
name|compareTo
argument_list|(
name|srcB
operator|.
name|getStartKey
argument_list|()
argument_list|)
operator|>
literal|0
operator|)
condition|)
block|{
comment|// A> B
name|HRegion
name|tmp
init|=
name|srcA
decl_stmt|;
name|srcA
operator|=
name|srcB
expr_stmt|;
name|srcB
operator|=
name|tmp
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|srcA
operator|.
name|getEndKey
argument_list|()
operator|.
name|equals
argument_list|(
name|srcB
operator|.
name|getStartKey
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot merge non-adjacent regions"
argument_list|)
throw|;
block|}
name|FileSystem
name|fs
init|=
name|srcA
operator|.
name|getFilesystem
argument_list|()
decl_stmt|;
name|Configuration
name|conf
init|=
name|srcA
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|HTableDescriptor
name|tabledesc
init|=
name|srcA
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
name|HLog
name|log
init|=
name|srcA
operator|.
name|getLog
argument_list|()
decl_stmt|;
name|Path
name|rootDir
init|=
name|srcA
operator|.
name|getRootDir
argument_list|()
decl_stmt|;
name|Text
name|startKey
init|=
name|srcA
operator|.
name|getStartKey
argument_list|()
decl_stmt|;
name|Text
name|endKey
init|=
name|srcB
operator|.
name|getEndKey
argument_list|()
decl_stmt|;
name|Path
name|merges
init|=
operator|new
name|Path
argument_list|(
name|srcA
operator|.
name|getRegionDir
argument_list|()
argument_list|,
name|MERGEDIR
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|merges
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|merges
argument_list|)
expr_stmt|;
block|}
name|HRegionInfo
name|newRegionInfo
init|=
operator|new
name|HRegionInfo
argument_list|(
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
argument_list|,
name|tabledesc
argument_list|,
name|startKey
argument_list|,
name|endKey
argument_list|)
decl_stmt|;
name|Path
name|newRegionDir
init|=
name|HStoreFile
operator|.
name|getHRegionDir
argument_list|(
name|merges
argument_list|,
name|newRegionInfo
operator|.
name|regionName
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|newRegionDir
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot merge; target file collision at "
operator|+
name|newRegionDir
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"starting merge of regions: "
operator|+
name|srcA
operator|.
name|getRegionName
argument_list|()
operator|+
literal|" and "
operator|+
name|srcB
operator|.
name|getRegionName
argument_list|()
operator|+
literal|" new region start key is '"
operator|+
operator|(
name|startKey
operator|==
literal|null
condition|?
literal|""
else|:
name|startKey
operator|)
operator|+
literal|"', end key is '"
operator|+
operator|(
name|endKey
operator|==
literal|null
condition|?
literal|""
else|:
name|endKey
operator|)
operator|+
literal|"'"
argument_list|)
expr_stmt|;
comment|// Flush each of the sources, and merge their files into a single
comment|// target for each column family.
name|TreeSet
argument_list|<
name|HStoreFile
argument_list|>
name|alreadyMerged
init|=
operator|new
name|TreeSet
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|>
name|filesToMerge
init|=
operator|new
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|HStoreFile
name|src
range|:
name|srcA
operator|.
name|flushcache
argument_list|(
literal|true
argument_list|)
control|)
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|v
init|=
name|filesToMerge
operator|.
name|get
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|v
operator|==
literal|null
condition|)
block|{
name|v
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
expr_stmt|;
name|filesToMerge
operator|.
name|put
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|v
argument_list|)
expr_stmt|;
block|}
name|v
operator|.
name|add
argument_list|(
name|src
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|HStoreFile
name|src
range|:
name|srcB
operator|.
name|flushcache
argument_list|(
literal|true
argument_list|)
control|)
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|v
init|=
name|filesToMerge
operator|.
name|get
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|v
operator|==
literal|null
condition|)
block|{
name|v
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
expr_stmt|;
name|filesToMerge
operator|.
name|put
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|v
argument_list|)
expr_stmt|;
block|}
name|v
operator|.
name|add
argument_list|(
name|src
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"merging stores"
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Text
argument_list|,
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|>
name|es
range|:
name|filesToMerge
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|Text
name|colFamily
init|=
name|es
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|srcFiles
init|=
name|es
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|HStoreFile
name|dst
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|merges
argument_list|,
name|newRegionInfo
operator|.
name|regionName
argument_list|,
name|colFamily
argument_list|,
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|dst
operator|.
name|mergeStoreFiles
argument_list|(
name|srcFiles
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|alreadyMerged
operator|.
name|addAll
argument_list|(
name|srcFiles
argument_list|)
expr_stmt|;
block|}
comment|// That should have taken care of the bulk of the data.
comment|// Now close the source HRegions for good, and repeat the above to take care
comment|// of any last-minute inserts
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing changes since start of merge for region "
operator|+
name|srcA
operator|.
name|getRegionName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|filesToMerge
operator|.
name|clear
argument_list|()
expr_stmt|;
for|for
control|(
name|HStoreFile
name|src
range|:
name|srcA
operator|.
name|close
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|alreadyMerged
operator|.
name|contains
argument_list|(
name|src
argument_list|)
condition|)
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|v
init|=
name|filesToMerge
operator|.
name|get
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|v
operator|==
literal|null
condition|)
block|{
name|v
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
expr_stmt|;
name|filesToMerge
operator|.
name|put
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|v
argument_list|)
expr_stmt|;
block|}
name|v
operator|.
name|add
argument_list|(
name|src
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing changes since start of merge for region "
operator|+
name|srcB
operator|.
name|getRegionName
argument_list|()
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|HStoreFile
name|src
range|:
name|srcB
operator|.
name|close
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|alreadyMerged
operator|.
name|contains
argument_list|(
name|src
argument_list|)
condition|)
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|v
init|=
name|filesToMerge
operator|.
name|get
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|v
operator|==
literal|null
condition|)
block|{
name|v
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
expr_stmt|;
name|filesToMerge
operator|.
name|put
argument_list|(
name|src
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|v
argument_list|)
expr_stmt|;
block|}
name|v
operator|.
name|add
argument_list|(
name|src
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"merging changes since start of merge"
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Text
argument_list|,
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|>
name|es
range|:
name|filesToMerge
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|Text
name|colFamily
init|=
name|es
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|srcFiles
init|=
name|es
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|HStoreFile
name|dst
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|merges
argument_list|,
name|newRegionInfo
operator|.
name|regionName
argument_list|,
name|colFamily
argument_list|,
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|dst
operator|.
name|mergeStoreFiles
argument_list|(
name|srcFiles
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
comment|// Done
name|HRegion
name|dstRegion
init|=
operator|new
name|HRegion
argument_list|(
name|rootDir
argument_list|,
name|log
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|,
name|newRegionInfo
argument_list|,
name|newRegionDir
argument_list|)
decl_stmt|;
comment|// Get rid of merges directory
name|fs
operator|.
name|delete
argument_list|(
name|merges
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"merge completed. New region is "
operator|+
name|dstRegion
operator|.
name|getRegionName
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|dstRegion
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Members
comment|//////////////////////////////////////////////////////////////////////////////
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|Long
argument_list|>
name|rowsToLocks
init|=
operator|new
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|Long
argument_list|>
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|Text
argument_list|>
name|locksToRows
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|Text
argument_list|>
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|HStore
argument_list|>
name|stores
init|=
operator|new
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|HStore
argument_list|>
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
argument_list|>
name|targetColumns
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|HMemcache
name|memcache
decl_stmt|;
name|Path
name|rootDir
decl_stmt|;
name|HLog
name|log
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
name|Configuration
name|conf
decl_stmt|;
name|HRegionInfo
name|regionInfo
decl_stmt|;
name|Path
name|regiondir
decl_stmt|;
specifier|static
class|class
name|WriteState
block|{
specifier|volatile
name|boolean
name|writesOngoing
decl_stmt|;
specifier|volatile
name|boolean
name|writesEnabled
decl_stmt|;
specifier|volatile
name|boolean
name|closed
decl_stmt|;
name|WriteState
parameter_list|()
block|{
name|this
operator|.
name|writesOngoing
operator|=
literal|true
expr_stmt|;
name|this
operator|.
name|writesEnabled
operator|=
literal|true
expr_stmt|;
name|this
operator|.
name|closed
operator|=
literal|false
expr_stmt|;
block|}
block|}
specifier|volatile
name|WriteState
name|writestate
init|=
operator|new
name|WriteState
argument_list|()
decl_stmt|;
name|int
name|recentCommits
init|=
literal|0
decl_stmt|;
specifier|volatile
name|int
name|commitsSinceFlush
init|=
literal|0
decl_stmt|;
name|int
name|maxUnflushedEntries
init|=
literal|0
decl_stmt|;
name|int
name|compactionThreshold
init|=
literal|0
decl_stmt|;
specifier|private
specifier|final
name|HLocking
name|lock
init|=
operator|new
name|HLocking
argument_list|()
decl_stmt|;
specifier|private
name|long
name|desiredMaxFileSize
decl_stmt|;
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Constructor
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * HRegion constructor.    *    * @param log The HLog is the outbound log for any updates to the HRegion    * (There's a single HLog for all the HRegions on a single HRegionServer.)    * The log file is a logfile from the previous execution that's    * custom-computed for this HRegion. The HRegionServer computes and sorts the    * appropriate log info for this HRegion. If there is a previous log file    * (implying that the HRegion has been written-to before), then read it from    * the supplied path.    *     * @param rootDir root directory for HBase instance    * @param log HLog where changes should be committed    * @param fs is the filesystem.      * @param conf is global configuration settings.    * @param regionInfo - HRegionInfo that describes the region    * @param initialFiles If there are initial files (implying that the HRegion    * is new), then read them from the supplied path.    *     * @throws IOException    */
name|HRegion
parameter_list|(
name|Path
name|rootDir
parameter_list|,
name|HLog
name|log
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|HRegionInfo
name|regionInfo
parameter_list|,
name|Path
name|initialFiles
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|rootDir
operator|=
name|rootDir
expr_stmt|;
name|this
operator|.
name|log
operator|=
name|log
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|regionInfo
operator|=
name|regionInfo
expr_stmt|;
name|this
operator|.
name|memcache
operator|=
operator|new
name|HMemcache
argument_list|()
expr_stmt|;
name|this
operator|.
name|writestate
operator|.
name|writesOngoing
operator|=
literal|true
expr_stmt|;
name|this
operator|.
name|writestate
operator|.
name|writesEnabled
operator|=
literal|true
expr_stmt|;
name|this
operator|.
name|writestate
operator|.
name|closed
operator|=
literal|false
expr_stmt|;
comment|// Declare the regionName.  This is a unique string for the region, used to
comment|// build a unique filename.
name|this
operator|.
name|regiondir
operator|=
name|HStoreFile
operator|.
name|getHRegionDir
argument_list|(
name|rootDir
argument_list|,
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|)
expr_stmt|;
name|Path
name|oldLogFile
init|=
operator|new
name|Path
argument_list|(
name|regiondir
argument_list|,
name|HREGION_OLDLOGFILE_NAME
argument_list|)
decl_stmt|;
comment|// Move prefab HStore files into place (if any)
if|if
condition|(
name|initialFiles
operator|!=
literal|null
operator|&&
name|fs
operator|.
name|exists
argument_list|(
name|initialFiles
argument_list|)
condition|)
block|{
name|fs
operator|.
name|rename
argument_list|(
name|initialFiles
argument_list|,
name|regiondir
argument_list|)
expr_stmt|;
block|}
comment|// Load in all the HStores.
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Text
argument_list|,
name|HColumnDescriptor
argument_list|>
name|e
range|:
name|this
operator|.
name|regionInfo
operator|.
name|tableDesc
operator|.
name|families
argument_list|()
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|Text
name|colFamily
init|=
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|)
decl_stmt|;
name|stores
operator|.
name|put
argument_list|(
name|colFamily
argument_list|,
operator|new
name|HStore
argument_list|(
name|rootDir
argument_list|,
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|,
name|e
operator|.
name|getValue
argument_list|()
argument_list|,
name|fs
argument_list|,
name|oldLogFile
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// Get rid of any splits or merges that were lost in-progress
name|Path
name|splits
init|=
operator|new
name|Path
argument_list|(
name|regiondir
argument_list|,
name|SPLITDIR
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|splits
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|splits
argument_list|)
expr_stmt|;
block|}
name|Path
name|merges
init|=
operator|new
name|Path
argument_list|(
name|regiondir
argument_list|,
name|MERGEDIR
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|merges
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|merges
argument_list|)
expr_stmt|;
block|}
comment|// By default, we flush the cache after 10,000 commits
name|this
operator|.
name|maxUnflushedEntries
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hregion.maxunflushed"
argument_list|,
literal|10000
argument_list|)
expr_stmt|;
comment|// By default, we compact the region if an HStore has more than 10 map files
name|this
operator|.
name|compactionThreshold
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hregion.compactionThreshold"
argument_list|,
literal|10
argument_list|)
expr_stmt|;
comment|// By default we split region if a file> DEFAULT_MAX_FILE_SIZE.
name|this
operator|.
name|desiredMaxFileSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hregion.max.filesize"
argument_list|,
name|DEFAULT_MAX_FILE_SIZE
argument_list|)
expr_stmt|;
comment|// HRegion is ready to go!
name|this
operator|.
name|writestate
operator|.
name|writesOngoing
operator|=
literal|false
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
operator|+
literal|" available"
argument_list|)
expr_stmt|;
block|}
comment|/** Returns a HRegionInfo object for this region */
name|HRegionInfo
name|getRegionInfo
parameter_list|()
block|{
return|return
name|this
operator|.
name|regionInfo
return|;
block|}
comment|/** returns true if region is closed */
name|boolean
name|isClosed
parameter_list|()
block|{
name|boolean
name|closed
init|=
literal|false
decl_stmt|;
synchronized|synchronized
init|(
name|writestate
init|)
block|{
name|closed
operator|=
name|writestate
operator|.
name|closed
expr_stmt|;
block|}
return|return
name|closed
return|;
block|}
comment|/**    * Close down this HRegion.  Flush the cache, shut down each HStore, don't     * service any more calls.    *    * The returned Vector is a list of all the storage files that the HRegion's     * component HStores make use of.  It's a list of HStoreFile objects.    *    * This method could take some time to execute, so don't call it from a     * time-sensitive thread.    */
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|boolean
name|shouldClose
init|=
literal|false
decl_stmt|;
synchronized|synchronized
init|(
name|writestate
init|)
block|{
if|if
condition|(
name|writestate
operator|.
name|closed
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
operator|+
literal|" closed"
argument_list|)
expr_stmt|;
return|return
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
return|;
block|}
while|while
condition|(
name|writestate
operator|.
name|writesOngoing
condition|)
block|{
try|try
block|{
name|writestate
operator|.
name|wait
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|iex
parameter_list|)
block|{
comment|// continue
block|}
block|}
name|writestate
operator|.
name|writesOngoing
operator|=
literal|true
expr_stmt|;
name|shouldClose
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|shouldClose
condition|)
block|{
return|return
literal|null
return|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"closing region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|)
expr_stmt|;
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|allHStoreFiles
init|=
name|internalFlushcache
argument_list|()
decl_stmt|;
for|for
control|(
name|HStore
name|store
range|:
name|stores
operator|.
name|values
argument_list|()
control|)
block|{
name|store
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
try|try
block|{
return|return
name|allHStoreFiles
return|;
block|}
finally|finally
block|{
synchronized|synchronized
init|(
name|writestate
init|)
block|{
name|writestate
operator|.
name|closed
operator|=
literal|true
expr_stmt|;
name|writestate
operator|.
name|writesOngoing
operator|=
literal|false
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
operator|+
literal|" closed"
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Split the HRegion to create two brand-new ones.  This will also close the     * current HRegion.    *    * Returns two brand-new (and open) HRegions    */
name|HRegion
index|[]
name|closeAndSplit
parameter_list|(
name|Text
name|midKey
parameter_list|,
name|RegionUnavailableListener
name|listener
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|(
operator|(
name|regionInfo
operator|.
name|startKey
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
operator|)
operator|&&
operator|(
name|regionInfo
operator|.
name|startKey
operator|.
name|compareTo
argument_list|(
name|midKey
argument_list|)
operator|>
literal|0
operator|)
operator|)
operator|||
operator|(
operator|(
name|regionInfo
operator|.
name|endKey
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
operator|)
operator|&&
operator|(
name|regionInfo
operator|.
name|endKey
operator|.
name|compareTo
argument_list|(
name|midKey
argument_list|)
operator|<
literal|0
operator|)
operator|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Region splitkey must lie within region "
operator|+
literal|"boundaries."
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Splitting region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|)
expr_stmt|;
name|Path
name|splits
init|=
operator|new
name|Path
argument_list|(
name|regiondir
argument_list|,
name|SPLITDIR
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|splits
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|splits
argument_list|)
expr_stmt|;
block|}
name|long
name|regionAId
init|=
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
decl_stmt|;
name|HRegionInfo
name|regionAInfo
init|=
operator|new
name|HRegionInfo
argument_list|(
name|regionAId
argument_list|,
name|regionInfo
operator|.
name|tableDesc
argument_list|,
name|regionInfo
operator|.
name|startKey
argument_list|,
name|midKey
argument_list|)
decl_stmt|;
name|long
name|regionBId
init|=
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
decl_stmt|;
name|HRegionInfo
name|regionBInfo
init|=
operator|new
name|HRegionInfo
argument_list|(
name|regionBId
argument_list|,
name|regionInfo
operator|.
name|tableDesc
argument_list|,
name|midKey
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|Path
name|dirA
init|=
name|HStoreFile
operator|.
name|getHRegionDir
argument_list|(
name|splits
argument_list|,
name|regionAInfo
operator|.
name|regionName
argument_list|)
decl_stmt|;
name|Path
name|dirB
init|=
name|HStoreFile
operator|.
name|getHRegionDir
argument_list|(
name|splits
argument_list|,
name|regionBInfo
operator|.
name|regionName
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dirA
argument_list|)
operator|||
name|fs
operator|.
name|exists
argument_list|(
name|dirB
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot split; target file collision at "
operator|+
name|dirA
operator|+
literal|" or "
operator|+
name|dirB
argument_list|)
throw|;
block|}
name|TreeSet
argument_list|<
name|HStoreFile
argument_list|>
name|alreadySplit
init|=
operator|new
name|TreeSet
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
comment|// Flush this HRegion out to storage, and turn off flushes
comment|// or compactions until close() is called.
comment|// TODO: flushcache can come back null if it can't do the flush. FIX.
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|hstoreFilesToSplit
init|=
name|flushcache
argument_list|(
literal|true
argument_list|)
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|hstoreFilesToSplit
control|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Splitting HStore "
operator|+
name|hsf
operator|.
name|getRegionName
argument_list|()
operator|+
literal|"/"
operator|+
name|hsf
operator|.
name|getColFamily
argument_list|()
operator|+
literal|"/"
operator|+
name|hsf
operator|.
name|fileId
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|HStoreFile
name|dstA
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|splits
argument_list|,
name|regionAInfo
operator|.
name|regionName
argument_list|,
name|hsf
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|HStoreFile
name|dstB
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|splits
argument_list|,
name|regionBInfo
operator|.
name|regionName
argument_list|,
name|hsf
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|hsf
operator|.
name|splitStoreFile
argument_list|(
name|midKey
argument_list|,
name|dstA
argument_list|,
name|dstB
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|alreadySplit
operator|.
name|add
argument_list|(
name|hsf
argument_list|)
expr_stmt|;
block|}
comment|// We just copied most of the data.
comment|// Notify the caller that we are about to close the region
name|listener
operator|.
name|closing
argument_list|(
name|this
operator|.
name|getRegionName
argument_list|()
argument_list|)
expr_stmt|;
comment|// Wait on the last row updates to come in.
name|waitOnRowLocks
argument_list|()
expr_stmt|;
comment|// Now close the HRegion
name|hstoreFilesToSplit
operator|=
name|close
argument_list|()
expr_stmt|;
comment|// Tell listener that region is now closed and that they can therefore
comment|// clean up any outstanding references.
name|listener
operator|.
name|closed
argument_list|(
name|this
operator|.
name|getRegionName
argument_list|()
argument_list|)
expr_stmt|;
comment|// Copy the small remainder
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|hstoreFilesToSplit
control|)
block|{
if|if
condition|(
operator|!
name|alreadySplit
operator|.
name|contains
argument_list|(
name|hsf
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Splitting HStore "
operator|+
name|hsf
operator|.
name|getRegionName
argument_list|()
operator|+
literal|"/"
operator|+
name|hsf
operator|.
name|getColFamily
argument_list|()
operator|+
literal|"/"
operator|+
name|hsf
operator|.
name|fileId
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|HStoreFile
name|dstA
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|splits
argument_list|,
name|regionAInfo
operator|.
name|regionName
argument_list|,
name|hsf
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|HStoreFile
name|dstB
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|splits
argument_list|,
name|regionBInfo
operator|.
name|regionName
argument_list|,
name|hsf
operator|.
name|getColFamily
argument_list|()
argument_list|,
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|hsf
operator|.
name|splitStoreFile
argument_list|(
name|midKey
argument_list|,
name|dstA
argument_list|,
name|dstB
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Done
name|HRegion
name|regionA
init|=
operator|new
name|HRegion
argument_list|(
name|rootDir
argument_list|,
name|log
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|,
name|regionAInfo
argument_list|,
name|dirA
argument_list|)
decl_stmt|;
name|HRegion
name|regionB
init|=
operator|new
name|HRegion
argument_list|(
name|rootDir
argument_list|,
name|log
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|,
name|regionBInfo
argument_list|,
name|dirB
argument_list|)
decl_stmt|;
comment|// Cleanup
name|fs
operator|.
name|delete
argument_list|(
name|splits
argument_list|)
expr_stmt|;
comment|// Get rid of splits directory
name|fs
operator|.
name|delete
argument_list|(
name|regiondir
argument_list|)
expr_stmt|;
comment|// and the directory for the old region
name|HRegion
name|regions
index|[]
init|=
operator|new
name|HRegion
index|[
literal|2
index|]
decl_stmt|;
name|regions
index|[
literal|0
index|]
operator|=
name|regionA
expr_stmt|;
name|regions
index|[
literal|1
index|]
operator|=
name|regionB
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Region split of "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
operator|+
literal|" complete. "
operator|+
literal|"New regions are: "
operator|+
name|regions
index|[
literal|0
index|]
operator|.
name|getRegionName
argument_list|()
operator|+
literal|", "
operator|+
name|regions
index|[
literal|1
index|]
operator|.
name|getRegionName
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|regions
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// HRegion accessors
comment|//////////////////////////////////////////////////////////////////////////////
name|Text
name|getStartKey
parameter_list|()
block|{
return|return
name|regionInfo
operator|.
name|startKey
return|;
block|}
name|Text
name|getEndKey
parameter_list|()
block|{
return|return
name|regionInfo
operator|.
name|endKey
return|;
block|}
name|long
name|getRegionId
parameter_list|()
block|{
return|return
name|regionInfo
operator|.
name|regionId
return|;
block|}
name|Text
name|getRegionName
parameter_list|()
block|{
return|return
name|regionInfo
operator|.
name|regionName
return|;
block|}
name|Path
name|getRootDir
parameter_list|()
block|{
return|return
name|rootDir
return|;
block|}
name|HTableDescriptor
name|getTableDesc
parameter_list|()
block|{
return|return
name|regionInfo
operator|.
name|tableDesc
return|;
block|}
name|HLog
name|getLog
parameter_list|()
block|{
return|return
name|log
return|;
block|}
name|Configuration
name|getConf
parameter_list|()
block|{
return|return
name|conf
return|;
block|}
name|Path
name|getRegionDir
parameter_list|()
block|{
return|return
name|regiondir
return|;
block|}
name|FileSystem
name|getFilesystem
parameter_list|()
block|{
return|return
name|fs
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// HRegion maintenance.
comment|//
comment|// These methods are meant to be called periodically by the HRegionServer for
comment|// upkeep.
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Iterates through all the HStores and finds the one with the largest MapFile    * size. If the size is greater than the (currently hard-coded) threshold,    * returns true indicating that the region should be split. The midKey for the    * largest MapFile is returned through the midKey parameter.    *     * @param midKey      - (return value) midKey of the largest MapFile    * @return            - true if the region should be split    */
name|boolean
name|needsSplit
parameter_list|(
name|Text
name|midKey
parameter_list|)
block|{
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Text
name|key
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
name|long
name|maxSize
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HStore
name|store
range|:
name|stores
operator|.
name|values
argument_list|()
control|)
block|{
name|long
name|size
init|=
name|store
operator|.
name|getLargestFileSize
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// Largest so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|midKey
operator|.
name|set
argument_list|(
name|key
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|(
name|maxSize
operator|>
operator|(
name|this
operator|.
name|desiredMaxFileSize
operator|+
operator|(
name|this
operator|.
name|desiredMaxFileSize
operator|/
literal|2
operator|)
operator|)
operator|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return - returns the size of the largest HStore    */
name|long
name|largestHStore
parameter_list|()
block|{
name|long
name|maxsize
init|=
literal|0
decl_stmt|;
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Text
name|key
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
for|for
control|(
name|HStore
name|h
range|:
name|stores
operator|.
name|values
argument_list|()
control|)
block|{
name|long
name|size
init|=
name|h
operator|.
name|getLargestFileSize
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxsize
condition|)
block|{
comment|// Largest so far
name|maxsize
operator|=
name|size
expr_stmt|;
block|}
block|}
return|return
name|maxsize
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return true if the region should be compacted.    */
name|boolean
name|needsCompaction
parameter_list|()
block|{
name|boolean
name|needsCompaction
init|=
literal|false
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
for|for
control|(
name|HStore
name|store
range|:
name|stores
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
name|store
operator|.
name|getNMaps
argument_list|()
operator|>
name|this
operator|.
name|compactionThreshold
condition|)
block|{
name|needsCompaction
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
return|return
name|needsCompaction
return|;
block|}
comment|/**    * Compact all the stores.  This should be called periodically to make sure     * the stores are kept manageable.      *    * This operation could block for a long time, so don't call it from a     * time-sensitive thread.    *    * If it returns TRUE, the compaction has completed.    *     * If it returns FALSE, the compaction was not carried out, because the     * HRegion is busy doing something else storage-intensive (like flushing the     * cache).  The caller should check back later.    */
name|boolean
name|compactStores
parameter_list|()
throws|throws
name|IOException
block|{
name|boolean
name|shouldCompact
init|=
literal|false
decl_stmt|;
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
synchronized|synchronized
init|(
name|writestate
init|)
block|{
if|if
condition|(
operator|(
operator|!
name|writestate
operator|.
name|writesOngoing
operator|)
operator|&&
name|writestate
operator|.
name|writesEnabled
operator|&&
operator|(
operator|!
name|writestate
operator|.
name|closed
operator|)
operator|&&
name|recentCommits
operator|>
name|MIN_COMMITS_FOR_COMPACTION
condition|)
block|{
name|writestate
operator|.
name|writesOngoing
operator|=
literal|true
expr_stmt|;
name|shouldCompact
operator|=
literal|true
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|shouldCompact
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"not compacting region "
operator|+
name|this
operator|.
name|regionInfo
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"starting compaction on region "
operator|+
name|this
operator|.
name|regionInfo
argument_list|)
expr_stmt|;
for|for
control|(
name|HStore
name|store
range|:
name|stores
operator|.
name|values
argument_list|()
control|)
block|{
name|store
operator|.
name|compact
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"compaction completed on region "
operator|+
name|this
operator|.
name|regionInfo
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
synchronized|synchronized
init|(
name|writestate
init|)
block|{
name|writestate
operator|.
name|writesOngoing
operator|=
literal|false
expr_stmt|;
name|recentCommits
operator|=
literal|0
expr_stmt|;
name|writestate
operator|.
name|notifyAll
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Each HRegion is given a periodic chance to flush the cache, which it should    * only take if there have been a lot of uncommitted writes.    */
name|void
name|optionallyFlush
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|commitsSinceFlush
operator|>
name|maxUnflushedEntries
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Flushing cache. Number of commits is: "
operator|+
name|commitsSinceFlush
argument_list|)
expr_stmt|;
block|}
name|flushcache
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Flush the cache.  This is called periodically to minimize the amount of log    * processing needed upon startup.    *     * The returned Vector is a list of all the files used by the component HStores.    * It is a list of HStoreFile objects.  If the returned value is NULL, then the    * flush could not be executed, because the HRegion is busy doing something    * else storage-intensive.  The caller should check back later.    *    * The 'disableFutureWrites' boolean indicates that the caller intends to     * close() the HRegion shortly, so the HRegion should not take on any new and     * potentially long-lasting disk operations.  This flush() should be the final    * pre-close() disk operation.    *    * This method may block for some time, so it should not be called from a     * time-sensitive thread.    */
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|flushcache
parameter_list|(
name|boolean
name|disableFutureWrites
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|shouldFlush
init|=
literal|false
decl_stmt|;
synchronized|synchronized
init|(
name|writestate
init|)
block|{
if|if
condition|(
operator|(
operator|!
name|writestate
operator|.
name|writesOngoing
operator|)
operator|&&
name|writestate
operator|.
name|writesEnabled
operator|&&
operator|(
operator|!
name|writestate
operator|.
name|closed
operator|)
condition|)
block|{
name|writestate
operator|.
name|writesOngoing
operator|=
literal|true
expr_stmt|;
name|shouldFlush
operator|=
literal|true
expr_stmt|;
if|if
condition|(
name|disableFutureWrites
condition|)
block|{
name|writestate
operator|.
name|writesEnabled
operator|=
literal|false
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
operator|!
name|shouldFlush
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"not flushing cache for region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
try|try
block|{
return|return
name|internalFlushcache
argument_list|()
return|;
block|}
finally|finally
block|{
synchronized|synchronized
init|(
name|writestate
init|)
block|{
name|writestate
operator|.
name|writesOngoing
operator|=
literal|false
expr_stmt|;
name|writestate
operator|.
name|notifyAll
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Flushing the cache is a little tricky. We have a lot of updates in the    * HMemcache, all of which have also been written to the log. We need to write    * those updates in the HMemcache out to disk, while being able to process    * reads/writes as much as possible during the flush operation. Also, the log    * has to state clearly the point in time at which the HMemcache was flushed.    * (That way, during recovery, we know when we can rely on the on-disk flushed    * structures and when we have to recover the HMemcache from the log.)    *     * So, we have a three-step process:    *     * A. Flush the memcache to the on-disk stores, noting the current sequence ID    * for the log.    *     * B. Write a FLUSHCACHE-COMPLETE message to the log, using the sequence ID    * that was current at the time of memcache-flush.    *     * C. Get rid of the memcache structures that are now redundant, as they've    * been flushed to the on-disk HStores.    *     * This method is protected, but can be accessed via several public routes.    *     * This method may block for some time.    */
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|internalFlushcache
parameter_list|()
throws|throws
name|IOException
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|allHStoreFiles
init|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing cache for region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|)
expr_stmt|;
block|}
comment|// We pass the log to the HMemcache, so we can lock down
comment|// both simultaneously.  We only have to do this for a moment:
comment|// we need the HMemcache state at the time of a known log sequence
comment|// number.  Since multiple HRegions may write to a single HLog,
comment|// the sequence numbers may zoom past unless we lock it.
comment|//
comment|// When execution returns from snapshotMemcacheForLog()
comment|// with a non-NULL value, the HMemcache will have a snapshot
comment|// object stored that must be explicitly cleaned up using
comment|// a call to deleteSnapshot().
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"starting memcache snapshot"
argument_list|)
expr_stmt|;
block|}
name|HMemcache
operator|.
name|Snapshot
name|retval
init|=
name|memcache
operator|.
name|snapshotMemcacheForLog
argument_list|(
name|log
argument_list|)
decl_stmt|;
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|BytesWritable
argument_list|>
name|memcacheSnapshot
init|=
name|retval
operator|.
name|memcacheSnapshot
decl_stmt|;
if|if
condition|(
name|memcacheSnapshot
operator|==
literal|null
condition|)
block|{
for|for
control|(
name|HStore
name|hstore
range|:
name|stores
operator|.
name|values
argument_list|()
control|)
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|hstoreFiles
init|=
name|hstore
operator|.
name|getAllMapFiles
argument_list|()
decl_stmt|;
name|allHStoreFiles
operator|.
name|addAll
argument_list|(
literal|0
argument_list|,
name|hstoreFiles
argument_list|)
expr_stmt|;
block|}
return|return
name|allHStoreFiles
return|;
block|}
name|long
name|logCacheFlushId
init|=
name|retval
operator|.
name|sequenceId
decl_stmt|;
comment|// A.  Flush memcache to all the HStores.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing memcache to HStores"
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Iterator
argument_list|<
name|HStore
argument_list|>
name|it
init|=
name|stores
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStore
name|hstore
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|hstoreFiles
init|=
name|hstore
operator|.
name|flushCache
argument_list|(
name|memcacheSnapshot
argument_list|,
name|logCacheFlushId
argument_list|)
decl_stmt|;
name|allHStoreFiles
operator|.
name|addAll
argument_list|(
literal|0
argument_list|,
name|hstoreFiles
argument_list|)
expr_stmt|;
block|}
comment|// B.  Write a FLUSHCACHE-COMPLETE message to the log.
comment|//     This tells future readers that the HStores were emitted correctly,
comment|//     and that all updates to the log for this regionName that have lower
comment|//     log-sequence-ids can be safely ignored.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"writing flush cache complete to log"
argument_list|)
expr_stmt|;
block|}
name|log
operator|.
name|completeCacheFlush
argument_list|(
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|,
name|regionInfo
operator|.
name|tableDesc
operator|.
name|getName
argument_list|()
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
comment|// C. Delete the now-irrelevant memcache snapshot; its contents have been
comment|//    dumped to disk-based HStores.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"deleting memcache snapshot"
argument_list|)
expr_stmt|;
block|}
name|memcache
operator|.
name|deleteSnapshot
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"cache flush complete for region "
operator|+
name|this
operator|.
name|regionInfo
operator|.
name|regionName
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|commitsSinceFlush
operator|=
literal|0
expr_stmt|;
return|return
name|allHStoreFiles
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// get() methods for client use.
comment|//////////////////////////////////////////////////////////////////////////////
comment|/** Fetch a single data item. */
name|BytesWritable
name|get
parameter_list|(
name|Text
name|row
parameter_list|,
name|Text
name|column
parameter_list|)
throws|throws
name|IOException
block|{
name|BytesWritable
index|[]
name|results
init|=
name|get
argument_list|(
name|row
argument_list|,
name|column
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|,
literal|1
argument_list|)
decl_stmt|;
return|return
operator|(
name|results
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|results
index|[
literal|0
index|]
return|;
block|}
comment|/** Fetch multiple versions of a single data item */
name|BytesWritable
index|[]
name|get
parameter_list|(
name|Text
name|row
parameter_list|,
name|Text
name|column
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|get
argument_list|(
name|row
argument_list|,
name|column
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|,
name|numVersions
argument_list|)
return|;
block|}
comment|/** Fetch multiple versions of a single data item, with timestamp. */
name|BytesWritable
index|[]
name|get
parameter_list|(
name|Text
name|row
parameter_list|,
name|Text
name|column
parameter_list|,
name|long
name|timestamp
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|writestate
operator|.
name|closed
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"HRegion is closed."
argument_list|)
throw|;
block|}
comment|// Make sure this is a valid row and valid column
name|checkRow
argument_list|(
name|row
argument_list|)
expr_stmt|;
name|checkColumn
argument_list|(
name|column
argument_list|)
expr_stmt|;
comment|// Obtain the row-lock
name|obtainRowLock
argument_list|(
name|row
argument_list|)
expr_stmt|;
try|try
block|{
comment|// Obtain the -col results
return|return
name|get
argument_list|(
operator|new
name|HStoreKey
argument_list|(
name|row
argument_list|,
name|column
argument_list|,
name|timestamp
argument_list|)
argument_list|,
name|numVersions
argument_list|)
return|;
block|}
finally|finally
block|{
name|releaseRowLock
argument_list|(
name|row
argument_list|)
expr_stmt|;
block|}
block|}
comment|/** Private implementation: get the value for the indicated HStoreKey */
specifier|private
name|BytesWritable
index|[]
name|get
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Check the memcache
name|BytesWritable
index|[]
name|result
init|=
name|memcache
operator|.
name|get
argument_list|(
name|key
argument_list|,
name|numVersions
argument_list|)
decl_stmt|;
if|if
condition|(
name|result
operator|!=
literal|null
condition|)
block|{
return|return
name|result
return|;
block|}
comment|// If unavailable in memcache, check the appropriate HStore
name|Text
name|colFamily
init|=
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|key
operator|.
name|getColumn
argument_list|()
argument_list|)
decl_stmt|;
name|HStore
name|targetStore
init|=
name|stores
operator|.
name|get
argument_list|(
name|colFamily
argument_list|)
decl_stmt|;
if|if
condition|(
name|targetStore
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|targetStore
operator|.
name|get
argument_list|(
name|key
argument_list|,
name|numVersions
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Fetch all the columns for the indicated row.    * Returns a TreeMap that maps column names to values.    *    * We should eventually use Bloom filters here, to reduce running time.  If     * the database has many column families and is very sparse, then we could be     * checking many files needlessly.  A small Bloom for each row would help us     * determine which column groups are useful for that row.  That would let us     * avoid a bunch of disk activity.    */
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
name|getFull
parameter_list|(
name|Text
name|row
parameter_list|)
throws|throws
name|IOException
block|{
name|HStoreKey
name|key
init|=
operator|new
name|HStoreKey
argument_list|(
name|row
argument_list|,
name|System
operator|.
name|currentTimeMillis
argument_list|()
argument_list|)
decl_stmt|;
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
name|memResult
init|=
name|memcache
operator|.
name|getFull
argument_list|(
name|key
argument_list|)
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|Text
argument_list|>
name|it
init|=
name|stores
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|Text
name|colFamily
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|HStore
name|targetStore
init|=
name|stores
operator|.
name|get
argument_list|(
name|colFamily
argument_list|)
decl_stmt|;
name|targetStore
operator|.
name|getFull
argument_list|(
name|key
argument_list|,
name|memResult
argument_list|)
expr_stmt|;
block|}
return|return
name|memResult
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Return an iterator that scans over the HRegion, returning the indicated     * columns.  This Iterator must be closed by the caller.    */
name|HInternalScannerInterface
name|getScanner
parameter_list|(
name|Text
index|[]
name|cols
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|TreeSet
argument_list|<
name|Text
argument_list|>
name|families
init|=
operator|new
name|TreeSet
argument_list|<
name|Text
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|cols
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|families
operator|.
name|add
argument_list|(
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|cols
index|[
name|i
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|HStore
index|[]
name|storelist
init|=
operator|new
name|HStore
index|[
name|families
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Text
name|family
range|:
name|families
control|)
block|{
name|storelist
index|[
name|i
operator|++
index|]
operator|=
name|stores
operator|.
name|get
argument_list|(
name|family
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|HScanner
argument_list|(
name|cols
argument_list|,
name|firstRow
argument_list|,
name|memcache
argument_list|,
name|storelist
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// set() methods for client use.
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * The caller wants to apply a series of writes to a single row in the    * HRegion. The caller will invoke startUpdate(), followed by a series of    * calls to put/delete, then finally either abort() or commit().    *    *<p>Note that we rely on the external caller to properly abort() or    * commit() every transaction.  If the caller is a network client, there    * should be a lease-system in place that automatically aborts() transactions    * after a specified quiet period.    *     * @param row Row to update    * @return lockid    * @see #put(long, Text, BytesWritable)    */
name|long
name|startUpdate
parameter_list|(
name|Text
name|row
parameter_list|)
throws|throws
name|IOException
block|{
comment|// We obtain a per-row lock, so other clients will block while one client
comment|// performs an update.  The read lock is released by the client calling
comment|// #commit or #abort or if the HRegionServer lease on the lock expires.
comment|// See HRegionServer#RegionListener for how the expire on HRegionServer
comment|// invokes a HRegion#abort.
return|return
name|obtainRowLock
argument_list|(
name|row
argument_list|)
return|;
block|}
comment|/**    * Put a cell value into the locked row.  The user indicates the row-lock, the    * target column, and the desired value.  This stuff is set into a temporary     * memory area until the user commits the change, at which point it's logged     * and placed into the memcache.    *    * This method really just tests the input, then calls an internal localput()     * method.    */
name|void
name|put
parameter_list|(
name|long
name|lockid
parameter_list|,
name|Text
name|targetCol
parameter_list|,
name|BytesWritable
name|val
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|val
operator|.
name|getSize
argument_list|()
operator|==
name|DELETE_BYTES
operator|.
name|getSize
argument_list|()
operator|&&
name|val
operator|.
name|compareTo
argument_list|(
name|DELETE_BYTES
argument_list|)
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot insert value: "
operator|+
name|val
argument_list|)
throw|;
block|}
name|localput
argument_list|(
name|lockid
argument_list|,
name|targetCol
argument_list|,
name|val
argument_list|)
expr_stmt|;
block|}
comment|/**    * Delete a value or write a value. This is a just a convenience method for put().    */
name|void
name|delete
parameter_list|(
name|long
name|lockid
parameter_list|,
name|Text
name|targetCol
parameter_list|)
throws|throws
name|IOException
block|{
name|localput
argument_list|(
name|lockid
argument_list|,
name|targetCol
argument_list|,
name|DELETE_BYTES
argument_list|)
expr_stmt|;
block|}
comment|/**    * Private implementation.    *     * localput() is used for both puts and deletes. We just place the values    * into a per-row pending area, until a commit() or abort() call is received.    * (Or until the user's write-lock expires.)    *     * @param lockid    * @param targetCol    * @param val Value to enter into cell    * @throws IOException    */
name|void
name|localput
parameter_list|(
specifier|final
name|long
name|lockid
parameter_list|,
specifier|final
name|Text
name|targetCol
parameter_list|,
specifier|final
name|BytesWritable
name|val
parameter_list|)
throws|throws
name|IOException
block|{
name|checkColumn
argument_list|(
name|targetCol
argument_list|)
expr_stmt|;
name|Text
name|row
init|=
name|getRowFromLock
argument_list|(
name|lockid
argument_list|)
decl_stmt|;
if|if
condition|(
name|row
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|LockException
argument_list|(
literal|"No write lock for lockid "
operator|+
name|lockid
argument_list|)
throw|;
block|}
comment|// This sync block makes localput() thread-safe when multiple
comment|// threads from the same client attempt an insert on the same
comment|// locked row (via lockid).
synchronized|synchronized
init|(
name|row
init|)
block|{
comment|// This check makes sure that another thread from the client
comment|// hasn't aborted/committed the write-operation.
if|if
condition|(
name|row
operator|!=
name|getRowFromLock
argument_list|(
name|lockid
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|LockException
argument_list|(
literal|"Locking error: put operation on lock "
operator|+
name|lockid
operator|+
literal|" unexpected aborted by another thread"
argument_list|)
throw|;
block|}
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
name|targets
init|=
name|targetColumns
operator|.
name|get
argument_list|(
name|lockid
argument_list|)
decl_stmt|;
if|if
condition|(
name|targets
operator|==
literal|null
condition|)
block|{
name|targets
operator|=
operator|new
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
argument_list|()
expr_stmt|;
name|targetColumns
operator|.
name|put
argument_list|(
name|lockid
argument_list|,
name|targets
argument_list|)
expr_stmt|;
block|}
name|targets
operator|.
name|put
argument_list|(
name|targetCol
argument_list|,
name|val
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Abort a pending set of writes. This dumps from memory all in-progress    * writes associated with the given row-lock.  These values have not yet    * been placed in memcache or written to the log.    */
name|void
name|abort
parameter_list|(
name|long
name|lockid
parameter_list|)
throws|throws
name|IOException
block|{
name|Text
name|row
init|=
name|getRowFromLock
argument_list|(
name|lockid
argument_list|)
decl_stmt|;
if|if
condition|(
name|row
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|LockException
argument_list|(
literal|"No write lock for lockid "
operator|+
name|lockid
argument_list|)
throw|;
block|}
comment|// This sync block makes abort() thread-safe when multiple
comment|// threads from the same client attempt to operate on the same
comment|// locked row (via lockid).
synchronized|synchronized
init|(
name|row
init|)
block|{
comment|// This check makes sure another thread from the client
comment|// hasn't aborted/committed the write-operation.
if|if
condition|(
name|row
operator|!=
name|getRowFromLock
argument_list|(
name|lockid
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|LockException
argument_list|(
literal|"Locking error: abort() operation on lock "
operator|+
name|lockid
operator|+
literal|" unexpected aborted by another thread"
argument_list|)
throw|;
block|}
name|targetColumns
operator|.
name|remove
argument_list|(
name|lockid
argument_list|)
expr_stmt|;
name|releaseRowLock
argument_list|(
name|row
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Commit a pending set of writes to the memcache. This also results in    * writing to the change log.    *    * Once updates hit the change log, they are safe.  They will either be moved     * into an HStore in the future, or they will be recovered from the log.    * @param lockid Lock for row we're to commit.    * @throws IOException    */
name|void
name|commit
parameter_list|(
specifier|final
name|long
name|lockid
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Remove the row from the pendingWrites list so
comment|// that repeated executions won't screw this up.
name|Text
name|row
init|=
name|getRowFromLock
argument_list|(
name|lockid
argument_list|)
decl_stmt|;
if|if
condition|(
name|row
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|LockException
argument_list|(
literal|"No write lock for lockid "
operator|+
name|lockid
argument_list|)
throw|;
block|}
comment|// This check makes sure that another thread from the client
comment|// hasn't aborted/committed the write-operation
synchronized|synchronized
init|(
name|row
init|)
block|{
comment|// Add updates to the log and add values to the memcache.
name|long
name|commitTimestamp
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|log
operator|.
name|append
argument_list|(
name|regionInfo
operator|.
name|regionName
argument_list|,
name|regionInfo
operator|.
name|tableDesc
operator|.
name|getName
argument_list|()
argument_list|,
name|row
argument_list|,
name|targetColumns
operator|.
name|get
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|lockid
argument_list|)
argument_list|)
argument_list|,
name|commitTimestamp
argument_list|)
expr_stmt|;
name|memcache
operator|.
name|add
argument_list|(
name|row
argument_list|,
name|targetColumns
operator|.
name|get
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|lockid
argument_list|)
argument_list|)
argument_list|,
name|commitTimestamp
argument_list|)
expr_stmt|;
comment|// OK, all done!
name|targetColumns
operator|.
name|remove
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|lockid
argument_list|)
argument_list|)
expr_stmt|;
name|releaseRowLock
argument_list|(
name|row
argument_list|)
expr_stmt|;
block|}
name|recentCommits
operator|++
expr_stmt|;
name|this
operator|.
name|commitsSinceFlush
operator|++
expr_stmt|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Support code
comment|//////////////////////////////////////////////////////////////////////////////
comment|/** Make sure this is a valid row for the HRegion */
name|void
name|checkRow
parameter_list|(
name|Text
name|row
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|(
operator|(
name|regionInfo
operator|.
name|startKey
operator|.
name|getLength
argument_list|()
operator|==
literal|0
operator|)
operator|||
operator|(
name|regionInfo
operator|.
name|startKey
operator|.
name|compareTo
argument_list|(
name|row
argument_list|)
operator|<=
literal|0
operator|)
operator|)
operator|&&
operator|(
operator|(
name|regionInfo
operator|.
name|endKey
operator|.
name|getLength
argument_list|()
operator|==
literal|0
operator|)
operator|||
operator|(
name|regionInfo
operator|.
name|endKey
operator|.
name|compareTo
argument_list|(
name|row
argument_list|)
operator|>
literal|0
operator|)
operator|)
condition|)
block|{
comment|// all's well
block|}
else|else
block|{
throw|throw
operator|new
name|WrongRegionException
argument_list|(
literal|"Requested row out of range for "
operator|+
literal|"HRegion "
operator|+
name|regionInfo
operator|.
name|regionName
operator|+
literal|", startKey='"
operator|+
name|regionInfo
operator|.
name|startKey
operator|+
literal|"', endKey='"
operator|+
name|regionInfo
operator|.
name|endKey
operator|+
literal|"', row='"
operator|+
name|row
operator|+
literal|"'"
argument_list|)
throw|;
block|}
block|}
comment|/** Make sure this is a valid column for the current table */
name|void
name|checkColumn
parameter_list|(
name|Text
name|columnName
parameter_list|)
throws|throws
name|IOException
block|{
name|Text
name|family
init|=
operator|new
name|Text
argument_list|(
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|columnName
argument_list|)
operator|+
literal|":"
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|regionInfo
operator|.
name|tableDesc
operator|.
name|hasFamily
argument_list|(
name|family
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Requested column family "
operator|+
name|family
operator|+
literal|" does not exist in HRegion "
operator|+
name|regionInfo
operator|.
name|regionName
operator|+
literal|" for table "
operator|+
name|regionInfo
operator|.
name|tableDesc
operator|.
name|getName
argument_list|()
argument_list|)
throw|;
block|}
block|}
comment|/**    * Obtain a lock on the given row.  Blocks until success.    *    * I know it's strange to have two mappings:    *<pre>    *   ROWS  ==> LOCKS    *</pre>    * as well as    *<pre>    *   LOCKS ==> ROWS    *</pre>    *    * But it acts as a guard on the client; a miswritten client just can't    * submit the name of a row and start writing to it; it must know the correct    * lockid, which matches the lock list in memory.    *     *<p>It would be more memory-efficient to assume a correctly-written client,     * which maybe we'll do in the future.    *     * @param row Name of row to lock.    * @return The id of the held lock.    */
name|long
name|obtainRowLock
parameter_list|(
name|Text
name|row
parameter_list|)
throws|throws
name|IOException
block|{
name|checkRow
argument_list|(
name|row
argument_list|)
expr_stmt|;
synchronized|synchronized
init|(
name|rowsToLocks
init|)
block|{
while|while
condition|(
name|rowsToLocks
operator|.
name|get
argument_list|(
name|row
argument_list|)
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|rowsToLocks
operator|.
name|wait
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|ie
parameter_list|)
block|{
comment|// Empty
block|}
block|}
name|long
name|lockid
init|=
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextLong
argument_list|()
argument_list|)
decl_stmt|;
name|rowsToLocks
operator|.
name|put
argument_list|(
name|row
argument_list|,
name|lockid
argument_list|)
expr_stmt|;
name|locksToRows
operator|.
name|put
argument_list|(
name|lockid
argument_list|,
name|row
argument_list|)
expr_stmt|;
name|rowsToLocks
operator|.
name|notifyAll
argument_list|()
expr_stmt|;
return|return
name|lockid
return|;
block|}
block|}
name|Text
name|getRowFromLock
parameter_list|(
name|long
name|lockid
parameter_list|)
block|{
comment|// Pattern is that all access to rowsToLocks and/or to
comment|// locksToRows is via a lock on rowsToLocks.
synchronized|synchronized
init|(
name|rowsToLocks
init|)
block|{
return|return
name|locksToRows
operator|.
name|get
argument_list|(
name|lockid
argument_list|)
return|;
block|}
block|}
comment|/**     * Release the row lock!    * @param lock Name of row whose lock we are to release    */
name|void
name|releaseRowLock
parameter_list|(
name|Text
name|row
parameter_list|)
block|{
synchronized|synchronized
init|(
name|rowsToLocks
init|)
block|{
name|long
name|lockid
init|=
name|rowsToLocks
operator|.
name|remove
argument_list|(
name|row
argument_list|)
operator|.
name|longValue
argument_list|()
decl_stmt|;
name|locksToRows
operator|.
name|remove
argument_list|(
name|lockid
argument_list|)
expr_stmt|;
name|rowsToLocks
operator|.
name|notifyAll
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|waitOnRowLocks
parameter_list|()
block|{
synchronized|synchronized
init|(
name|this
operator|.
name|rowsToLocks
init|)
block|{
while|while
condition|(
name|this
operator|.
name|rowsToLocks
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
try|try
block|{
name|this
operator|.
name|rowsToLocks
operator|.
name|wait
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
comment|// Catch. Let while test determine loop-end.
block|}
block|}
block|}
block|}
comment|/**    * HScanner is an iterator through a bunch of rows in an HRegion.    */
specifier|private
specifier|static
class|class
name|HScanner
implements|implements
name|HInternalScannerInterface
block|{
specifier|private
name|HInternalScannerInterface
index|[]
name|scanners
decl_stmt|;
specifier|private
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
index|[]
name|resultSets
decl_stmt|;
specifier|private
name|HStoreKey
index|[]
name|keys
decl_stmt|;
specifier|private
name|boolean
name|wildcardMatch
decl_stmt|;
specifier|private
name|boolean
name|multipleMatchers
decl_stmt|;
comment|/** Create an HScanner with a handle on many HStores. */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
name|HScanner
parameter_list|(
name|Text
index|[]
name|cols
parameter_list|,
name|Text
name|firstRow
parameter_list|,
name|HMemcache
name|memcache
parameter_list|,
name|HStore
index|[]
name|stores
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|scanTime
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|this
operator|.
name|scanners
operator|=
operator|new
name|HInternalScannerInterface
index|[
name|stores
operator|.
name|length
operator|+
literal|1
index|]
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|this
operator|.
name|scanners
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|this
operator|.
name|scanners
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
block|}
name|this
operator|.
name|resultSets
operator|=
operator|new
name|TreeMap
index|[
name|scanners
operator|.
name|length
index|]
expr_stmt|;
name|this
operator|.
name|keys
operator|=
operator|new
name|HStoreKey
index|[
name|scanners
operator|.
name|length
index|]
expr_stmt|;
name|this
operator|.
name|wildcardMatch
operator|=
literal|false
expr_stmt|;
name|this
operator|.
name|multipleMatchers
operator|=
literal|false
expr_stmt|;
comment|// Advance to the first key in each store.
comment|// All results will match the required column-set and scanTime.
comment|// NOTE: the memcache scanner should be the first scanner
try|try
block|{
name|HInternalScannerInterface
name|scanner
init|=
name|memcache
operator|.
name|getScanner
argument_list|(
name|scanTime
argument_list|,
name|cols
argument_list|,
name|firstRow
argument_list|)
decl_stmt|;
if|if
condition|(
name|scanner
operator|.
name|isWildcardScanner
argument_list|()
condition|)
block|{
name|this
operator|.
name|wildcardMatch
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
name|scanner
operator|.
name|isMultipleMatchScanner
argument_list|()
condition|)
block|{
name|this
operator|.
name|multipleMatchers
operator|=
literal|true
expr_stmt|;
block|}
name|scanners
index|[
literal|0
index|]
operator|=
name|scanner
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|stores
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|scanner
operator|=
name|stores
index|[
name|i
index|]
operator|.
name|getScanner
argument_list|(
name|scanTime
argument_list|,
name|cols
argument_list|,
name|firstRow
argument_list|)
expr_stmt|;
if|if
condition|(
name|scanner
operator|.
name|isWildcardScanner
argument_list|()
condition|)
block|{
name|this
operator|.
name|wildcardMatch
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
name|scanner
operator|.
name|isMultipleMatchScanner
argument_list|()
condition|)
block|{
name|this
operator|.
name|multipleMatchers
operator|=
literal|true
expr_stmt|;
block|}
name|scanners
index|[
name|i
operator|+
literal|1
index|]
operator|=
name|scanner
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|this
operator|.
name|scanners
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|scanners
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
name|closeScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
throw|throw
name|e
throw|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|scanners
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|keys
index|[
name|i
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
name|resultSets
index|[
name|i
index|]
operator|=
operator|new
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
argument_list|()
expr_stmt|;
if|if
condition|(
name|scanners
index|[
name|i
index|]
operator|!=
literal|null
operator|&&
operator|!
name|scanners
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|resultSets
index|[
name|i
index|]
argument_list|)
condition|)
block|{
name|closeScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/* (non-Javadoc)      * @see org.apache.hadoop.hbase.HInternalScannerInterface#isWildcardScanner()      */
specifier|public
name|boolean
name|isWildcardScanner
parameter_list|()
block|{
return|return
name|wildcardMatch
return|;
block|}
comment|/* (non-Javadoc)      * @see org.apache.hadoop.hbase.HInternalScannerInterface#isMultipleMatchScanner()      */
specifier|public
name|boolean
name|isMultipleMatchScanner
parameter_list|()
block|{
return|return
name|multipleMatchers
return|;
block|}
comment|/* (non-Javadoc)      *       * Grab the next row's worth of values.  The HScanner will return the most       * recent data value for each row that is not newer than the target time.      *      * @see org.apache.hadoop.hbase.HInternalScannerInterface#next(org.apache.hadoop.hbase.HStoreKey, java.util.TreeMap)      */
specifier|public
name|boolean
name|next
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Find the lowest-possible key.
name|Text
name|chosenRow
init|=
literal|null
decl_stmt|;
name|long
name|chosenTimestamp
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|this
operator|.
name|keys
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|scanners
index|[
name|i
index|]
operator|!=
literal|null
operator|&&
operator|(
name|chosenRow
operator|==
literal|null
operator|||
operator|(
name|keys
index|[
name|i
index|]
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|chosenRow
argument_list|)
operator|<
literal|0
operator|)
operator|||
operator|(
operator|(
name|keys
index|[
name|i
index|]
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|chosenRow
argument_list|)
operator|==
literal|0
operator|)
operator|&&
operator|(
name|keys
index|[
name|i
index|]
operator|.
name|getTimestamp
argument_list|()
operator|>
name|chosenTimestamp
operator|)
operator|)
operator|)
condition|)
block|{
name|chosenRow
operator|=
operator|new
name|Text
argument_list|(
name|keys
index|[
name|i
index|]
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|chosenTimestamp
operator|=
name|keys
index|[
name|i
index|]
operator|.
name|getTimestamp
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Store the key and results for each sub-scanner. Merge them as appropriate.
name|boolean
name|insertedItem
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|chosenTimestamp
operator|>
literal|0
condition|)
block|{
name|key
operator|.
name|setRow
argument_list|(
name|chosenRow
argument_list|)
expr_stmt|;
name|key
operator|.
name|setVersion
argument_list|(
name|chosenTimestamp
argument_list|)
expr_stmt|;
name|key
operator|.
name|setColumn
argument_list|(
operator|new
name|Text
argument_list|(
literal|""
argument_list|)
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|scanners
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
while|while
condition|(
operator|(
name|scanners
index|[
name|i
index|]
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|keys
index|[
name|i
index|]
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|chosenRow
argument_list|)
operator|==
literal|0
operator|)
condition|)
block|{
comment|// If we are doing a wild card match or there are multiple matchers
comment|// per column, we need to scan all the older versions of this row
comment|// to pick up the rest of the family members
if|if
condition|(
operator|!
name|wildcardMatch
operator|&&
operator|!
name|multipleMatchers
operator|&&
operator|(
name|keys
index|[
name|i
index|]
operator|.
name|getTimestamp
argument_list|()
operator|!=
name|chosenTimestamp
operator|)
condition|)
block|{
break|break;
block|}
comment|// NOTE: We used to do results.putAll(resultSets[i]);
comment|//       but this had the effect of overwriting newer
comment|//       values with older ones. So now we only insert
comment|//       a result if the map does not contain the key.
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Text
argument_list|,
name|BytesWritable
argument_list|>
name|e
range|:
name|resultSets
index|[
name|i
index|]
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|results
operator|.
name|containsKey
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|)
condition|)
block|{
name|results
operator|.
name|put
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|,
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
name|insertedItem
operator|=
literal|true
expr_stmt|;
block|}
block|}
name|resultSets
index|[
name|i
index|]
operator|.
name|clear
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|scanners
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|resultSets
index|[
name|i
index|]
argument_list|)
condition|)
block|{
name|closeScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
comment|// If the current scanner is non-null AND has a lower-or-equal
comment|// row label, then its timestamp is bad.  We need to advance it.
while|while
condition|(
operator|(
name|scanners
index|[
name|i
index|]
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|keys
index|[
name|i
index|]
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|chosenRow
argument_list|)
operator|<=
literal|0
operator|)
condition|)
block|{
name|resultSets
index|[
name|i
index|]
operator|.
name|clear
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|scanners
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|resultSets
index|[
name|i
index|]
argument_list|)
condition|)
block|{
name|closeScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
return|return
name|insertedItem
return|;
block|}
comment|/** Shut down a single scanner */
name|void
name|closeScanner
parameter_list|(
name|int
name|i
parameter_list|)
block|{
try|try
block|{
name|scanners
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|scanners
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|resultSets
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|/* (non-Javadoc)      * @see org.apache.hadoop.hbase.HInternalScannerInterface#close()      */
specifier|public
name|void
name|close
parameter_list|()
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|scanners
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|scanners
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
name|closeScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// Utility methods
comment|/**    * Convenience method creating new HRegions.    * @param regionId ID to use    * @param tableDesc Descriptor    * @param rootDir Root directory of HBase instance    * @param conf    * @return New META region (ROOT or META).    * @throws IOException    */
specifier|static
name|HRegion
name|createHRegion
parameter_list|(
specifier|final
name|long
name|regionId
parameter_list|,
specifier|final
name|HTableDescriptor
name|tableDesc
parameter_list|,
specifier|final
name|Path
name|rootDir
parameter_list|,
specifier|final
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|createHRegion
argument_list|(
operator|new
name|HRegionInfo
argument_list|(
name|regionId
argument_list|,
name|tableDesc
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
argument_list|,
name|rootDir
argument_list|,
name|conf
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Convenience method creating new HRegions. Used by createTable and by the    * bootstrap code in the HMaster constructor    *     * @param info Info for region to create.    * @param rootDir Root directory for HBase instance    * @param conf    * @param initialFiles InitialFiles to pass new HRegion. Pass null if none.    * @return new HRegion    *     * @throws IOException    */
specifier|static
name|HRegion
name|createHRegion
parameter_list|(
specifier|final
name|HRegionInfo
name|info
parameter_list|,
specifier|final
name|Path
name|rootDir
parameter_list|,
specifier|final
name|Configuration
name|conf
parameter_list|,
specifier|final
name|Path
name|initialFiles
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|regionDir
init|=
name|HStoreFile
operator|.
name|getHRegionDir
argument_list|(
name|rootDir
argument_list|,
name|info
operator|.
name|regionName
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|regionDir
argument_list|)
expr_stmt|;
return|return
operator|new
name|HRegion
argument_list|(
name|rootDir
argument_list|,
operator|new
name|HLog
argument_list|(
name|fs
argument_list|,
operator|new
name|Path
argument_list|(
name|regionDir
argument_list|,
name|HREGION_LOGDIR_NAME
argument_list|)
argument_list|,
name|conf
argument_list|)
argument_list|,
name|fs
argument_list|,
name|conf
argument_list|,
name|info
argument_list|,
name|initialFiles
argument_list|)
return|;
block|}
comment|/**    * Inserts a new region's meta information into the passed    *<code>meta</code> region. Used by the HMaster bootstrap code adding    * new table to ROOT table.    *     * @param meta META HRegion to be updated    * @param r HRegion to add to<code>meta</code>    *    * @throws IOException    */
specifier|static
name|void
name|addRegionToMETA
parameter_list|(
name|HRegion
name|meta
parameter_list|,
name|HRegion
name|r
parameter_list|)
throws|throws
name|IOException
block|{
comment|// The row key is the region name
name|long
name|writeid
init|=
name|meta
operator|.
name|startUpdate
argument_list|(
name|r
operator|.
name|getRegionName
argument_list|()
argument_list|)
decl_stmt|;
name|ByteArrayOutputStream
name|bytes
init|=
operator|new
name|ByteArrayOutputStream
argument_list|()
decl_stmt|;
name|DataOutputStream
name|s
init|=
operator|new
name|DataOutputStream
argument_list|(
name|bytes
argument_list|)
decl_stmt|;
name|r
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|write
argument_list|(
name|s
argument_list|)
expr_stmt|;
name|meta
operator|.
name|put
argument_list|(
name|writeid
argument_list|,
name|COL_REGIONINFO
argument_list|,
operator|new
name|BytesWritable
argument_list|(
name|bytes
operator|.
name|toByteArray
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|meta
operator|.
name|commit
argument_list|(
name|writeid
argument_list|)
expr_stmt|;
block|}
specifier|static
name|void
name|addRegionToMETA
parameter_list|(
specifier|final
name|HClient
name|client
parameter_list|,
specifier|final
name|Text
name|table
parameter_list|,
specifier|final
name|HRegion
name|region
parameter_list|,
specifier|final
name|HServerAddress
name|serverAddress
parameter_list|,
specifier|final
name|long
name|startCode
parameter_list|)
throws|throws
name|IOException
block|{
name|client
operator|.
name|openTable
argument_list|(
name|table
argument_list|)
expr_stmt|;
name|ByteArrayOutputStream
name|bytes
init|=
operator|new
name|ByteArrayOutputStream
argument_list|()
decl_stmt|;
name|DataOutputStream
name|out
init|=
operator|new
name|DataOutputStream
argument_list|(
name|bytes
argument_list|)
decl_stmt|;
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
name|long
name|lockid
init|=
name|client
operator|.
name|startUpdate
argument_list|(
name|region
operator|.
name|getRegionName
argument_list|()
argument_list|)
decl_stmt|;
name|client
operator|.
name|put
argument_list|(
name|lockid
argument_list|,
name|COL_REGIONINFO
argument_list|,
name|bytes
operator|.
name|toByteArray
argument_list|()
argument_list|)
expr_stmt|;
name|client
operator|.
name|put
argument_list|(
name|lockid
argument_list|,
name|COL_SERVER
argument_list|,
name|serverAddress
operator|.
name|toString
argument_list|()
operator|.
name|getBytes
argument_list|(
name|UTF8_ENCODING
argument_list|)
argument_list|)
expr_stmt|;
name|client
operator|.
name|put
argument_list|(
name|lockid
argument_list|,
name|COL_STARTCODE
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|startCode
argument_list|)
operator|.
name|getBytes
argument_list|(
name|UTF8_ENCODING
argument_list|)
argument_list|)
expr_stmt|;
name|client
operator|.
name|commit
argument_list|(
name|lockid
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Added region "
operator|+
name|region
operator|.
name|getRegionName
argument_list|()
operator|+
literal|" to table "
operator|+
name|table
argument_list|)
expr_stmt|;
block|}
comment|/**    * Delete<code>region</code> from META<code>table</code>.    * @param client Client to use running update.    * @param table META table we are to delete region from.    * @param regionName Region to remove.    * @throws IOException    */
specifier|static
name|void
name|removeRegionFromMETA
parameter_list|(
specifier|final
name|HClient
name|client
parameter_list|,
specifier|final
name|Text
name|table
parameter_list|,
specifier|final
name|Text
name|regionName
parameter_list|)
throws|throws
name|IOException
block|{
name|client
operator|.
name|openTable
argument_list|(
name|table
argument_list|)
expr_stmt|;
name|long
name|lockid
init|=
name|client
operator|.
name|startUpdate
argument_list|(
name|regionName
argument_list|)
decl_stmt|;
name|client
operator|.
name|delete
argument_list|(
name|lockid
argument_list|,
name|COL_REGIONINFO
argument_list|)
expr_stmt|;
name|client
operator|.
name|delete
argument_list|(
name|lockid
argument_list|,
name|COL_SERVER
argument_list|)
expr_stmt|;
name|client
operator|.
name|delete
argument_list|(
name|lockid
argument_list|,
name|COL_STARTCODE
argument_list|)
expr_stmt|;
name|client
operator|.
name|commit
argument_list|(
name|lockid
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Removed "
operator|+
name|regionName
operator|+
literal|" from table "
operator|+
name|table
argument_list|)
expr_stmt|;
block|}
comment|/**    * @param data Map of META row labelled column data.    * @return Server    */
specifier|static
name|HRegionInfo
name|getRegionInfo
parameter_list|(
specifier|final
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|byte
index|[]
argument_list|>
name|data
parameter_list|)
throws|throws
name|IOException
block|{
name|byte
index|[]
name|bytes
init|=
name|data
operator|.
name|get
argument_list|(
name|COL_REGIONINFO
argument_list|)
decl_stmt|;
if|if
condition|(
name|bytes
operator|==
literal|null
operator|||
name|bytes
operator|.
name|length
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"no value for "
operator|+
name|COL_REGIONINFO
argument_list|)
throw|;
block|}
name|DataInputBuffer
name|in
init|=
operator|new
name|DataInputBuffer
argument_list|()
decl_stmt|;
name|in
operator|.
name|reset
argument_list|(
name|bytes
argument_list|,
name|bytes
operator|.
name|length
argument_list|)
expr_stmt|;
name|HRegionInfo
name|info
init|=
operator|new
name|HRegionInfo
argument_list|()
decl_stmt|;
name|info
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
return|return
name|info
return|;
block|}
comment|/**    * @param data Map of META row labelled column data.    * @return Server    */
specifier|static
name|String
name|getServerName
parameter_list|(
specifier|final
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|byte
index|[]
argument_list|>
name|data
parameter_list|)
block|{
name|byte
index|[]
name|bytes
init|=
name|data
operator|.
name|get
argument_list|(
name|COL_SERVER
argument_list|)
decl_stmt|;
name|String
name|name
init|=
literal|null
decl_stmt|;
try|try
block|{
name|name
operator|=
operator|(
name|bytes
operator|!=
literal|null
operator|&&
name|bytes
operator|.
name|length
operator|!=
literal|0
operator|)
condition|?
operator|new
name|String
argument_list|(
name|bytes
argument_list|,
name|UTF8_ENCODING
argument_list|)
else|:
literal|null
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|e
parameter_list|)
block|{
assert|assert
operator|(
literal|false
operator|)
assert|;
block|}
return|return
operator|(
name|name
operator|!=
literal|null
operator|)
condition|?
name|name
operator|.
name|trim
argument_list|()
else|:
name|name
return|;
block|}
comment|/**    * @param data Map of META row labelled column data.    * @return Start code.    */
specifier|static
name|long
name|getStartCode
parameter_list|(
specifier|final
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|byte
index|[]
argument_list|>
name|data
parameter_list|)
block|{
name|long
name|startCode
init|=
operator|-
literal|1L
decl_stmt|;
name|byte
index|[]
name|bytes
init|=
name|data
operator|.
name|get
argument_list|(
name|COL_STARTCODE
argument_list|)
decl_stmt|;
if|if
condition|(
name|bytes
operator|!=
literal|null
operator|&&
name|bytes
operator|.
name|length
operator|!=
literal|0
condition|)
block|{
try|try
block|{
name|startCode
operator|=
name|Long
operator|.
name|parseLong
argument_list|(
operator|new
name|String
argument_list|(
name|bytes
argument_list|,
name|UTF8_ENCODING
argument_list|)
operator|.
name|trim
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NumberFormatException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed getting "
operator|+
name|COL_STARTCODE
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed getting "
operator|+
name|COL_STARTCODE
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|startCode
return|;
block|}
block|}
end_class

end_unit

