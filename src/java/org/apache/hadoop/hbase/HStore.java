begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2007 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Vector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataInputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|ImmutableBytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|MapFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|*
import|;
end_import

begin_comment
comment|/**  * HStore maintains a bunch of data files.  It is responsible for maintaining   * the memory/file hierarchy and for periodic flushes to disk and compacting   * edits to the file.  *  * Locking and transactions are handled at a higher level.  This API should not   * be called directly by any writer, but rather by an HRegion manager.  */
end_comment

begin_class
class|class
name|HStore
implements|implements
name|HConstants
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HStore
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_DIR
init|=
literal|"compaction.tmp"
decl_stmt|;
specifier|static
specifier|final
name|String
name|WORKING_COMPACTION
init|=
literal|"compaction.inprogress"
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_TO_REPLACE
init|=
literal|"toreplace"
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_DONE
init|=
literal|"done"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|BLOOMFILTER_FILE_NAME
init|=
literal|"filter"
decl_stmt|;
name|Path
name|dir
decl_stmt|;
name|Text
name|regionName
decl_stmt|;
name|HColumnDescriptor
name|family
decl_stmt|;
name|Text
name|familyName
decl_stmt|;
name|SequenceFile
operator|.
name|CompressionType
name|compression
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
name|Configuration
name|conf
decl_stmt|;
name|Path
name|mapdir
decl_stmt|;
name|Path
name|compactdir
decl_stmt|;
name|Path
name|loginfodir
decl_stmt|;
name|Path
name|filterDir
decl_stmt|;
name|Filter
name|bloomFilter
decl_stmt|;
name|Integer
name|compactLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|Integer
name|flushLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|final
name|HLocking
name|lock
init|=
operator|new
name|HLocking
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
name|maps
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|mapFiles
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|Random
name|rand
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|/**    * An HStore is a set of zero or more MapFiles, which stretch backwards over     * time.  A given HStore is responsible for a certain set of columns for a    * row in the HRegion.    *    *<p>The HRegion starts writing to its set of HStores when the HRegion's     * memcache is flushed.  This results in a round of new MapFiles, one for    * each HStore.    *    *<p>There's no reason to consider append-logging at this level; all logging     * and locking is handled at the HRegion level.  HStore just provides    * services to manage sets of MapFiles.  One of the most important of those    * services is MapFile-compaction services.    *    *<p>The only thing having to do with logs that HStore needs to deal with is    * the reconstructionLog.  This is a segment of an HRegion's log that might    * NOT be present upon startup.  If the param is NULL, there's nothing to do.    * If the param is non-NULL, we need to process the log to reconstruct    * a TreeMap that might not have been written to disk before the process    * died.    *    *<p>It's assumed that after this constructor returns, the reconstructionLog    * file will be deleted (by whoever has instantiated the HStore).    *    * @param dir log file directory    * @param regionName name of region    * @param family name of column family    * @param fs file system object    * @param reconstructionLog existing log file to apply if any    * @param conf configuration object    * @throws IOException    */
name|HStore
parameter_list|(
name|Path
name|dir
parameter_list|,
name|Text
name|regionName
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|reconstructionLog
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
name|this
operator|.
name|regionName
operator|=
name|regionName
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|familyName
operator|=
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|NONE
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|!=
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|NONE
condition|)
block|{
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|BLOCK
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|BLOCK
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|RECORD
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|RECORD
expr_stmt|;
block|}
else|else
block|{
assert|assert
operator|(
literal|false
operator|)
assert|;
block|}
block|}
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|mapdir
operator|=
name|HStoreFile
operator|.
name|getMapDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|mapdir
argument_list|)
expr_stmt|;
name|this
operator|.
name|loginfodir
operator|=
name|HStoreFile
operator|.
name|getInfoDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|loginfodir
argument_list|)
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|bloomFilter
operator|==
literal|null
condition|)
block|{
name|this
operator|.
name|filterDir
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|bloomFilter
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|filterDir
operator|=
name|HStoreFile
operator|.
name|getFilterDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|filterDir
argument_list|)
expr_stmt|;
name|loadOrCreateBloomFilter
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"starting HStore for "
operator|+
name|regionName
operator|+
literal|"/"
operator|+
name|familyName
argument_list|)
expr_stmt|;
block|}
comment|// Either restart or get rid of any leftover compaction work.  Either way,
comment|// by the time processReadyCompaction() returns, we can get rid of the
comment|// existing compaction-dir.
name|this
operator|.
name|compactdir
operator|=
operator|new
name|Path
argument_list|(
name|dir
argument_list|,
name|COMPACTION_DIR
argument_list|)
expr_stmt|;
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|curCompactStore
argument_list|)
condition|)
block|{
name|processReadyCompaction
argument_list|()
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|curCompactStore
argument_list|)
expr_stmt|;
block|}
comment|// Go through the 'mapdir' and 'loginfodir' together, make sure that all
comment|// MapFiles are in a reliable state.  Every entry in 'mapdir' must have a
comment|// corresponding one in 'loginfodir'. Without a corresponding log info
comment|// file, the entry in 'mapdir' must be deleted.
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|hstoreFiles
init|=
name|HStoreFile
operator|.
name|loadHStoreFiles
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
name|fs
argument_list|)
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|hstoreFiles
control|)
block|{
name|mapFiles
operator|.
name|put
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
argument_list|)
argument_list|,
name|hsf
argument_list|)
expr_stmt|;
block|}
comment|// Now go through all the HSTORE_LOGINFOFILEs and figure out the
comment|// most-recent log-seq-ID that's present.  The most-recent such ID means we
comment|// can ignore all log messages up to and including that ID (because they're
comment|// already reflected in the TreeMaps).
comment|//
comment|// If the HSTORE_LOGINFOFILE doesn't contain a number, just ignore it. That
comment|// means it was built prior to the previous run of HStore, and so it cannot
comment|// contain any updates also contained in the log.
name|long
name|maxSeqID
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|hstoreFiles
control|)
block|{
name|long
name|seqid
init|=
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|seqid
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|seqid
operator|>
name|maxSeqID
condition|)
block|{
name|maxSeqID
operator|=
name|seqid
expr_stmt|;
block|}
block|}
block|}
name|doReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|maxSeqID
argument_list|)
expr_stmt|;
comment|// Compact all the MapFiles into a single file.  The resulting MapFile
comment|// should be "timeless"; that is, it should not have an associated seq-ID,
comment|// because all log messages have been reflected in the TreeMaps at this
comment|// point.
if|if
condition|(
name|mapFiles
operator|.
name|size
argument_list|()
operator|>=
literal|1
condition|)
block|{
name|compactHelper
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
comment|// Finally, start up all the map readers! (There should be just one at this
comment|// point, as we've compacted them all.)
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"starting map readers"
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|mapFiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
comment|// TODO - is this really necessary?  Don't I do this inside compact()?
name|maps
operator|.
name|put
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|,
name|getMapFileReader
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"HStore online for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
comment|/*    * Read the reconstructionLog to see whether we need to build a brand-new     * MapFile out of non-flushed log entries.      *    * We can ignore any log message that has a sequence ID that's equal to or     * lower than maxSeqID.  (Because we know such log messages are already     * reflected in the MapFiles.)    */
specifier|private
name|void
name|doReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|maxSeqID
parameter_list|)
throws|throws
name|UnsupportedEncodingException
throws|,
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"reading reconstructionLog"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|reconstructionLog
operator|==
literal|null
operator|||
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
condition|)
block|{
return|return;
block|}
name|long
name|maxSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|reconstructedCache
init|=
operator|new
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|()
decl_stmt|;
name|SequenceFile
operator|.
name|Reader
name|login
init|=
operator|new
name|SequenceFile
operator|.
name|Reader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|reconstructionLog
argument_list|,
name|this
operator|.
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|HLogKey
name|key
init|=
operator|new
name|HLogKey
argument_list|()
decl_stmt|;
name|HLogEdit
name|val
init|=
operator|new
name|HLogEdit
argument_list|()
decl_stmt|;
while|while
condition|(
name|login
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
condition|)
block|{
name|maxSeqIdInLog
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSeqIdInLog
argument_list|,
name|key
operator|.
name|getLogSeqNum
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|<=
name|maxSeqID
condition|)
block|{
continue|continue;
block|}
comment|// Check this edit is for me. Also, guard against writing
comment|// METACOLUMN info such as HBASE::CACHEFLUSH entries
name|Text
name|column
init|=
name|val
operator|.
name|getColumn
argument_list|()
decl_stmt|;
if|if
condition|(
name|column
operator|.
name|equals
argument_list|(
name|HLog
operator|.
name|METACOLUMN
argument_list|)
operator|||
operator|!
name|key
operator|.
name|getRegionName
argument_list|()
operator|.
name|equals
argument_list|(
name|this
operator|.
name|regionName
argument_list|)
operator|||
operator|!
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|column
argument_list|)
operator|.
name|equals
argument_list|(
name|this
operator|.
name|familyName
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Passing on edit "
operator|+
name|key
operator|.
name|getRegionName
argument_list|()
operator|+
literal|", "
operator|+
name|column
operator|.
name|toString
argument_list|()
operator|+
literal|": "
operator|+
operator|new
name|String
argument_list|(
name|val
operator|.
name|getVal
argument_list|()
argument_list|)
operator|+
literal|", my region: "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|", my column: "
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
continue|continue;
block|}
name|HStoreKey
name|k
init|=
operator|new
name|HStoreKey
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|column
argument_list|,
name|val
operator|.
name|getTimestamp
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Applying edit "
operator|+
name|k
operator|.
name|toString
argument_list|()
operator|+
literal|"="
operator|+
operator|new
name|String
argument_list|(
name|val
operator|.
name|getVal
argument_list|()
argument_list|,
name|UTF8_ENCODING
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|reconstructedCache
operator|.
name|put
argument_list|(
name|k
argument_list|,
name|val
operator|.
name|getVal
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|login
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|reconstructedCache
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// We create a "virtual flush" at maxSeqIdInLog+1.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing reconstructionCache"
argument_list|)
expr_stmt|;
block|}
name|flushCacheHelper
argument_list|(
name|reconstructedCache
argument_list|,
name|maxSeqIdInLog
operator|+
literal|1
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Bloom filters
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Called by constructor if a bloom filter is enabled for this column family.    * If the HStore already exists, it will read in the bloom filter saved    * previously. Otherwise, it will create a new bloom filter.    */
specifier|private
name|void
name|loadOrCreateBloomFilter
parameter_list|()
throws|throws
name|IOException
block|{
name|Path
name|filterFile
init|=
operator|new
name|Path
argument_list|(
name|filterDir
argument_list|,
name|BLOOMFILTER_FILE_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|filterFile
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loading bloom filter for "
operator|+
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
switch|switch
condition|(
name|family
operator|.
name|bloomFilter
operator|.
name|filterType
condition|)
block|{
case|case
name|BloomFilterDescriptor
operator|.
name|BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|BloomFilter
argument_list|()
expr_stmt|;
break|break;
case|case
name|BloomFilterDescriptor
operator|.
name|COUNTING_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|CountingBloomFilter
argument_list|()
expr_stmt|;
break|break;
case|case
name|BloomFilterDescriptor
operator|.
name|RETOUCHED_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|RetouchedBloomFilter
argument_list|()
expr_stmt|;
block|}
name|FSDataInputStream
name|in
init|=
name|fs
operator|.
name|open
argument_list|(
name|filterFile
argument_list|)
decl_stmt|;
name|bloomFilter
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
name|fs
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"creating bloom filter for "
operator|+
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
switch|switch
condition|(
name|family
operator|.
name|bloomFilter
operator|.
name|filterType
condition|)
block|{
case|case
name|BloomFilterDescriptor
operator|.
name|BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|BloomFilter
argument_list|(
name|family
operator|.
name|bloomFilter
operator|.
name|vectorSize
argument_list|,
name|family
operator|.
name|bloomFilter
operator|.
name|nbHash
argument_list|)
expr_stmt|;
break|break;
case|case
name|BloomFilterDescriptor
operator|.
name|COUNTING_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|CountingBloomFilter
argument_list|(
name|family
operator|.
name|bloomFilter
operator|.
name|vectorSize
argument_list|,
name|family
operator|.
name|bloomFilter
operator|.
name|nbHash
argument_list|)
expr_stmt|;
break|break;
case|case
name|BloomFilterDescriptor
operator|.
name|RETOUCHED_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|RetouchedBloomFilter
argument_list|(
name|family
operator|.
name|bloomFilter
operator|.
name|vectorSize
argument_list|,
name|family
operator|.
name|bloomFilter
operator|.
name|nbHash
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Flushes bloom filter to disk    *     * @throws IOException    */
specifier|private
name|void
name|flushBloomFilter
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing bloom filter for "
operator|+
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
operator|new
name|Path
argument_list|(
name|filterDir
argument_list|,
name|BLOOMFILTER_FILE_NAME
argument_list|)
argument_list|)
decl_stmt|;
name|bloomFilter
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushed bloom filter for "
operator|+
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/** Generates a bloom filter key from the row and column keys */
name|Key
name|getBloomFilterKey
parameter_list|(
name|HStoreKey
name|k
parameter_list|)
block|{
name|StringBuilder
name|s
init|=
operator|new
name|StringBuilder
argument_list|(
name|k
operator|.
name|getRow
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|s
operator|.
name|append
argument_list|(
name|k
operator|.
name|getColumn
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|byte
index|[]
name|bytes
init|=
literal|null
decl_stmt|;
try|try
block|{
name|bytes
operator|=
name|s
operator|.
name|toString
argument_list|()
operator|.
name|getBytes
argument_list|(
name|HConstants
operator|.
name|UTF8_ENCODING
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
assert|assert
operator|(
literal|false
operator|)
assert|;
block|}
return|return
operator|new
name|Key
argument_list|(
name|bytes
argument_list|)
return|;
block|}
comment|/**     * Extends MapFile.Reader and overrides get and getClosest to consult the    * bloom filter before attempting to read from disk.    */
specifier|private
class|class
name|BloomFilterReader
extends|extends
name|MapFile
operator|.
name|Reader
block|{
name|BloomFilterReader
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|String
name|dirName
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|fs
argument_list|,
name|dirName
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
comment|/** {@inheritDoc} */
annotation|@
name|Override
specifier|public
name|Writable
name|get
parameter_list|(
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|val
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Note - the key being passed to us is always a HStoreKey
if|if
condition|(
name|bloomFilter
operator|.
name|membershipTest
argument_list|(
name|getBloomFilterKey
argument_list|(
operator|(
name|HStoreKey
operator|)
name|key
argument_list|)
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"bloom filter reported that key exists"
argument_list|)
expr_stmt|;
block|}
return|return
name|super
operator|.
name|get
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
return|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"bloom filter reported that key does not exist"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** {@inheritDoc} */
annotation|@
name|Override
specifier|public
name|WritableComparable
name|getClosest
parameter_list|(
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|val
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Note - the key being passed to us is always a HStoreKey
if|if
condition|(
name|bloomFilter
operator|.
name|membershipTest
argument_list|(
name|getBloomFilterKey
argument_list|(
operator|(
name|HStoreKey
operator|)
name|key
argument_list|)
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"bloom filter reported that key exists"
argument_list|)
expr_stmt|;
block|}
return|return
name|super
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
return|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"bloom filter reported that key does not exist"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
block|}
comment|/**    * Extends MapFile.Writer and overrides append, so that whenever a MapFile    * is written to, the key is added to the bloom filter.    */
specifier|private
class|class
name|BloomFilterWriter
extends|extends
name|MapFile
operator|.
name|Writer
block|{
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
name|BloomFilterWriter
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|String
name|dirName
parameter_list|,
name|Class
name|keyClass
parameter_list|,
name|Class
name|valClass
parameter_list|,
name|SequenceFile
operator|.
name|CompressionType
name|compression
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|dirName
argument_list|,
name|keyClass
argument_list|,
name|valClass
argument_list|,
name|compression
argument_list|)
expr_stmt|;
block|}
comment|/** {@inheritDoc} */
annotation|@
name|Override
specifier|public
name|void
name|append
parameter_list|(
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|val
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Note - the key being passed to us is always a HStoreKey
name|bloomFilter
operator|.
name|add
argument_list|(
name|getBloomFilterKey
argument_list|(
operator|(
name|HStoreKey
operator|)
name|key
argument_list|)
argument_list|)
expr_stmt|;
name|super
operator|.
name|append
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Get a MapFile reader    * This allows us to substitute a BloomFilterReader if a bloom filter is enabled    */
name|MapFile
operator|.
name|Reader
name|getMapFileReader
parameter_list|(
name|String
name|dirName
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|bloomFilter
operator|!=
literal|null
condition|)
block|{
return|return
operator|new
name|BloomFilterReader
argument_list|(
name|fs
argument_list|,
name|dirName
argument_list|,
name|conf
argument_list|)
return|;
block|}
return|return
operator|new
name|MapFile
operator|.
name|Reader
argument_list|(
name|fs
argument_list|,
name|dirName
argument_list|,
name|conf
argument_list|)
return|;
block|}
comment|/**    * Get a MapFile writer    * This allows us to substitute a BloomFilterWriter if a bloom filter is    * enabled    *     * @param dirName Directory with store files.    * @return Map file.    * @throws IOException    */
name|MapFile
operator|.
name|Writer
name|getMapFileWriter
parameter_list|(
name|String
name|dirName
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|bloomFilter
operator|!=
literal|null
condition|)
block|{
return|return
operator|new
name|BloomFilterWriter
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|dirName
argument_list|,
name|HStoreKey
operator|.
name|class
argument_list|,
name|ImmutableBytesWritable
operator|.
name|class
argument_list|,
name|compression
argument_list|)
return|;
block|}
return|return
operator|new
name|MapFile
operator|.
name|Writer
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|dirName
argument_list|,
name|HStoreKey
operator|.
name|class
argument_list|,
name|ImmutableBytesWritable
operator|.
name|class
argument_list|,
name|compression
argument_list|)
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// End bloom filters
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Turn off all the MapFile readers    *     * @throws IOException    */
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"closing HStore for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
for|for
control|(
name|MapFile
operator|.
name|Reader
name|map
range|:
name|maps
operator|.
name|values
argument_list|()
control|)
block|{
name|map
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|maps
operator|.
name|clear
argument_list|()
expr_stmt|;
name|mapFiles
operator|.
name|clear
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"HStore closed for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Flush changes to disk
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Write out a brand-new set of items to the disk.    *    * We should only store key/vals that are appropriate for the data-columns     * stored in this HStore.    *    * Also, we are not expecting any reads of this MapFile just yet.    *    * Return the entire list of HStoreFiles currently used by the HStore.    *    * @param inputCache          - memcache to flush    * @param logCacheFlushId     - flush sequence number    * @return - Vector of all the HStoreFiles in use    * @throws IOException    */
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|flushCache
parameter_list|(
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|inputCache
parameter_list|,
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|flushCacheHelper
argument_list|(
name|inputCache
argument_list|,
name|logCacheFlushId
argument_list|,
literal|true
argument_list|)
return|;
block|}
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|flushCacheHelper
parameter_list|(
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|inputCache
parameter_list|,
name|long
name|logCacheFlushId
parameter_list|,
name|boolean
name|addToAvailableMaps
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing HStore "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
comment|// A. Write the TreeMap out to the disk
name|HStoreFile
name|flushedFile
init|=
name|HStoreFile
operator|.
name|obtainNewHStoreFile
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|Path
name|mapfile
init|=
name|flushedFile
operator|.
name|getMapFilePath
argument_list|()
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"map file is: "
operator|+
name|mapfile
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|MapFile
operator|.
name|Writer
name|out
init|=
name|getMapFileWriter
argument_list|(
name|mapfile
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|es
range|:
name|inputCache
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreKey
name|curkey
init|=
name|es
operator|.
name|getKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|familyName
operator|.
name|equals
argument_list|(
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|curkey
operator|.
name|getColumn
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
name|out
operator|.
name|append
argument_list|(
name|curkey
argument_list|,
operator|new
name|ImmutableBytesWritable
argument_list|(
name|es
operator|.
name|getValue
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"HStore "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
operator|+
literal|" flushed"
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// B. Write out the log sequence number that corresponds to this output
comment|// MapFile.  The MapFile is current up to and including the log seq num.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"writing log cache flush id"
argument_list|)
expr_stmt|;
block|}
name|flushedFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
comment|// C. Flush the bloom filter if any
if|if
condition|(
name|bloomFilter
operator|!=
literal|null
condition|)
block|{
name|flushBloomFilter
argument_list|()
expr_stmt|;
block|}
comment|// D. Finally, make the new MapFile available.
if|if
condition|(
name|addToAvailableMaps
condition|)
block|{
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Long
name|flushid
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|logCacheFlushId
argument_list|)
decl_stmt|;
name|maps
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|getMapFileReader
argument_list|(
name|mapfile
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|mapFiles
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|flushedFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"HStore available for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
operator|+
literal|" flush id="
operator|+
name|logCacheFlushId
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|getAllMapFiles
argument_list|()
return|;
block|}
block|}
comment|/**    * @return - vector of all the HStore files in use    */
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|getAllMapFiles
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|mapFiles
operator|.
name|values
argument_list|()
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the back-HStores.  This method may take some time, so the calling     * thread must be able to block for long periods.    *     * During this time, the HStore can work as usual, getting values from    * MapFiles and writing new MapFiles from given memcaches.    *     * Existing MapFiles are not destroyed until the new compacted TreeMap is     * completely written-out to disk.    *    * The compactLock block prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *     * We don't want to hold the structureLock for the whole time, as a compact()     * can be lengthy and we want to allow cache-flushes during this period.    *     * @throws IOException    */
name|void
name|compact
parameter_list|()
throws|throws
name|IOException
block|{
name|compactHelper
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
name|void
name|compactHelper
parameter_list|(
name|boolean
name|deleteSequenceInfo
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"started compaction of "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|curCompactStore
argument_list|)
expr_stmt|;
try|try
block|{
comment|// Grab a list of files to compact.
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
init|=
literal|null
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|toCompactFiles
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|mapFiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
comment|// Compute the max-sequenceID seen in any of the to-be-compacted TreeMaps
name|long
name|maxSeenSeqID
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|toCompactFiles
control|)
block|{
name|long
name|seqid
init|=
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|seqid
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|seqid
operator|>
name|maxSeenSeqID
condition|)
block|{
name|maxSeenSeqID
operator|=
name|seqid
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"max sequence id: "
operator|+
name|maxSeenSeqID
argument_list|)
expr_stmt|;
block|}
name|HStoreFile
name|compactedOutputFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|toCompactFiles
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"nothing to compact for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
name|HStoreFile
name|hsf
init|=
name|toCompactFiles
operator|.
name|elementAt
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
operator|==
operator|-
literal|1
condition|)
block|{
return|return;
block|}
block|}
comment|// Step through them, writing to the brand-new TreeMap
name|MapFile
operator|.
name|Writer
name|compactedOut
init|=
name|getMapFileWriter
argument_list|(
name|compactedOutputFile
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
comment|// We create a new set of MapFile.Reader objects so we don't screw up
comment|// the caching associated with the currently-loaded ones.
comment|//
comment|// Our iteration-based access pattern is practically designed to ruin
comment|// the cache.
comment|//
comment|// We work by opening a single MapFile.Reader for each file, and
comment|// iterating through them in parallel.  We always increment the
comment|// lowest-ranked one.  Updates to a single row/column will appear
comment|// ranked by timestamp.  This allows us to throw out deleted values or
comment|// obsolete versions.
name|MapFile
operator|.
name|Reader
index|[]
name|readers
init|=
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|HStoreKey
index|[]
name|keys
init|=
operator|new
name|HStoreKey
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|ImmutableBytesWritable
index|[]
name|vals
init|=
operator|new
name|ImmutableBytesWritable
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|boolean
index|[]
name|done
init|=
operator|new
name|boolean
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|int
name|pos
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|toCompactFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|readers
index|[
name|pos
index|]
operator|=
name|getMapFileReader
argument_list|(
name|hsf
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|keys
index|[
name|pos
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
name|vals
index|[
name|pos
index|]
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
name|done
index|[
name|pos
index|]
operator|=
literal|false
expr_stmt|;
name|pos
operator|++
expr_stmt|;
block|}
comment|// Now, advance through the readers in order.  This will have the
comment|// effect of a run-time sort of the entire dataset.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"processing HStoreFile readers"
argument_list|)
expr_stmt|;
block|}
name|int
name|numDone
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|readers
index|[
name|i
index|]
operator|.
name|reset
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
operator|!
name|readers
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|vals
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
name|numDone
operator|++
expr_stmt|;
block|}
block|}
name|int
name|timesSeen
init|=
literal|0
decl_stmt|;
name|Text
name|lastRow
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
name|Text
name|lastColumn
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
while|while
condition|(
name|numDone
operator|<
name|done
operator|.
name|length
condition|)
block|{
comment|// Find the reader with the smallest key
name|int
name|smallestKey
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|smallestKey
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|keys
index|[
name|i
index|]
operator|.
name|compareTo
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|)
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
block|}
block|}
comment|// Reflect the current key/val in the output
name|HStoreKey
name|sk
init|=
name|keys
index|[
name|smallestKey
index|]
decl_stmt|;
if|if
condition|(
name|lastRow
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
operator|&&
name|lastColumn
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
condition|)
block|{
name|timesSeen
operator|++
expr_stmt|;
block|}
else|else
block|{
name|timesSeen
operator|=
literal|1
expr_stmt|;
block|}
if|if
condition|(
name|timesSeen
operator|<=
name|family
operator|.
name|getMaxVersions
argument_list|()
condition|)
block|{
comment|// Keep old versions until we have maxVersions worth.
comment|// Then just skip them.
if|if
condition|(
name|sk
operator|.
name|getRow
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
operator|&&
name|sk
operator|.
name|getColumn
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
condition|)
block|{
comment|// Only write out objects which have a non-zero length key and value
name|compactedOut
operator|.
name|append
argument_list|(
name|sk
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
expr_stmt|;
block|}
block|}
comment|//TODO: I don't know what to do about deleted values.  I currently
comment|// include the fact that the item was deleted as a legitimate
comment|// "version" of the data.  Maybe it should just drop the deleted val?
comment|// Update last-seen items
name|lastRow
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|lastColumn
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
comment|// Advance the smallest key.  If that reader's all finished, then
comment|// mark it as done.
if|if
condition|(
operator|!
name|readers
index|[
name|smallestKey
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
condition|)
block|{
name|done
index|[
name|smallestKey
index|]
operator|=
literal|true
expr_stmt|;
name|readers
index|[
name|smallestKey
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
name|numDone
operator|++
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"all HStores processed"
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|compactedOut
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"writing new compacted HStore"
argument_list|)
expr_stmt|;
block|}
comment|// Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.
if|if
condition|(
operator|(
operator|!
name|deleteSequenceInfo
operator|)
operator|&&
name|maxSeenSeqID
operator|>=
literal|0
condition|)
block|{
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|maxSeenSeqID
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
comment|// Write out a list of data files that we're replacing
name|Path
name|filesToReplace
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_TO_REPLACE
argument_list|)
decl_stmt|;
name|DataOutputStream
name|out
init|=
operator|new
name|DataOutputStream
argument_list|(
name|fs
operator|.
name|create
argument_list|(
name|filesToReplace
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|toCompactFiles
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|toCompactFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|hsf
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Indicate that we're done.
name|Path
name|doneFile
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_DONE
argument_list|)
decl_stmt|;
operator|(
operator|new
name|DataOutputStream
argument_list|(
name|fs
operator|.
name|create
argument_list|(
name|doneFile
argument_list|)
argument_list|)
operator|)
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Move the compaction into place.
name|processReadyCompaction
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"compaction complete for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|fs
operator|.
name|delete
argument_list|(
name|compactdir
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * It's assumed that the compactLock  will be acquired prior to calling this     * method!  Otherwise, it is not thread-safe!    *    * It works by processing a compaction that's been written to disk.    *     * It is usually invoked at the end of a compaction, but might also be invoked    * at HStore startup, if the prior execution died midway through.    */
name|void
name|processReadyCompaction
parameter_list|()
throws|throws
name|IOException
block|{
comment|// Move the compacted TreeMap into place.
comment|// That means:
comment|// 1) Acquiring the write-lock
comment|// 2) Figuring out what MapFiles are going to be replaced
comment|// 3) Unloading all the replaced MapFiles.
comment|// 4) Deleting all the old MapFile files.
comment|// 5) Moving the new MapFile into place
comment|// 6) Loading the new TreeMap.
comment|// 7) Releasing the write-lock
comment|// 1. Acquiring the write-lock
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Path
name|doneFile
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_DONE
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|doneFile
argument_list|)
condition|)
block|{
comment|// The last execution didn't finish the compaction, so there's nothing
comment|// we can do.  We'll just have to redo it. Abandon it and return.
return|return;
block|}
comment|// OK, there's actually compaction work that needs to be put into place.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"compaction starting"
argument_list|)
expr_stmt|;
block|}
comment|// 2. Load in the files to be deleted.
comment|//    (Figuring out what MapFiles are going to be replaced)
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
init|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|Path
name|filesToReplace
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_TO_REPLACE
argument_list|)
decl_stmt|;
name|DataInputStream
name|in
init|=
operator|new
name|DataInputStream
argument_list|(
name|fs
operator|.
name|open
argument_list|(
name|filesToReplace
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|int
name|numfiles
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numfiles
condition|;
name|i
operator|++
control|)
block|{
name|HStoreFile
name|hsf
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|hsf
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
name|toCompactFiles
operator|.
name|add
argument_list|(
name|hsf
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded files to be deleted"
argument_list|)
expr_stmt|;
block|}
comment|// 3. Unload all the replaced MapFiles.
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it2
init|=
name|mapFiles
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|MapFile
operator|.
name|Reader
argument_list|>
name|it
init|=
name|maps
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|MapFile
operator|.
name|Reader
name|curReader
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|HStoreFile
name|curMapFile
init|=
name|it2
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|toCompactFiles
operator|.
name|contains
argument_list|(
name|curMapFile
argument_list|)
condition|)
block|{
name|curReader
operator|.
name|close
argument_list|()
expr_stmt|;
name|it
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
block|}
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|mapFiles
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|curMapFile
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|toCompactFiles
operator|.
name|contains
argument_list|(
name|curMapFile
argument_list|)
condition|)
block|{
name|it
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"unloaded existing MapFiles"
argument_list|)
expr_stmt|;
block|}
comment|// What if we crash at this point?  No big deal; we will restart
comment|// processReadyCompaction(), and nothing has been lost.
comment|// 4. Delete all the old files, no longer needed
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|toCompactFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|hsf
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|hsf
operator|.
name|getInfoFilePath
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"old files deleted"
argument_list|)
expr_stmt|;
block|}
comment|// What if we fail now?  The above deletes will fail silently. We'd better
comment|// make sure not to write out any new files with the same names as
comment|// something we delete, though.
comment|// 5. Moving the new MapFile into place
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"moving new MapFile into place"
argument_list|)
expr_stmt|;
block|}
name|HStoreFile
name|compactedFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
name|HStoreFile
name|finalCompactedFile
init|=
name|HStoreFile
operator|.
name|obtainNewHStoreFile
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|fs
operator|.
name|rename
argument_list|(
name|compactedFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|,
name|finalCompactedFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
expr_stmt|;
comment|// Fail here?  No problem.
name|fs
operator|.
name|rename
argument_list|(
name|compactedFile
operator|.
name|getInfoFilePath
argument_list|()
argument_list|,
name|finalCompactedFile
operator|.
name|getInfoFilePath
argument_list|()
argument_list|)
expr_stmt|;
comment|// Fail here?  No worries.
name|Long
name|orderVal
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|finalCompactedFile
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
comment|// 6. Loading the new TreeMap.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loading new TreeMap"
argument_list|)
expr_stmt|;
block|}
name|mapFiles
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|finalCompactedFile
argument_list|)
expr_stmt|;
name|maps
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|getMapFileReader
argument_list|(
name|finalCompactedFile
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// 7. Releasing the write-lock
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return all the available columns for the given key.  The key indicates a     * row and timestamp, but not a column name.    *    * The returned object should map column names to byte arrays (byte[]).    */
name|void
name|getFull
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|byte
index|[]
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|maps
operator|.
name|values
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|maps
operator|.
name|size
argument_list|()
index|]
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
do|do
block|{
name|Text
name|readcol
init|=
name|readkey
operator|.
name|getColumn
argument_list|()
decl_stmt|;
if|if
condition|(
name|results
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|==
literal|null
operator|&&
name|key
operator|.
name|matchesWithoutColumn
argument_list|(
name|readkey
argument_list|)
condition|)
block|{
if|if
condition|(
name|readval
operator|.
name|equals
argument_list|(
name|HConstants
operator|.
name|DELETE_BYTES
argument_list|)
condition|)
block|{
break|break;
block|}
name|results
operator|.
name|put
argument_list|(
operator|new
name|Text
argument_list|(
name|readcol
argument_list|)
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|key
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
break|break;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
block|}
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Get the value for the indicated HStoreKey.  Grab the target value and the     * previous 'numVersions-1' values, as well.    *    * If 'numVersions' is negative, the method returns all available versions.    */
name|byte
index|[]
index|[]
name|get
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|numVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|byte
index|[]
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|()
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|maps
operator|.
name|values
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|maps
operator|.
name|size
argument_list|()
index|]
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// map.getClosest returns null if the passed key is> than the
comment|// last key in the map file.  getClosest is a bit of a misnomer
comment|// since it returns exact match or the next closest key AFTER not
comment|// BEFORE.
continue|continue;
block|}
if|if
condition|(
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
if|if
condition|(
name|readval
operator|.
name|equals
argument_list|(
name|HConstants
operator|.
name|DELETE_BYTES
argument_list|)
condition|)
block|{
break|break;
block|}
name|results
operator|.
name|add
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
while|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
operator|&&
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
if|if
condition|(
operator|(
name|numVersions
operator|>
literal|0
operator|&&
operator|(
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
operator|)
operator|)
operator|||
name|readval
operator|.
name|equals
argument_list|(
name|HConstants
operator|.
name|DELETE_BYTES
argument_list|)
condition|)
block|{
break|break;
block|}
name|results
operator|.
name|add
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
condition|)
block|{
break|break;
block|}
block|}
return|return
name|results
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|?
literal|null
else|:
name|ImmutableBytesWritable
operator|.
name|toArray
argument_list|(
name|results
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Gets the size of the largest MapFile and its mid key.    *     * @param midKey      - the middle key for the largest MapFile    * @return            - size of the largest MapFile    */
name|long
name|getLargestFileSize
parameter_list|(
name|Text
name|midKey
parameter_list|)
block|{
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|mapFiles
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
name|maxSize
return|;
block|}
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Long
name|mapIndex
init|=
name|Long
operator|.
name|valueOf
argument_list|(
literal|0L
argument_list|)
decl_stmt|;
comment|// Iterate through all the MapFiles
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|mapFiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreFile
name|curHSF
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|long
name|size
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
operator|new
name|Path
argument_list|(
name|curHSF
operator|.
name|getMapFilePath
argument_list|()
argument_list|,
name|MapFile
operator|.
name|DATA_FILE_NAME
argument_list|)
argument_list|)
operator|.
name|getLen
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|mapIndex
operator|=
name|e
operator|.
name|getKey
argument_list|()
expr_stmt|;
block|}
block|}
name|MapFile
operator|.
name|Reader
name|r
init|=
name|maps
operator|.
name|get
argument_list|(
name|mapIndex
argument_list|)
decl_stmt|;
name|midKey
operator|.
name|set
argument_list|(
operator|(
operator|(
name|HStoreKey
operator|)
name|r
operator|.
name|midKey
argument_list|()
operator|)
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
return|return
name|maxSize
return|;
block|}
comment|/**    * @return    Returns the number of map files currently in use    */
name|int
name|getNMaps
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|maps
operator|.
name|size
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/** Generate a random unique filename suffix */
name|String
name|obtainFileLabel
parameter_list|(
name|Path
name|prefix
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|testsuffix
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|rand
operator|.
name|nextInt
argument_list|(
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
decl_stmt|;
name|Path
name|testpath
init|=
operator|new
name|Path
argument_list|(
name|prefix
operator|.
name|toString
argument_list|()
operator|+
name|testsuffix
argument_list|)
decl_stmt|;
while|while
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|testpath
argument_list|)
condition|)
block|{
name|testsuffix
operator|=
name|String
operator|.
name|valueOf
argument_list|(
name|rand
operator|.
name|nextInt
argument_list|(
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
expr_stmt|;
name|testpath
operator|=
operator|new
name|Path
argument_list|(
name|prefix
operator|.
name|toString
argument_list|()
operator|+
name|testsuffix
argument_list|)
expr_stmt|;
block|}
return|return
name|testsuffix
return|;
block|}
comment|/**    * Return a set of MapFile.Readers, one for each HStore file.    * These should be closed after the user is done with them.    */
name|HInternalScannerInterface
name|getScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|Text
name|targetCols
index|[]
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|new
name|HStoreScanner
argument_list|(
name|timestamp
argument_list|,
name|targetCols
argument_list|,
name|firstRow
argument_list|)
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// This class implements the HScannerInterface.
comment|// It lets the caller scan the contents of this HStore.
comment|//////////////////////////////////////////////////////////////////////////////
class|class
name|HStoreScanner
extends|extends
name|HAbstractScanner
block|{
specifier|private
name|MapFile
operator|.
name|Reader
index|[]
name|readers
decl_stmt|;
name|HStoreScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|Text
index|[]
name|targetCols
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|timestamp
argument_list|,
name|targetCols
argument_list|)
expr_stmt|;
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|this
operator|.
name|readers
operator|=
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|mapFiles
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
comment|// Most recent map file should be first
name|int
name|i
init|=
name|readers
operator|.
name|length
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|mapFiles
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|curHSF
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|readers
index|[
name|i
operator|--
index|]
operator|=
name|getMapFileReader
argument_list|(
name|curHSF
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|keys
operator|=
operator|new
name|HStoreKey
index|[
name|readers
operator|.
name|length
index|]
expr_stmt|;
name|this
operator|.
name|vals
operator|=
operator|new
name|byte
index|[
name|readers
operator|.
name|length
index|]
index|[]
expr_stmt|;
comment|// Advance the readers to the first pos.
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|keys
index|[
name|i
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
if|if
condition|(
name|firstRow
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|findFirstRow
argument_list|(
name|i
argument_list|,
name|firstRow
argument_list|)
condition|)
block|{
continue|continue;
block|}
block|}
while|while
condition|(
name|getNext
argument_list|(
name|i
argument_list|)
condition|)
block|{
if|if
condition|(
name|columnMatch
argument_list|(
name|i
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|ex
argument_list|)
expr_stmt|;
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**      * The user didn't want to start scanning at the first row. This method      * seeks to the requested row.      *      * @param i         - which iterator to advance      * @param firstRow  - seek to this row      * @return          - true if this is the first row or if the row was not found      */
annotation|@
name|Override
name|boolean
name|findFirstRow
parameter_list|(
name|int
name|i
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
name|ImmutableBytesWritable
name|ibw
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|firstKey
init|=
operator|(
name|HStoreKey
operator|)
name|readers
index|[
name|i
index|]
operator|.
name|getClosest
argument_list|(
operator|new
name|HStoreKey
argument_list|(
name|firstRow
argument_list|)
argument_list|,
name|ibw
argument_list|)
decl_stmt|;
if|if
condition|(
name|firstKey
operator|==
literal|null
condition|)
block|{
comment|// Didn't find it. Close the scanner and return TRUE
name|closeSubScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
name|this
operator|.
name|vals
index|[
name|i
index|]
operator|=
name|ibw
operator|.
name|get
argument_list|()
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setRow
argument_list|(
name|firstKey
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setColumn
argument_list|(
name|firstKey
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setVersion
argument_list|(
name|firstKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|columnMatch
argument_list|(
name|i
argument_list|)
return|;
block|}
comment|/**      * Get the next value from the specified reader.      *       * @param i - which reader to fetch next value from      * @return - true if there is more data available      */
annotation|@
name|Override
name|boolean
name|getNext
parameter_list|(
name|int
name|i
parameter_list|)
throws|throws
name|IOException
block|{
name|ImmutableBytesWritable
name|ibw
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|readers
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|ibw
argument_list|)
condition|)
block|{
name|closeSubScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
name|vals
index|[
name|i
index|]
operator|=
name|ibw
operator|.
name|get
argument_list|()
expr_stmt|;
return|return
literal|true
return|;
block|}
comment|/** Close down the indicated reader. */
annotation|@
name|Override
name|void
name|closeSubScanner
parameter_list|(
name|int
name|i
parameter_list|)
block|{
try|try
block|{
if|if
condition|(
name|readers
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|readers
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|readers
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|vals
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|/** Shut it down! */
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
block|{
if|if
condition|(
operator|!
name|scannerClosed
condition|)
block|{
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|readers
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|readers
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
name|scannerClosed
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
end_class

end_unit

