begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2007 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Vector
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataInputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|ImmutableBytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|MapFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|BloomFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|CountingBloomFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|Filter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|onelab
operator|.
name|filter
operator|.
name|RetouchedBloomFilter
import|;
end_import

begin_comment
comment|/**  * HStore maintains a bunch of data files.  It is responsible for maintaining   * the memory/file hierarchy and for periodic flushes to disk and compacting   * edits to the file.  *  * Locking and transactions are handled at a higher level.  This API should not   * be called directly by any writer, but rather by an HRegion manager.  */
end_comment

begin_class
class|class
name|HStore
implements|implements
name|HConstants
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HStore
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_TO_REPLACE
init|=
literal|"toreplace"
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_DONE
init|=
literal|"done"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|BLOOMFILTER_FILE_NAME
init|=
literal|"filter"
decl_stmt|;
name|Path
name|dir
decl_stmt|;
name|Text
name|regionName
decl_stmt|;
name|HColumnDescriptor
name|family
decl_stmt|;
name|Text
name|familyName
decl_stmt|;
name|SequenceFile
operator|.
name|CompressionType
name|compression
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
name|Configuration
name|conf
decl_stmt|;
name|Path
name|mapdir
decl_stmt|;
name|Path
name|loginfodir
decl_stmt|;
name|Path
name|filterDir
decl_stmt|;
name|Filter
name|bloomFilter
decl_stmt|;
specifier|private
name|String
name|storeName
decl_stmt|;
specifier|private
specifier|final
name|Path
name|compactionDir
decl_stmt|;
name|Integer
name|compactLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|Integer
name|flushLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|final
name|HLocking
name|lock
init|=
operator|new
name|HLocking
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|storefiles
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
name|readers
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
argument_list|()
decl_stmt|;
name|Random
name|rand
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
specifier|private
name|long
name|maxSeqId
decl_stmt|;
specifier|private
name|int
name|compactionThreshold
decl_stmt|;
comment|/**    * An HStore is a set of zero or more MapFiles, which stretch backwards over     * time.  A given HStore is responsible for a certain set of columns for a    * row in the HRegion.    *    *<p>The HRegion starts writing to its set of HStores when the HRegion's     * memcache is flushed.  This results in a round of new MapFiles, one for    * each HStore.    *    *<p>There's no reason to consider append-logging at this level; all logging     * and locking is handled at the HRegion level.  HStore just provides    * services to manage sets of MapFiles.  One of the most important of those    * services is MapFile-compaction services.    *    *<p>The only thing having to do with logs that HStore needs to deal with is    * the reconstructionLog.  This is a segment of an HRegion's log that might    * NOT be present upon startup.  If the param is NULL, there's nothing to do.    * If the param is non-NULL, we need to process the log to reconstruct    * a TreeMap that might not have been written to disk before the process    * died.    *    *<p>It's assumed that after this constructor returns, the reconstructionLog    * file will be deleted (by whoever has instantiated the HStore).    *    * @param dir log file directory    * @param regionName name of region    * @param family name of column family    * @param fs file system object    * @param reconstructionLog existing log file to apply if any    * @param conf configuration object    * @throws IOException    */
name|HStore
parameter_list|(
name|Path
name|dir
parameter_list|,
name|Text
name|regionName
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|reconstructionLog
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
name|this
operator|.
name|compactionDir
operator|=
operator|new
name|Path
argument_list|(
name|dir
argument_list|,
literal|"compaction.dir"
argument_list|)
expr_stmt|;
name|this
operator|.
name|regionName
operator|=
name|regionName
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|familyName
operator|=
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|NONE
expr_stmt|;
name|this
operator|.
name|storeName
operator|=
name|this
operator|.
name|regionName
operator|.
name|toString
argument_list|()
operator|+
literal|"/"
operator|+
name|this
operator|.
name|familyName
operator|.
name|toString
argument_list|()
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|!=
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|NONE
condition|)
block|{
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|BLOCK
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|BLOCK
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|family
operator|.
name|getCompression
argument_list|()
operator|==
name|HColumnDescriptor
operator|.
name|CompressionType
operator|.
name|RECORD
condition|)
block|{
name|this
operator|.
name|compression
operator|=
name|SequenceFile
operator|.
name|CompressionType
operator|.
name|RECORD
expr_stmt|;
block|}
else|else
block|{
assert|assert
operator|(
literal|false
operator|)
assert|;
block|}
block|}
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|mapdir
operator|=
name|HStoreFile
operator|.
name|getMapDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|mapdir
argument_list|)
expr_stmt|;
name|this
operator|.
name|loginfodir
operator|=
name|HStoreFile
operator|.
name|getInfoDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|loginfodir
argument_list|)
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|==
literal|null
condition|)
block|{
name|this
operator|.
name|filterDir
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|bloomFilter
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|filterDir
operator|=
name|HStoreFile
operator|.
name|getFilterDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|filterDir
argument_list|)
expr_stmt|;
name|loadOrCreateBloomFilter
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"starting "
operator|+
name|this
operator|.
name|storeName
operator|+
operator|(
operator|(
name|reconstructionLog
operator|==
literal|null
operator|||
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
operator|)
condition|?
literal|" (no reconstruction log)"
else|:
literal|" with reconstruction log: "
operator|+
name|reconstructionLog
operator|.
name|toString
argument_list|()
operator|)
argument_list|)
expr_stmt|;
block|}
comment|// Go through the 'mapdir' and 'loginfodir' together, make sure that all
comment|// MapFiles are in a reliable state.  Every entry in 'mapdir' must have a
comment|// corresponding one in 'loginfodir'. Without a corresponding log info
comment|// file, the entry in 'mapdir' must be deleted.
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|hstoreFiles
init|=
name|HStoreFile
operator|.
name|loadHStoreFiles
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
name|fs
argument_list|)
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|hstoreFiles
control|)
block|{
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
argument_list|)
argument_list|,
name|hsf
argument_list|)
expr_stmt|;
block|}
comment|// Now go through all the HSTORE_LOGINFOFILEs and figure out the
comment|// most-recent log-seq-ID that's present.  The most-recent such ID means we
comment|// can ignore all log messages up to and including that ID (because they're
comment|// already reflected in the TreeMaps).
comment|//
comment|// If the HSTORE_LOGINFOFILE doesn't contain a number, just ignore it. That
comment|// means it was built prior to the previous run of HStore, and so it cannot
comment|// contain any updates also contained in the log.
name|long
name|maxSeqID
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|hstoreFiles
control|)
block|{
name|long
name|seqid
init|=
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|seqid
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|seqid
operator|>
name|maxSeqID
condition|)
block|{
name|maxSeqID
operator|=
name|seqid
expr_stmt|;
block|}
block|}
block|}
name|this
operator|.
name|maxSeqId
operator|=
name|maxSeqID
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"maximum sequence id for hstore "
operator|+
name|storeName
operator|+
literal|" is "
operator|+
name|this
operator|.
name|maxSeqId
argument_list|)
expr_stmt|;
block|}
name|doReconstructionLog
argument_list|(
name|reconstructionLog
argument_list|,
name|maxSeqId
argument_list|)
expr_stmt|;
comment|// By default, we compact if an HStore has more than
comment|// MIN_COMMITS_FOR_COMPACTION map files
name|this
operator|.
name|compactionThreshold
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
expr_stmt|;
comment|// We used to compact in here before bringing the store online.  Instead
comment|// get it online quick even if it needs compactions so we can start
comment|// taking updates as soon as possible (Once online, can take updates even
comment|// during a compaction).
comment|// Move maxSeqId on by one. Why here?  And not in HRegion?
name|this
operator|.
name|maxSeqId
operator|+=
literal|1
expr_stmt|;
comment|// Finally, start up all the map readers! (There should be just one at this
comment|// point, as we've compacted them all.)
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|,
name|e
operator|.
name|getValue
argument_list|()
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|this
operator|.
name|maxSeqId
return|;
block|}
comment|/*    * Read the reconstructionLog to see whether we need to build a brand-new     * MapFile out of non-flushed log entries.      *    * We can ignore any log message that has a sequence ID that's equal to or     * lower than maxSeqID.  (Because we know such log messages are already     * reflected in the MapFiles.)    */
specifier|private
name|void
name|doReconstructionLog
parameter_list|(
specifier|final
name|Path
name|reconstructionLog
parameter_list|,
specifier|final
name|long
name|maxSeqID
parameter_list|)
throws|throws
name|UnsupportedEncodingException
throws|,
name|IOException
block|{
if|if
condition|(
name|reconstructionLog
operator|==
literal|null
operator|||
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
condition|)
block|{
comment|// Nothing to do.
return|return;
block|}
name|long
name|maxSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|reconstructedCache
init|=
operator|new
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|()
decl_stmt|;
name|SequenceFile
operator|.
name|Reader
name|login
init|=
operator|new
name|SequenceFile
operator|.
name|Reader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|reconstructionLog
argument_list|,
name|this
operator|.
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|HLogKey
name|key
init|=
operator|new
name|HLogKey
argument_list|()
decl_stmt|;
name|HLogEdit
name|val
init|=
operator|new
name|HLogEdit
argument_list|()
decl_stmt|;
while|while
condition|(
name|login
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
condition|)
block|{
name|maxSeqIdInLog
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSeqIdInLog
argument_list|,
name|key
operator|.
name|getLogSeqNum
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|<=
name|maxSeqID
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping edit<"
operator|+
name|key
operator|.
name|toString
argument_list|()
operator|+
literal|"="
operator|+
name|val
operator|.
name|toString
argument_list|()
operator|+
literal|"> key sequence: "
operator|+
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|+
literal|" max sequence: "
operator|+
name|maxSeqID
argument_list|)
expr_stmt|;
block|}
continue|continue;
block|}
comment|// Check this edit is for me. Also, guard against writing
comment|// METACOLUMN info such as HBASE::CACHEFLUSH entries
name|Text
name|column
init|=
name|val
operator|.
name|getColumn
argument_list|()
decl_stmt|;
if|if
condition|(
name|column
operator|.
name|equals
argument_list|(
name|HLog
operator|.
name|METACOLUMN
argument_list|)
operator|||
operator|!
name|key
operator|.
name|getRegionName
argument_list|()
operator|.
name|equals
argument_list|(
name|this
operator|.
name|regionName
argument_list|)
operator|||
operator|!
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|column
argument_list|)
operator|.
name|equals
argument_list|(
name|this
operator|.
name|familyName
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Passing on edit "
operator|+
name|key
operator|.
name|getRegionName
argument_list|()
operator|+
literal|", "
operator|+
name|column
operator|.
name|toString
argument_list|()
operator|+
literal|": "
operator|+
operator|new
name|String
argument_list|(
name|val
operator|.
name|getVal
argument_list|()
argument_list|,
name|UTF8_ENCODING
argument_list|)
operator|+
literal|", my region: "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|", my column: "
operator|+
name|this
operator|.
name|familyName
argument_list|)
expr_stmt|;
block|}
continue|continue;
block|}
name|HStoreKey
name|k
init|=
operator|new
name|HStoreKey
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|column
argument_list|,
name|val
operator|.
name|getTimestamp
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Applying edit<"
operator|+
name|k
operator|.
name|toString
argument_list|()
operator|+
literal|"="
operator|+
name|val
operator|.
name|toString
argument_list|()
operator|+
literal|">"
argument_list|)
expr_stmt|;
block|}
name|reconstructedCache
operator|.
name|put
argument_list|(
name|k
argument_list|,
name|val
operator|.
name|getVal
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|login
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|reconstructedCache
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// We create a "virtual flush" at maxSeqIdInLog+1.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing reconstructionCache"
argument_list|)
expr_stmt|;
block|}
name|flushCacheHelper
argument_list|(
name|reconstructedCache
argument_list|,
name|maxSeqIdInLog
operator|+
literal|1
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Bloom filters
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Called by constructor if a bloom filter is enabled for this column family.    * If the HStore already exists, it will read in the bloom filter saved    * previously. Otherwise, it will create a new bloom filter.    */
specifier|private
name|void
name|loadOrCreateBloomFilter
parameter_list|()
throws|throws
name|IOException
block|{
name|Path
name|filterFile
init|=
operator|new
name|Path
argument_list|(
name|filterDir
argument_list|,
name|BLOOMFILTER_FILE_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|filterFile
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loading bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
name|BloomFilterDescriptor
operator|.
name|BloomFilterType
name|type
init|=
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|filterType
decl_stmt|;
switch|switch
condition|(
name|type
condition|)
block|{
case|case
name|BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|BloomFilter
argument_list|()
expr_stmt|;
break|break;
case|case
name|COUNTING_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|CountingBloomFilter
argument_list|()
expr_stmt|;
break|break;
case|case
name|RETOUCHED_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|RetouchedBloomFilter
argument_list|()
expr_stmt|;
block|}
name|FSDataInputStream
name|in
init|=
name|fs
operator|.
name|open
argument_list|(
name|filterFile
argument_list|)
decl_stmt|;
name|bloomFilter
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
name|fs
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"creating bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
name|BloomFilterDescriptor
operator|.
name|BloomFilterType
name|type
init|=
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|filterType
decl_stmt|;
switch|switch
condition|(
name|type
condition|)
block|{
case|case
name|BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|BloomFilter
argument_list|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|vectorSize
argument_list|,
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|nbHash
argument_list|)
expr_stmt|;
break|break;
case|case
name|COUNTING_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|CountingBloomFilter
argument_list|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|vectorSize
argument_list|,
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|nbHash
argument_list|)
expr_stmt|;
break|break;
case|case
name|RETOUCHED_BLOOMFILTER
case|:
name|bloomFilter
operator|=
operator|new
name|RetouchedBloomFilter
argument_list|(
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|vectorSize
argument_list|,
name|family
operator|.
name|getBloomFilter
argument_list|()
operator|.
name|nbHash
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Flushes bloom filter to disk    *     * @throws IOException    */
specifier|private
name|void
name|flushBloomFilter
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
operator|new
name|Path
argument_list|(
name|filterDir
argument_list|,
name|BLOOMFILTER_FILE_NAME
argument_list|)
argument_list|)
decl_stmt|;
name|bloomFilter
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushed bloom filter for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// End bloom filters
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Close all the MapFile readers    * @throws IOException    */
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|result
init|=
literal|null
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
for|for
control|(
name|MapFile
operator|.
name|Reader
name|reader
range|:
name|this
operator|.
name|readers
operator|.
name|values
argument_list|()
control|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|readers
operator|.
name|clear
argument_list|()
expr_stmt|;
name|result
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|clear
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"closed "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Flush changes to disk
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Write out a brand-new set of items to the disk.    *    * We should only store key/vals that are appropriate for the data-columns     * stored in this HStore.    *    * Also, we are not expecting any reads of this MapFile just yet.    *    * Return the entire list of HStoreFiles currently used by the HStore.    *    * @param inputCache memcache to flush    * @param logCacheFlushId flush sequence number    * @throws IOException    */
name|void
name|flushCache
parameter_list|(
specifier|final
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|inputCache
parameter_list|,
specifier|final
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
name|flushCacheHelper
argument_list|(
name|inputCache
argument_list|,
name|logCacheFlushId
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|void
name|flushCacheHelper
parameter_list|(
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|inputCache
parameter_list|,
name|long
name|logCacheFlushId
parameter_list|,
name|boolean
name|addToAvailableMaps
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
comment|// A. Write the TreeMap out to the disk
name|HStoreFile
name|flushedFile
init|=
name|HStoreFile
operator|.
name|obtainNewHStoreFile
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|String
name|name
init|=
name|flushedFile
operator|.
name|toString
argument_list|()
decl_stmt|;
name|MapFile
operator|.
name|Writer
name|out
init|=
name|flushedFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
decl_stmt|;
comment|// hbase.hstore.compact.on.flush=true enables picking up an existing
comment|// HStoreFIle from disk interlacing the memcache flush compacting as we
comment|// go.  The notion is that interlacing would take as long as a pure
comment|// flush with the added benefit of having one less file in the store.
comment|// Experiments show that it takes two to three times the amount of time
comment|// flushing -- more column families makes it so the two timings come
comment|// closer together -- but it also complicates the flush. Disabled for
comment|// now.  Needs work picking which file to interlace (favor references
comment|// first, etc.)
comment|//
comment|// Related, looks like 'merging compactions' in BigTable paper interlaces
comment|// a memcache flush.  We don't.
try|try
block|{
if|if
condition|(
name|this
operator|.
name|conf
operator|.
name|getBoolean
argument_list|(
literal|"hbase.hstore.compact.on.flush"
argument_list|,
literal|false
argument_list|)
operator|&&
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|compact
argument_list|(
name|out
argument_list|,
name|inputCache
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
argument_list|,
name|this
operator|.
name|readers
operator|.
name|get
argument_list|(
name|this
operator|.
name|storefiles
operator|.
name|firstKey
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|es
range|:
name|inputCache
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreKey
name|curkey
init|=
name|es
operator|.
name|getKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|familyName
operator|.
name|equals
argument_list|(
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|curkey
operator|.
name|getColumn
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
name|out
operator|.
name|append
argument_list|(
name|curkey
argument_list|,
operator|new
name|ImmutableBytesWritable
argument_list|(
name|es
operator|.
name|getValue
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// B. Write out the log sequence number that corresponds to this output
comment|// MapFile.  The MapFile is current up to and including the log seq num.
name|flushedFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
comment|// C. Flush the bloom filter if any
if|if
condition|(
name|bloomFilter
operator|!=
literal|null
condition|)
block|{
name|flushBloomFilter
argument_list|()
expr_stmt|;
block|}
comment|// D. Finally, make the new MapFile available.
if|if
condition|(
name|addToAvailableMaps
condition|)
block|{
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Long
name|flushid
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|logCacheFlushId
argument_list|)
decl_stmt|;
comment|// Open the map file reader.
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|flushedFile
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|flushid
argument_list|,
name|flushedFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Added "
operator|+
name|name
operator|+
literal|" with sequence id "
operator|+
name|logCacheFlushId
operator|+
literal|" and size "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|flushedFile
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
return|return;
block|}
block|}
comment|/**    * @return - vector of all the HStore files in use    */
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|getAllStoreFiles
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * @return True if this store needs compaction.    */
specifier|public
name|boolean
name|needsCompaction
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|!=
literal|null
operator|&&
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>=
name|this
operator|.
name|compactionThreshold
return|;
block|}
comment|/**    * Compact the back-HStores.  This method may take some time, so the calling     * thread must be able to block for long periods.    *     *<p>During this time, the HStore can work as usual, getting values from    * MapFiles and writing new MapFiles from given memcaches.    *     * Existing MapFiles are not destroyed until the new compacted TreeMap is     * completely written-out to disk.    *    * The compactLock block prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *     * We don't want to hold the structureLock for the whole time, as a compact()     * can be lengthy and we want to allow cache-flushes during this period.    *     * @throws IOException    */
name|void
name|compact
parameter_list|()
throws|throws
name|IOException
block|{
name|compactHelper
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
name|void
name|compactHelper
parameter_list|(
specifier|final
name|boolean
name|deleteSequenceInfo
parameter_list|)
throws|throws
name|IOException
block|{
name|compactHelper
argument_list|(
name|deleteSequenceInfo
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
comment|/*     * @param deleteSequenceInfo True if we are to set the sequence number to -1    * on compacted file.    * @param maxSeenSeqID We may have already calculated the maxSeenSeqID.  If    * so, pass it here.  Otherwise, pass -1 and it will be calculated inside in    * this method.    * @throws IOException    */
name|void
name|compactHelper
parameter_list|(
specifier|final
name|boolean
name|deleteSequenceInfo
parameter_list|,
name|long
name|maxSeenSeqID
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|maxId
init|=
name|maxSeenSeqID
decl_stmt|;
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|this
operator|.
name|compactionDir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"started compaction of "
operator|+
name|storefiles
operator|.
name|size
argument_list|()
operator|+
literal|" files in "
operator|+
name|curCompactStore
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|curCompactStore
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Cleaning up a previous incomplete compaction at "
operator|+
name|curCompactStore
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|delete
argument_list|(
name|curCompactStore
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Deleted returned false on "
operator|+
name|curCompactStore
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
try|try
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
init|=
name|getFilesToCompact
argument_list|()
decl_stmt|;
name|HStoreFile
name|compactedOutputFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|this
operator|.
name|compactionDir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|toCompactFiles
operator|.
name|size
argument_list|()
operator|<
literal|1
operator|||
operator|(
name|toCompactFiles
operator|.
name|size
argument_list|()
operator|==
literal|1
operator|&&
operator|!
name|toCompactFiles
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|isReference
argument_list|()
operator|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"nothing to compact for "
operator|+
name|this
operator|.
name|storeName
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|deleteSequenceInfo
operator|&&
name|toCompactFiles
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
name|toCompactFiles
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|curCompactStore
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mkdir on "
operator|+
name|curCompactStore
operator|.
name|toString
argument_list|()
operator|+
literal|" failed"
argument_list|)
expr_stmt|;
block|}
comment|// Compute the max-sequenceID seen in any of the to-be-compacted
comment|// TreeMaps if it hasn't been passed in to us.
if|if
condition|(
name|maxId
operator|==
operator|-
literal|1
condition|)
block|{
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|toCompactFiles
control|)
block|{
name|long
name|seqid
init|=
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|seqid
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|seqid
operator|>
name|maxId
condition|)
block|{
name|maxId
operator|=
name|seqid
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// Step through them, writing to the brand-new TreeMap
name|MapFile
operator|.
name|Writer
name|compactedOut
init|=
name|compactedOutputFile
operator|.
name|getWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
decl_stmt|;
try|try
block|{
name|compact
argument_list|(
name|compactedOut
argument_list|,
name|toCompactFiles
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|compactedOut
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.
if|if
condition|(
operator|(
operator|!
name|deleteSequenceInfo
operator|)
operator|&&
name|maxId
operator|>=
literal|0
condition|)
block|{
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|maxId
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
comment|// Write out a list of data files that we're replacing
name|Path
name|filesToReplace
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_TO_REPLACE
argument_list|)
decl_stmt|;
name|DataOutputStream
name|out
init|=
operator|new
name|DataOutputStream
argument_list|(
name|fs
operator|.
name|create
argument_list|(
name|filesToReplace
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|toCompactFiles
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|toCompactFiles
control|)
block|{
name|hsf
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Indicate that we're done.
name|Path
name|doneFile
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_DONE
argument_list|)
decl_stmt|;
operator|(
operator|new
name|DataOutputStream
argument_list|(
name|fs
operator|.
name|create
argument_list|(
name|doneFile
argument_list|)
argument_list|)
operator|)
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Move the compaction into place.
name|processReadyCompaction
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
comment|// Clean up the parent -- the region dir in the compactions directory.
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|curCompactStore
operator|.
name|getParent
argument_list|()
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|delete
argument_list|(
name|curCompactStore
operator|.
name|getParent
argument_list|()
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Delete returned false deleting "
operator|+
name|curCompactStore
operator|.
name|getParent
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
comment|/*    * @return list of files to compact    */
specifier|private
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|getFilesToCompact
parameter_list|()
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
init|=
literal|null
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|toCompactFiles
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|storefiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
return|return
name|toCompactFiles
return|;
block|}
comment|/*    * Compact passed<code>toCompactFiles</code> into<code>compactedOut</code>.     * We create a new set of MapFile.Reader objects so we don't screw up     * the caching associated with the currently-loaded ones. Our    * iteration-based access pattern is practically designed to ruin     * the cache.    *    * We work by opening a single MapFile.Reader for each file, and     * iterating through them in parallel.  We always increment the     * lowest-ranked one.  Updates to a single row/column will appear     * ranked by timestamp.  This allows us to throw out deleted values or    * obsolete versions.    * @param compactedOut    * @param toCompactFiles    * @throws IOException    */
name|void
name|compact
parameter_list|(
specifier|final
name|MapFile
operator|.
name|Writer
name|compactedOut
parameter_list|,
specifier|final
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|size
init|=
name|toCompactFiles
operator|.
name|size
argument_list|()
decl_stmt|;
name|CompactionReader
index|[]
name|rdrs
init|=
operator|new
name|CompactionReader
index|[
name|size
index|]
decl_stmt|;
name|int
name|index
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|toCompactFiles
control|)
block|{
try|try
block|{
name|rdrs
index|[
name|index
operator|++
index|]
operator|=
operator|new
name|MapFileCompactionReader
argument_list|(
name|hsf
operator|.
name|getReader
argument_list|(
name|fs
argument_list|,
name|bloomFilter
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// Add info about which file threw exception. It may not be in the
comment|// exception message so output a message here where we know the
comment|// culprit.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed with "
operator|+
name|e
operator|.
name|toString
argument_list|()
operator|+
literal|": "
operator|+
name|hsf
operator|.
name|toString
argument_list|()
operator|+
operator|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|?
literal|" "
operator|+
name|hsf
operator|.
name|getReference
argument_list|()
operator|.
name|toString
argument_list|()
else|:
literal|""
operator|)
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
block|}
try|try
block|{
name|compact
argument_list|(
name|compactedOut
argument_list|,
name|rdrs
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|rdrs
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|rdrs
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Exception closing reader"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
comment|/** Interface for generic reader for compactions */
interface|interface
name|CompactionReader
block|{
comment|/**      * Closes the reader      * @throws IOException      */
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
function_decl|;
comment|/**      * Get the next key/value pair      *       * @param key      * @param val      * @return true if more data was returned      * @throws IOException      */
specifier|public
name|boolean
name|next
parameter_list|(
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|val
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**      * Resets the reader      * @throws IOException      */
specifier|public
name|void
name|reset
parameter_list|()
throws|throws
name|IOException
function_decl|;
block|}
comment|/** A compaction reader for MapFile */
specifier|static
class|class
name|MapFileCompactionReader
implements|implements
name|CompactionReader
block|{
specifier|final
name|MapFile
operator|.
name|Reader
name|reader
decl_stmt|;
name|MapFileCompactionReader
parameter_list|(
specifier|final
name|MapFile
operator|.
name|Reader
name|r
parameter_list|)
block|{
name|this
operator|.
name|reader
operator|=
name|r
expr_stmt|;
block|}
comment|/** {@inheritDoc} */
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/** {@inheritDoc} */
specifier|public
name|boolean
name|next
parameter_list|(
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|val
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|this
operator|.
name|reader
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
return|;
block|}
comment|/** {@inheritDoc} */
specifier|public
name|void
name|reset
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|reader
operator|.
name|reset
argument_list|()
expr_stmt|;
block|}
block|}
name|void
name|compact
parameter_list|(
specifier|final
name|MapFile
operator|.
name|Writer
name|compactedOut
parameter_list|,
specifier|final
name|Iterator
argument_list|<
name|Entry
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|>
name|iterator
parameter_list|,
specifier|final
name|MapFile
operator|.
name|Reader
name|reader
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Make an instance of a CompactionReader that wraps the iterator.
name|CompactionReader
name|cr
init|=
operator|new
name|CompactionReader
argument_list|()
block|{
specifier|public
name|boolean
name|next
parameter_list|(
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|val
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
while|while
condition|(
name|iterator
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Entry
argument_list|<
name|HStoreKey
argument_list|,
name|byte
index|[]
argument_list|>
name|e
init|=
name|iterator
operator|.
name|next
argument_list|()
decl_stmt|;
name|HStoreKey
name|hsk
init|=
name|e
operator|.
name|getKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|familyName
operator|.
name|equals
argument_list|(
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|hsk
operator|.
name|getColumn
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
operator|(
operator|(
name|HStoreKey
operator|)
name|key
operator|)
operator|.
name|set
argument_list|(
name|hsk
argument_list|)
expr_stmt|;
operator|(
operator|(
name|ImmutableBytesWritable
operator|)
name|val
operator|)
operator|.
name|set
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
name|result
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
return|return
name|result
return|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unused"
argument_list|)
specifier|public
name|void
name|reset
parameter_list|()
throws|throws
name|IOException
block|{
comment|// noop.
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unused"
argument_list|)
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
comment|// noop.
block|}
block|}
decl_stmt|;
name|compact
argument_list|(
name|compactedOut
argument_list|,
operator|new
name|CompactionReader
index|[]
block|{
name|cr
block|,
operator|new
name|MapFileCompactionReader
argument_list|(
name|reader
argument_list|)
block|}
argument_list|)
expr_stmt|;
block|}
name|void
name|compact
parameter_list|(
specifier|final
name|MapFile
operator|.
name|Writer
name|compactedOut
parameter_list|,
specifier|final
name|CompactionReader
index|[]
name|rdrs
parameter_list|)
throws|throws
name|IOException
block|{
name|HStoreKey
index|[]
name|keys
init|=
operator|new
name|HStoreKey
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
name|ImmutableBytesWritable
index|[]
name|vals
init|=
operator|new
name|ImmutableBytesWritable
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
name|boolean
index|[]
name|done
init|=
operator|new
name|boolean
index|[
name|rdrs
operator|.
name|length
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|keys
index|[
name|i
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
name|vals
index|[
name|i
index|]
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
literal|false
expr_stmt|;
block|}
comment|// Now, advance through the readers in order.  This will have the
comment|// effect of a run-time sort of the entire dataset.
name|int
name|numDone
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|rdrs
index|[
name|i
index|]
operator|.
name|reset
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
operator|!
name|rdrs
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|vals
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
name|numDone
operator|++
expr_stmt|;
block|}
block|}
name|int
name|timesSeen
init|=
literal|0
decl_stmt|;
name|Text
name|lastRow
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
name|Text
name|lastColumn
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
while|while
condition|(
name|numDone
operator|<
name|done
operator|.
name|length
condition|)
block|{
comment|// Find the reader with the smallest key
name|int
name|smallestKey
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|rdrs
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|smallestKey
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|keys
index|[
name|i
index|]
operator|.
name|compareTo
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|)
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
block|}
block|}
comment|// Reflect the current key/val in the output
name|HStoreKey
name|sk
init|=
name|keys
index|[
name|smallestKey
index|]
decl_stmt|;
if|if
condition|(
name|lastRow
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
operator|&&
name|lastColumn
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
condition|)
block|{
name|timesSeen
operator|++
expr_stmt|;
block|}
else|else
block|{
name|timesSeen
operator|=
literal|1
expr_stmt|;
block|}
if|if
condition|(
name|timesSeen
operator|<=
name|family
operator|.
name|getMaxVersions
argument_list|()
condition|)
block|{
comment|// Keep old versions until we have maxVersions worth.
comment|// Then just skip them.
if|if
condition|(
name|sk
operator|.
name|getRow
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
operator|&&
name|sk
operator|.
name|getColumn
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
condition|)
block|{
comment|// Only write out objects which have a non-zero length key and
comment|// value
name|compactedOut
operator|.
name|append
argument_list|(
name|sk
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
expr_stmt|;
block|}
block|}
comment|// TODO: I don't know what to do about deleted values.  I currently
comment|// include the fact that the item was deleted as a legitimate
comment|// "version" of the data.  Maybe it should just drop the deleted
comment|// val?
comment|// Update last-seen items
name|lastRow
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|lastColumn
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
comment|// Advance the smallest key.  If that reader's all finished, then
comment|// mark it as done.
if|if
condition|(
operator|!
name|rdrs
index|[
name|smallestKey
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
condition|)
block|{
name|done
index|[
name|smallestKey
index|]
operator|=
literal|true
expr_stmt|;
name|rdrs
index|[
name|smallestKey
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
name|rdrs
index|[
name|smallestKey
index|]
operator|=
literal|null
expr_stmt|;
name|numDone
operator|++
expr_stmt|;
block|}
block|}
block|}
comment|/*    * It's assumed that the compactLock  will be acquired prior to calling this     * method!  Otherwise, it is not thread-safe!    *    * It works by processing a compaction that's been written to disk.    *     *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *     *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Acquiring the write-lock    * 2) Figuring out what MapFiles are going to be replaced    * 3) Moving the new compacted MapFile into place    * 4) Unloading all the replaced MapFiles.    * 5) Deleting all the old MapFile files.    * 6) Loading the new TreeMap.    * 7) Releasing the write-lock    *</pre>    */
name|void
name|processReadyCompaction
parameter_list|()
throws|throws
name|IOException
block|{
comment|// 1. Acquiring the write-lock
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|this
operator|.
name|compactionDir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Path
name|doneFile
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_DONE
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|doneFile
argument_list|)
condition|)
block|{
comment|// The last execution didn't finish the compaction, so there's nothing
comment|// we can do.  We'll just have to redo it. Abandon it and return.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Redo failed compaction (missing 'done' file)"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// 2. Load in the files to be deleted.
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
init|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|Path
name|filesToReplace
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_TO_REPLACE
argument_list|)
decl_stmt|;
name|DataInputStream
name|in
init|=
operator|new
name|DataInputStream
argument_list|(
name|fs
operator|.
name|open
argument_list|(
name|filesToReplace
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|int
name|numfiles
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numfiles
condition|;
name|i
operator|++
control|)
block|{
name|HStoreFile
name|hsf
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|hsf
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
name|toCompactFiles
operator|.
name|add
argument_list|(
name|hsf
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// 3. Moving the new MapFile into place.
name|HStoreFile
name|compactedFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|this
operator|.
name|compactionDir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
comment|// obtainNewHStoreFile does its best to generate a filename that does not
comment|// currently exist.
name|HStoreFile
name|finalCompactedFile
init|=
name|HStoreFile
operator|.
name|obtainNewHStoreFile
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|familyName
argument_list|,
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"moving "
operator|+
name|compactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|" in "
operator|+
name|this
operator|.
name|compactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" to "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|" in "
operator|+
name|dir
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|compactedFile
operator|.
name|rename
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|finalCompactedFile
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// 4. and 5. Unload all the replaced MapFiles, close and delete.
name|Vector
argument_list|<
name|Long
argument_list|>
name|toDelete
init|=
operator|new
name|Vector
argument_list|<
name|Long
argument_list|>
argument_list|(
name|toCompactFiles
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|this
operator|.
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|toCompactFiles
operator|.
name|contains
argument_list|(
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Long
name|key
init|=
name|e
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|MapFile
operator|.
name|Reader
name|reader
init|=
name|this
operator|.
name|readers
operator|.
name|remove
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|toDelete
operator|.
name|add
argument_list|(
name|key
argument_list|)
expr_stmt|;
block|}
try|try
block|{
for|for
control|(
name|Long
name|key
range|:
name|toDelete
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|this
operator|.
name|storefiles
operator|.
name|remove
argument_list|(
name|key
argument_list|)
decl_stmt|;
name|hsf
operator|.
name|delete
argument_list|()
expr_stmt|;
block|}
comment|// 6. Loading the new TreeMap.
name|Long
name|orderVal
init|=
name|Long
operator|.
name|valueOf
argument_list|(
name|finalCompactedFile
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
name|this
operator|.
name|readers
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|finalCompactedFile
operator|.
name|getReader
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|this
operator|.
name|bloomFilter
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|finalCompactedFile
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files. Compacted file is "
operator|+
name|finalCompactedFile
operator|.
name|toString
argument_list|()
operator|+
literal|".  Files replaced are "
operator|+
name|toCompactFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
comment|// 7. Releasing the write-lock
name|this
operator|.
name|lock
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return all the available columns for the given key.  The key indicates a     * row and timestamp, but not a column name.    *    * The returned object should map column names to byte arrays (byte[]).    */
name|void
name|getFull
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|byte
index|[]
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
continue|continue;
block|}
do|do
block|{
name|Text
name|readcol
init|=
name|readkey
operator|.
name|getColumn
argument_list|()
decl_stmt|;
if|if
condition|(
name|results
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|==
literal|null
operator|&&
name|key
operator|.
name|matchesWithoutColumn
argument_list|(
name|readkey
argument_list|)
condition|)
block|{
if|if
condition|(
name|readval
operator|.
name|equals
argument_list|(
name|HGlobals
operator|.
name|deleteBytes
argument_list|)
condition|)
block|{
break|break;
block|}
name|results
operator|.
name|put
argument_list|(
operator|new
name|Text
argument_list|(
name|readcol
argument_list|)
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|key
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
break|break;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
block|}
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|MapFile
operator|.
name|Reader
index|[]
name|getReaders
parameter_list|()
block|{
return|return
name|this
operator|.
name|readers
operator|.
name|values
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|this
operator|.
name|readers
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
comment|/**    * Get the value for the indicated HStoreKey.  Grab the target value and the     * previous 'numVersions-1' values, as well.    *    * If 'numVersions' is negative, the method returns all available versions.    */
name|byte
index|[]
index|[]
name|get
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|numVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|byte
index|[]
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|()
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|getReaders
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|ImmutableBytesWritable
name|readval
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|==
literal|null
condition|)
block|{
comment|// map.getClosest returns null if the passed key is> than the
comment|// last key in the map file.  getClosest is a bit of a misnomer
comment|// since it returns exact match or the next closest key AFTER not
comment|// BEFORE.
continue|continue;
block|}
if|if
condition|(
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
if|if
condition|(
name|readval
operator|.
name|equals
argument_list|(
name|HGlobals
operator|.
name|deleteBytes
argument_list|)
condition|)
block|{
break|break;
block|}
name|results
operator|.
name|add
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
while|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
operator|&&
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
if|if
condition|(
operator|(
name|numVersions
operator|>
literal|0
operator|&&
operator|(
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
operator|)
operator|)
operator|||
name|readval
operator|.
name|equals
argument_list|(
name|HGlobals
operator|.
name|deleteBytes
argument_list|)
condition|)
block|{
break|break;
block|}
name|results
operator|.
name|add
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|ImmutableBytesWritable
argument_list|()
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
condition|)
block|{
break|break;
block|}
block|}
return|return
name|results
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|?
literal|null
else|:
name|ImmutableBytesWritable
operator|.
name|toArray
argument_list|(
name|results
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Data structure to hold result of a look at store file sizes.    */
specifier|static
class|class
name|HStoreSize
block|{
specifier|final
name|long
name|aggregate
decl_stmt|;
specifier|final
name|long
name|largest
decl_stmt|;
name|boolean
name|splitable
decl_stmt|;
name|HStoreSize
parameter_list|(
specifier|final
name|long
name|a
parameter_list|,
specifier|final
name|long
name|l
parameter_list|,
specifier|final
name|boolean
name|s
parameter_list|)
block|{
name|this
operator|.
name|aggregate
operator|=
name|a
expr_stmt|;
name|this
operator|.
name|largest
operator|=
name|l
expr_stmt|;
name|this
operator|.
name|splitable
operator|=
name|s
expr_stmt|;
block|}
name|long
name|getAggregate
parameter_list|()
block|{
return|return
name|this
operator|.
name|aggregate
return|;
block|}
name|long
name|getLargest
parameter_list|()
block|{
return|return
name|this
operator|.
name|largest
return|;
block|}
name|boolean
name|isSplitable
parameter_list|()
block|{
return|return
name|this
operator|.
name|splitable
return|;
block|}
name|void
name|setSplitable
parameter_list|(
specifier|final
name|boolean
name|s
parameter_list|)
block|{
name|this
operator|.
name|splitable
operator|=
name|s
expr_stmt|;
block|}
block|}
comment|/**    * Gets size for the store.    *     * @param midKey Gets set to the middle key of the largest splitable store    * file or its set to empty if largest is not splitable.    * @return Sizes for the store and the passed<code>midKey</code> is    * set to midKey of largest splitable.  Otherwise, its set to empty    * to indicate we couldn't find a midkey to split on    */
name|HStoreSize
name|size
parameter_list|(
name|Text
name|midKey
parameter_list|)
block|{
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|long
name|aggregateSize
init|=
literal|0L
decl_stmt|;
comment|// Not splitable if we find a reference store file present in the store.
name|boolean
name|splitable
init|=
literal|true
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
operator|new
name|HStoreSize
argument_list|(
literal|0
argument_list|,
literal|0
argument_list|,
name|splitable
argument_list|)
return|;
block|}
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|Long
name|mapIndex
init|=
name|Long
operator|.
name|valueOf
argument_list|(
literal|0L
argument_list|)
decl_stmt|;
comment|// Iterate through all the MapFiles
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
range|:
name|storefiles
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|HStoreFile
name|curHSF
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|long
name|size
init|=
name|curHSF
operator|.
name|length
argument_list|()
decl_stmt|;
name|aggregateSize
operator|+=
name|size
expr_stmt|;
if|if
condition|(
name|maxSize
operator|==
literal|0L
operator|||
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|mapIndex
operator|=
name|e
operator|.
name|getKey
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|splitable
condition|)
block|{
name|splitable
operator|=
operator|!
name|curHSF
operator|.
name|isReference
argument_list|()
expr_stmt|;
block|}
block|}
name|MapFile
operator|.
name|Reader
name|r
init|=
name|this
operator|.
name|readers
operator|.
name|get
argument_list|(
name|mapIndex
argument_list|)
decl_stmt|;
name|WritableComparable
name|midkey
init|=
name|r
operator|.
name|midKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|midkey
operator|!=
literal|null
condition|)
block|{
name|midKey
operator|.
name|set
argument_list|(
operator|(
operator|(
name|HStoreKey
operator|)
name|midkey
operator|)
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|""
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
return|return
operator|new
name|HStoreSize
argument_list|(
name|aggregateSize
argument_list|,
name|maxSize
argument_list|,
name|splitable
argument_list|)
return|;
block|}
comment|/**    * @return    Returns the number of map files currently in use    */
name|int
name|countOfStoreFiles
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
name|boolean
name|hasReferences
parameter_list|()
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
for|for
control|(
name|HStoreFile
name|hsf
range|:
name|this
operator|.
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|)
block|{
break|break;
block|}
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/** Generate a random unique filename suffix */
name|String
name|obtainFileLabel
parameter_list|(
name|Path
name|prefix
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|testsuffix
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|rand
operator|.
name|nextInt
argument_list|(
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
decl_stmt|;
name|Path
name|testpath
init|=
operator|new
name|Path
argument_list|(
name|prefix
operator|.
name|toString
argument_list|()
operator|+
name|testsuffix
argument_list|)
decl_stmt|;
while|while
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|testpath
argument_list|)
condition|)
block|{
name|testsuffix
operator|=
name|String
operator|.
name|valueOf
argument_list|(
name|rand
operator|.
name|nextInt
argument_list|(
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
expr_stmt|;
name|testpath
operator|=
operator|new
name|Path
argument_list|(
name|prefix
operator|.
name|toString
argument_list|()
operator|+
name|testsuffix
argument_list|)
expr_stmt|;
block|}
return|return
name|testsuffix
return|;
block|}
comment|/**    * Return a set of MapFile.Readers, one for each HStore file.    * These should be closed after the user is done with them.    */
name|HInternalScannerInterface
name|getScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|Text
name|targetCols
index|[]
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|new
name|HStoreScanner
argument_list|(
name|timestamp
argument_list|,
name|targetCols
argument_list|,
name|firstRow
argument_list|)
return|;
block|}
comment|/** {@inheritDoc} */
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|this
operator|.
name|storeName
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// This class implements the HScannerInterface.
comment|// It lets the caller scan the contents of this HStore.
comment|//////////////////////////////////////////////////////////////////////////////
class|class
name|HStoreScanner
extends|extends
name|HAbstractScanner
block|{
annotation|@
name|SuppressWarnings
argument_list|(
literal|"hiding"
argument_list|)
specifier|private
name|MapFile
operator|.
name|Reader
index|[]
name|readers
decl_stmt|;
name|HStoreScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|Text
index|[]
name|targetCols
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|timestamp
argument_list|,
name|targetCols
argument_list|)
expr_stmt|;
name|lock
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|this
operator|.
name|readers
operator|=
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|storefiles
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
comment|// Most recent map file should be first
name|int
name|i
init|=
name|readers
operator|.
name|length
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|HStoreFile
name|curHSF
range|:
name|storefiles
operator|.
name|values
argument_list|()
control|)
block|{
name|readers
index|[
name|i
operator|--
index|]
operator|=
name|curHSF
operator|.
name|getReader
argument_list|(
name|fs
argument_list|,
name|bloomFilter
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|keys
operator|=
operator|new
name|HStoreKey
index|[
name|readers
operator|.
name|length
index|]
expr_stmt|;
name|this
operator|.
name|vals
operator|=
operator|new
name|byte
index|[
name|readers
operator|.
name|length
index|]
index|[]
expr_stmt|;
comment|// Advance the readers to the first pos.
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|keys
index|[
name|i
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
if|if
condition|(
name|firstRow
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|findFirstRow
argument_list|(
name|i
argument_list|,
name|firstRow
argument_list|)
condition|)
block|{
continue|continue;
block|}
block|}
while|while
condition|(
name|getNext
argument_list|(
name|i
argument_list|)
condition|)
block|{
if|if
condition|(
name|columnMatch
argument_list|(
name|i
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed construction"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
name|close
argument_list|()
expr_stmt|;
name|IOException
name|e
init|=
operator|new
name|IOException
argument_list|(
literal|"HStoreScanner failed construction"
argument_list|)
decl_stmt|;
name|e
operator|.
name|initCause
argument_list|(
name|ex
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
block|}
comment|/**      * The user didn't want to start scanning at the first row. This method      * seeks to the requested row.      *      * @param i         - which iterator to advance      * @param firstRow  - seek to this row      * @return          - true if this is the first row or if the row was not found      */
annotation|@
name|Override
name|boolean
name|findFirstRow
parameter_list|(
name|int
name|i
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
name|ImmutableBytesWritable
name|ibw
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
name|HStoreKey
name|firstKey
init|=
operator|(
name|HStoreKey
operator|)
name|readers
index|[
name|i
index|]
operator|.
name|getClosest
argument_list|(
operator|new
name|HStoreKey
argument_list|(
name|firstRow
argument_list|)
argument_list|,
name|ibw
argument_list|)
decl_stmt|;
if|if
condition|(
name|firstKey
operator|==
literal|null
condition|)
block|{
comment|// Didn't find it. Close the scanner and return TRUE
name|closeSubScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
name|this
operator|.
name|vals
index|[
name|i
index|]
operator|=
name|ibw
operator|.
name|get
argument_list|()
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setRow
argument_list|(
name|firstKey
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setColumn
argument_list|(
name|firstKey
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setVersion
argument_list|(
name|firstKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|columnMatch
argument_list|(
name|i
argument_list|)
return|;
block|}
comment|/**      * Get the next value from the specified reader.      *       * @param i - which reader to fetch next value from      * @return - true if there is more data available      */
annotation|@
name|Override
name|boolean
name|getNext
parameter_list|(
name|int
name|i
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
name|ImmutableBytesWritable
name|ibw
init|=
operator|new
name|ImmutableBytesWritable
argument_list|()
decl_stmt|;
while|while
condition|(
literal|true
condition|)
block|{
if|if
condition|(
operator|!
name|readers
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|ibw
argument_list|)
condition|)
block|{
name|closeSubScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
break|break;
block|}
if|if
condition|(
name|keys
index|[
name|i
index|]
operator|.
name|getTimestamp
argument_list|()
operator|<=
name|this
operator|.
name|timestamp
condition|)
block|{
name|vals
index|[
name|i
index|]
operator|=
name|ibw
operator|.
name|get
argument_list|()
expr_stmt|;
name|result
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
return|return
name|result
return|;
block|}
comment|/** Close down the indicated reader. */
annotation|@
name|Override
name|void
name|closeSubScanner
parameter_list|(
name|int
name|i
parameter_list|)
block|{
try|try
block|{
if|if
condition|(
name|readers
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|readers
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Sub-scanner close"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|readers
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|vals
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|/** Shut it down! */
specifier|public
name|void
name|close
parameter_list|()
block|{
if|if
condition|(
operator|!
name|scannerClosed
condition|)
block|{
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|readers
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|readers
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Scanner close"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
finally|finally
block|{
name|lock
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
name|scannerClosed
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
end_class

end_unit

