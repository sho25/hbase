begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2006 The Apache Software Foundation  *  * Licensed under the Apache License, Version 2.0 (the "License");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|*
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|*
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|*
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|*
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|*
import|;
end_import

begin_comment
comment|/*******************************************************************************  * HStore maintains a bunch of data files.  It is responsible for maintaining   * the memory/file hierarchy and for periodic flushes to disk and compacting   * edits to the file.  *  * Locking and transactions are handled at a higher level.  This API should not   * be called directly by any writer, but rather by an HRegion manager.  ******************************************************************************/
end_comment

begin_class
specifier|public
class|class
name|HStore
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HStore
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_DIR
init|=
literal|"compaction.tmp"
decl_stmt|;
specifier|static
specifier|final
name|String
name|WORKING_COMPACTION
init|=
literal|"compaction.inprogress"
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_TO_REPLACE
init|=
literal|"toreplace"
decl_stmt|;
specifier|static
specifier|final
name|String
name|COMPACTION_DONE
init|=
literal|"done"
decl_stmt|;
name|Path
name|dir
decl_stmt|;
name|Text
name|regionName
decl_stmt|;
name|Text
name|colFamily
decl_stmt|;
name|int
name|maxVersions
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
name|Configuration
name|conf
decl_stmt|;
name|Path
name|mapdir
decl_stmt|;
name|Path
name|compactdir
decl_stmt|;
name|Path
name|loginfodir
decl_stmt|;
name|Integer
name|compactLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|Integer
name|flushLock
init|=
operator|new
name|Integer
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|HLocking
name|locking
init|=
operator|new
name|HLocking
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
name|maps
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|MapFile
operator|.
name|Reader
argument_list|>
argument_list|()
decl_stmt|;
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|mapFiles
init|=
operator|new
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|Random
name|rand
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Constructors, destructors, etc
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * An HStore is a set of zero or more MapFiles, which stretch backwards over     * time.  A given HStore is responsible for a certain set of columns for a row    * in the HRegion.    *    * The HRegion starts writing to its set of HStores when the HRegion's     * memcache is flushed.  This results in a round of new MapFiles, one for    * each HStore.    *    * There's no reason to consider append-logging at this level; all logging and     * locking is handled at the HRegion level.  HStore just provides services to     * manage sets of MapFiles.  One of the most important of those services is     * MapFile-compaction services.    *    * The only thing having to do with logs that HStore needs to deal with is    * the reconstructionLog.  This is a segment of an HRegion's log that might    * be present upon startup.  If the param is NULL, there's nothing to do.    * If the param is non-NULL, we need to process the log to reconstruct    * a TreeMap that might not have been written to disk before the process died.    *    * It's assumed that after this constructor returns, the reconstructionLog file    * will be deleted (by whoever has instantiated the HStore).    */
specifier|public
name|HStore
parameter_list|(
name|Path
name|dir
parameter_list|,
name|Text
name|regionName
parameter_list|,
name|Text
name|colFamily
parameter_list|,
name|int
name|maxVersions
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|reconstructionLog
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
name|this
operator|.
name|regionName
operator|=
name|regionName
expr_stmt|;
name|this
operator|.
name|colFamily
operator|=
name|colFamily
expr_stmt|;
name|this
operator|.
name|maxVersions
operator|=
name|maxVersions
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|mapdir
operator|=
name|HStoreFile
operator|.
name|getMapDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|mapdir
argument_list|)
expr_stmt|;
name|this
operator|.
name|loginfodir
operator|=
name|HStoreFile
operator|.
name|getInfoDir
argument_list|(
name|dir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|)
expr_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|loginfodir
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"starting HStore for "
operator|+
name|regionName
operator|+
literal|"/"
operator|+
name|colFamily
argument_list|)
expr_stmt|;
comment|// Either restart or get rid of any leftover compaction work.  Either way,
comment|// by the time processReadyCompaction() returns, we can get rid of the
comment|// existing compaction-dir.
name|this
operator|.
name|compactdir
operator|=
operator|new
name|Path
argument_list|(
name|dir
argument_list|,
name|COMPACTION_DIR
argument_list|)
expr_stmt|;
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|curCompactStore
argument_list|)
condition|)
block|{
name|processReadyCompaction
argument_list|()
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|curCompactStore
argument_list|)
expr_stmt|;
block|}
comment|// Go through the 'mapdir' and 'loginfodir' together, make sure that all
comment|// MapFiles are in a reliable state.  Every entry in 'mapdir' must have a
comment|// corresponding one in 'loginfodir'. Without a corresponding log info file,
comment|// the entry in 'mapdir'must be deleted.
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|hstoreFiles
init|=
name|HStoreFile
operator|.
name|loadHStoreFiles
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|,
name|fs
argument_list|)
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|hstoreFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|mapFiles
operator|.
name|put
argument_list|(
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
argument_list|,
name|hsf
argument_list|)
expr_stmt|;
block|}
comment|// Now go through all the HSTORE_LOGINFOFILEs and figure out the most-recent
comment|// log-seq-ID that's present.  The most-recent such ID means we can ignore
comment|// all log messages up to and including that ID (because they're already
comment|// reflected in the TreeMaps).
comment|//
comment|// If the HSTORE_LOGINFOFILE doesn't contain a number, just ignore it.  That
comment|// means it was built prior to the previous run of HStore, and so it cannot
comment|// contain any updates also contained in the log.
name|long
name|maxSeqID
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|hstoreFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|long
name|seqid
init|=
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|seqid
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|seqid
operator|>
name|maxSeqID
condition|)
block|{
name|maxSeqID
operator|=
name|seqid
expr_stmt|;
block|}
block|}
block|}
comment|// Read the reconstructionLog to see whether we need to build a brand-new
comment|// MapFile out of non-flushed log entries.
comment|//
comment|// We can ignore any log message that has a sequence ID that's equal to or
comment|// lower than maxSeqID.  (Because we know such log messages are already
comment|// reflected in the MapFiles.)
name|LOG
operator|.
name|debug
argument_list|(
literal|"reading reconstructionLog"
argument_list|)
expr_stmt|;
if|if
condition|(
name|reconstructionLog
operator|!=
literal|null
operator|&&
name|fs
operator|.
name|exists
argument_list|(
name|reconstructionLog
argument_list|)
condition|)
block|{
name|long
name|maxSeqIdInLog
init|=
operator|-
literal|1
decl_stmt|;
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|BytesWritable
argument_list|>
name|reconstructedCache
init|=
operator|new
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|BytesWritable
argument_list|>
argument_list|()
decl_stmt|;
name|SequenceFile
operator|.
name|Reader
name|login
init|=
operator|new
name|SequenceFile
operator|.
name|Reader
argument_list|(
name|fs
argument_list|,
name|reconstructionLog
argument_list|,
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|HLogKey
name|key
init|=
operator|new
name|HLogKey
argument_list|()
decl_stmt|;
name|HLogEdit
name|val
init|=
operator|new
name|HLogEdit
argument_list|()
decl_stmt|;
while|while
condition|(
name|login
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
condition|)
block|{
name|maxSeqIdInLog
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSeqIdInLog
argument_list|,
name|key
operator|.
name|getLogSeqNum
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|key
operator|.
name|getLogSeqNum
argument_list|()
operator|<=
name|maxSeqID
condition|)
block|{
continue|continue;
block|}
name|reconstructedCache
operator|.
name|put
argument_list|(
operator|new
name|HStoreKey
argument_list|(
name|key
operator|.
name|getRow
argument_list|()
argument_list|,
name|val
operator|.
name|getColumn
argument_list|()
argument_list|,
name|val
operator|.
name|getTimestamp
argument_list|()
argument_list|)
argument_list|,
name|val
operator|.
name|getVal
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|login
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|reconstructedCache
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// We create a "virtual flush" at maxSeqIdInLog+1.
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing reconstructionCache"
argument_list|)
expr_stmt|;
name|flushCacheHelper
argument_list|(
name|reconstructedCache
argument_list|,
name|maxSeqIdInLog
operator|+
literal|1
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Compact all the MapFiles into a single file.  The resulting MapFile
comment|// should be "timeless"; that is, it should not have an associated seq-ID,
comment|// because all log messages have been reflected in the TreeMaps at this point.
if|if
condition|(
name|mapFiles
operator|.
name|size
argument_list|()
operator|>=
literal|1
condition|)
block|{
name|compactHelper
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
comment|// Finally, start up all the map readers! (There should be just one at this
comment|// point, as we've compacted them all.)
name|LOG
operator|.
name|debug
argument_list|(
literal|"starting map readers"
argument_list|)
expr_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|Long
argument_list|>
name|it
init|=
name|mapFiles
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|Long
name|key
init|=
name|it
operator|.
name|next
argument_list|()
operator|.
name|longValue
argument_list|()
decl_stmt|;
name|HStoreFile
name|hsf
init|=
name|mapFiles
operator|.
name|get
argument_list|(
name|key
argument_list|)
decl_stmt|;
comment|//TODO - is this really necessary?  Don't I do this inside compact()?
name|maps
operator|.
name|put
argument_list|(
name|key
argument_list|,
operator|new
name|MapFile
operator|.
name|Reader
argument_list|(
name|fs
argument_list|,
name|hsf
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"HStore online for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
block|}
comment|/** Turn off all the MapFile readers */
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|locking
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"closing HStore for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
try|try
block|{
for|for
control|(
name|Iterator
argument_list|<
name|MapFile
operator|.
name|Reader
argument_list|>
name|it
init|=
name|maps
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|map
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|maps
operator|.
name|clear
argument_list|()
expr_stmt|;
name|mapFiles
operator|.
name|clear
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"HStore closed for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|locking
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Flush changes to disk
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Write out a brand-new set of items to the disk.    *    * We should only store key/vals that are appropriate for the data-columns     * stored in this HStore.    *    * Also, we are not expecting any reads of this MapFile just yet.    *    * Return the entire list of HStoreFiles currently used by the HStore.    */
specifier|public
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|flushCache
parameter_list|(
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|BytesWritable
argument_list|>
name|inputCache
parameter_list|,
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|flushCacheHelper
argument_list|(
name|inputCache
argument_list|,
name|logCacheFlushId
argument_list|,
literal|true
argument_list|)
return|;
block|}
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|flushCacheHelper
parameter_list|(
name|TreeMap
argument_list|<
name|HStoreKey
argument_list|,
name|BytesWritable
argument_list|>
name|inputCache
parameter_list|,
name|long
name|logCacheFlushId
parameter_list|,
name|boolean
name|addToAvailableMaps
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"flushing HStore "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
comment|// A. Write the TreeMap out to the disk
name|HStoreFile
name|flushedFile
init|=
name|HStoreFile
operator|.
name|obtainNewHStoreFile
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|Path
name|mapfile
init|=
name|flushedFile
operator|.
name|getMapFilePath
argument_list|()
decl_stmt|;
name|MapFile
operator|.
name|Writer
name|out
init|=
operator|new
name|MapFile
operator|.
name|Writer
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|mapfile
operator|.
name|toString
argument_list|()
argument_list|,
name|HStoreKey
operator|.
name|class
argument_list|,
name|BytesWritable
operator|.
name|class
argument_list|)
decl_stmt|;
try|try
block|{
for|for
control|(
name|Iterator
argument_list|<
name|HStoreKey
argument_list|>
name|it
init|=
name|inputCache
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreKey
name|curkey
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|colFamily
operator|.
name|equals
argument_list|(
name|HStoreKey
operator|.
name|extractFamily
argument_list|(
name|curkey
operator|.
name|getColumn
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
name|BytesWritable
name|val
init|=
name|inputCache
operator|.
name|get
argument_list|(
name|curkey
argument_list|)
decl_stmt|;
name|out
operator|.
name|append
argument_list|(
name|curkey
argument_list|,
name|val
argument_list|)
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"HStore "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
operator|+
literal|" flushed"
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// B. Write out the log sequence number that corresponds to this output
comment|// MapFile.  The MapFile is current up to and including the log seq num.
name|LOG
operator|.
name|debug
argument_list|(
literal|"writing log cache flush id"
argument_list|)
expr_stmt|;
name|flushedFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|logCacheFlushId
argument_list|)
expr_stmt|;
comment|// C. Finally, make the new MapFile available.
if|if
condition|(
name|addToAvailableMaps
condition|)
block|{
name|locking
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|maps
operator|.
name|put
argument_list|(
name|logCacheFlushId
argument_list|,
operator|new
name|MapFile
operator|.
name|Reader
argument_list|(
name|fs
argument_list|,
name|mapfile
operator|.
name|toString
argument_list|()
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
name|mapFiles
operator|.
name|put
argument_list|(
name|logCacheFlushId
argument_list|,
name|flushedFile
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"HStore available for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|locking
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|getAllMapFiles
argument_list|()
return|;
block|}
block|}
specifier|public
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|getAllMapFiles
parameter_list|()
block|{
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|flushedFiles
init|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|mapFiles
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|flushedFiles
operator|.
name|add
argument_list|(
name|hsf
argument_list|)
expr_stmt|;
block|}
return|return
name|flushedFiles
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the back-HStores.  This method may take some time, so the calling     * thread must be able to block for long periods.    *     * During this time, the HStore can work as usual, getting values from MapFiles    * and writing new MapFiles from given memcaches.    *     * Existing MapFiles are not destroyed until the new compacted TreeMap is     * completely written-out to disk.    *    * The compactLock block prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *     * We don't want to hold the structureLock for the whole time, as a compact()     * can be lengthy and we want to allow cache-flushes during this period.    */
specifier|public
name|void
name|compact
parameter_list|()
throws|throws
name|IOException
block|{
name|compactHelper
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
name|void
name|compactHelper
parameter_list|(
name|boolean
name|deleteSequenceInfo
parameter_list|)
throws|throws
name|IOException
block|{
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"started compaction of "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|curCompactStore
argument_list|)
expr_stmt|;
try|try
block|{
comment|// Grab a list of files to compact.
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
init|=
literal|null
decl_stmt|;
name|locking
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
try|try
block|{
name|toCompactFiles
operator|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|(
name|mapFiles
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|locking
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
comment|// Compute the max-sequenceID seen in any of the to-be-compacted TreeMaps
name|long
name|maxSeenSeqID
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|toCompactFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|long
name|seqid
init|=
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|seqid
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|seqid
operator|>
name|maxSeenSeqID
condition|)
block|{
name|maxSeenSeqID
operator|=
name|seqid
expr_stmt|;
block|}
block|}
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"max sequence id ="
operator|+
name|maxSeenSeqID
argument_list|)
expr_stmt|;
name|HStoreFile
name|compactedOutputFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|toCompactFiles
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"nothing to compact for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
name|HStoreFile
name|hsf
init|=
name|toCompactFiles
operator|.
name|elementAt
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|hsf
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
operator|==
operator|-
literal|1
condition|)
block|{
return|return;
block|}
block|}
comment|// Step through them, writing to the brand-new TreeMap
name|MapFile
operator|.
name|Writer
name|compactedOut
init|=
operator|new
name|MapFile
operator|.
name|Writer
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|compactedOutputFile
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|HStoreKey
operator|.
name|class
argument_list|,
name|BytesWritable
operator|.
name|class
argument_list|)
decl_stmt|;
try|try
block|{
comment|// We create a new set of MapFile.Reader objects so we don't screw up
comment|// the caching associated with the currently-loaded ones.
comment|//
comment|// Our iteration-based access pattern is practically designed to ruin
comment|// the cache.
comment|//
comment|// We work by opening a single MapFile.Reader for each file, and
comment|// iterating through them in parallel.  We always increment the
comment|// lowest-ranked one.  Updates to a single row/column will appear
comment|// ranked by timestamp.  This allows us to throw out deleted values or
comment|// obsolete versions.
name|MapFile
operator|.
name|Reader
index|[]
name|readers
init|=
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|HStoreKey
index|[]
name|keys
init|=
operator|new
name|HStoreKey
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|BytesWritable
index|[]
name|vals
init|=
operator|new
name|BytesWritable
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|boolean
index|[]
name|done
init|=
operator|new
name|boolean
index|[
name|toCompactFiles
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|int
name|pos
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|toCompactFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|readers
index|[
name|pos
index|]
operator|=
operator|new
name|MapFile
operator|.
name|Reader
argument_list|(
name|fs
argument_list|,
name|hsf
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|keys
index|[
name|pos
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
name|vals
index|[
name|pos
index|]
operator|=
operator|new
name|BytesWritable
argument_list|()
expr_stmt|;
name|done
index|[
name|pos
index|]
operator|=
literal|false
expr_stmt|;
name|pos
operator|++
expr_stmt|;
block|}
comment|// Now, advance through the readers in order.  This will have the
comment|// effect of a run-time sort of the entire dataset.
name|LOG
operator|.
name|debug
argument_list|(
literal|"processing HStoreFile readers"
argument_list|)
expr_stmt|;
name|int
name|numDone
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|readers
index|[
name|i
index|]
operator|.
name|reset
argument_list|()
expr_stmt|;
name|done
index|[
name|i
index|]
operator|=
operator|!
name|readers
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|vals
index|[
name|i
index|]
argument_list|)
expr_stmt|;
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
name|numDone
operator|++
expr_stmt|;
block|}
block|}
name|int
name|timesSeen
init|=
literal|0
decl_stmt|;
name|Text
name|lastRow
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
name|Text
name|lastColumn
init|=
operator|new
name|Text
argument_list|()
decl_stmt|;
while|while
condition|(
name|numDone
operator|<
name|done
operator|.
name|length
condition|)
block|{
comment|// Find the reader with the smallest key
name|int
name|smallestKey
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|done
index|[
name|i
index|]
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|smallestKey
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|keys
index|[
name|i
index|]
operator|.
name|compareTo
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|)
operator|<
literal|0
condition|)
block|{
name|smallestKey
operator|=
name|i
expr_stmt|;
block|}
block|}
block|}
comment|// Reflect the current key/val in the output
name|HStoreKey
name|sk
init|=
name|keys
index|[
name|smallestKey
index|]
decl_stmt|;
if|if
condition|(
name|lastRow
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
operator|&&
name|lastColumn
operator|.
name|equals
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
condition|)
block|{
name|timesSeen
operator|++
expr_stmt|;
block|}
else|else
block|{
name|timesSeen
operator|=
literal|1
expr_stmt|;
block|}
if|if
condition|(
name|timesSeen
operator|<=
name|maxVersions
condition|)
block|{
comment|// Keep old versions until we have maxVersions worth.
comment|// Then just skip them.
if|if
condition|(
name|sk
operator|.
name|getRow
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
operator|&&
name|sk
operator|.
name|getColumn
argument_list|()
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
condition|)
block|{
comment|// Only write out objects which have a non-zero length key and value
name|compactedOut
operator|.
name|append
argument_list|(
name|sk
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
expr_stmt|;
block|}
block|}
comment|//TODO: I don't know what to do about deleted values.  I currently
comment|// include the fact that the item was deleted as a legitimate
comment|// "version" of the data.  Maybe it should just drop the deleted val?
comment|// Update last-seen items
name|lastRow
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|lastColumn
operator|.
name|set
argument_list|(
name|sk
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
comment|// Advance the smallest key.  If that reader's all finished, then
comment|// mark it as done.
if|if
condition|(
operator|!
name|readers
index|[
name|smallestKey
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|smallestKey
index|]
argument_list|,
name|vals
index|[
name|smallestKey
index|]
argument_list|)
condition|)
block|{
name|done
index|[
name|smallestKey
index|]
operator|=
literal|true
expr_stmt|;
name|readers
index|[
name|smallestKey
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
name|numDone
operator|++
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"all HStores processed"
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|compactedOut
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"writing new compacted HStore"
argument_list|)
expr_stmt|;
comment|// Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.
if|if
condition|(
operator|(
operator|!
name|deleteSequenceInfo
operator|)
operator|&&
name|maxSeenSeqID
operator|>=
literal|0
condition|)
block|{
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
name|maxSeenSeqID
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|compactedOutputFile
operator|.
name|writeInfo
argument_list|(
name|fs
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
comment|// Write out a list of data files that we're replacing
name|Path
name|filesToReplace
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_TO_REPLACE
argument_list|)
decl_stmt|;
name|DataOutputStream
name|out
init|=
operator|new
name|DataOutputStream
argument_list|(
name|fs
operator|.
name|create
argument_list|(
name|filesToReplace
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|out
operator|.
name|writeInt
argument_list|(
name|toCompactFiles
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|toCompactFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|hsf
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Indicate that we're done.
name|Path
name|doneFile
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_DONE
argument_list|)
decl_stmt|;
name|out
operator|=
operator|new
name|DataOutputStream
argument_list|(
name|fs
operator|.
name|create
argument_list|(
name|doneFile
argument_list|)
argument_list|)
expr_stmt|;
try|try
block|{         }
finally|finally
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Move the compaction into place.
name|processReadyCompaction
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"compaction complete for "
operator|+
name|this
operator|.
name|regionName
operator|+
literal|"/"
operator|+
name|this
operator|.
name|colFamily
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|fs
operator|.
name|delete
argument_list|(
name|compactdir
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * It's assumed that the compactLock  will be acquired prior to calling this     * method!  Otherwise, it is not thread-safe!    *    * It works by processing a compaction that's been written to disk.    *     * It is usually invoked at the end of a compaction, but might also be invoked    * at HStore startup, if the prior execution died midway through.    */
name|void
name|processReadyCompaction
parameter_list|()
throws|throws
name|IOException
block|{
comment|// Move the compacted TreeMap into place.
comment|// That means:
comment|// 1) Acquiring the write-lock
comment|// 2) Figuring out what MapFiles are going to be replaced
comment|// 3) Unloading all the replaced MapFiles.
comment|// 4) Deleting all the old MapFile files.
comment|// 5) Moving the new MapFile into place
comment|// 6) Loading the new TreeMap.
comment|// 7) Releasing the write-lock
comment|// 1. Acquiring the write-lock
name|locking
operator|.
name|obtainWriteLock
argument_list|()
expr_stmt|;
name|Path
name|curCompactStore
init|=
name|HStoreFile
operator|.
name|getHStoreDir
argument_list|(
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|)
decl_stmt|;
try|try
block|{
name|Path
name|doneFile
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_DONE
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|doneFile
argument_list|)
condition|)
block|{
comment|// The last execution didn't finish the compaction, so there's nothing
comment|// we can do.  We'll just have to redo it. Abandon it and return.
return|return;
block|}
comment|// OK, there's actually compaction work that needs to be put into place.
name|LOG
operator|.
name|debug
argument_list|(
literal|"compaction starting"
argument_list|)
expr_stmt|;
comment|// 2. Load in the files to be deleted.
comment|//    (Figuring out what MapFiles are going to be replaced)
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
name|toCompactFiles
init|=
operator|new
name|Vector
argument_list|<
name|HStoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|Path
name|filesToReplace
init|=
operator|new
name|Path
argument_list|(
name|curCompactStore
argument_list|,
name|COMPACTION_TO_REPLACE
argument_list|)
decl_stmt|;
name|DataInputStream
name|in
init|=
operator|new
name|DataInputStream
argument_list|(
name|fs
operator|.
name|open
argument_list|(
name|filesToReplace
argument_list|)
argument_list|)
decl_stmt|;
try|try
block|{
name|int
name|numfiles
init|=
name|in
operator|.
name|readInt
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numfiles
condition|;
name|i
operator|++
control|)
block|{
name|HStoreFile
name|hsf
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|hsf
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
name|toCompactFiles
operator|.
name|add
argument_list|(
name|hsf
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded files to be deleted"
argument_list|)
expr_stmt|;
comment|// 3. Unload all the replaced MapFiles.
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it2
init|=
name|mapFiles
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|MapFile
operator|.
name|Reader
argument_list|>
name|it
init|=
name|maps
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|MapFile
operator|.
name|Reader
name|curReader
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|HStoreFile
name|curMapFile
init|=
name|it2
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|toCompactFiles
operator|.
name|contains
argument_list|(
name|curMapFile
argument_list|)
condition|)
block|{
name|curReader
operator|.
name|close
argument_list|()
expr_stmt|;
name|it
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
block|}
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|mapFiles
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|curMapFile
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|toCompactFiles
operator|.
name|contains
argument_list|(
name|curMapFile
argument_list|)
condition|)
block|{
name|it
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"unloaded existing MapFiles"
argument_list|)
expr_stmt|;
comment|// What if we crash at this point?  No big deal; we will restart
comment|// processReadyCompaction(), and nothing has been lost.
comment|// 4. Delete all the old files, no longer needed
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|toCompactFiles
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|hsf
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|hsf
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|hsf
operator|.
name|getInfoFilePath
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"old files deleted"
argument_list|)
expr_stmt|;
comment|// What if we fail now?  The above deletes will fail silently. We'd better
comment|// make sure not to write out any new files with the same names as
comment|// something we delete, though.
comment|// 5. Moving the new MapFile into place
name|LOG
operator|.
name|debug
argument_list|(
literal|"moving new MapFile into place"
argument_list|)
expr_stmt|;
name|HStoreFile
name|compactedFile
init|=
operator|new
name|HStoreFile
argument_list|(
name|conf
argument_list|,
name|compactdir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
name|HStoreFile
name|finalCompactedFile
init|=
name|HStoreFile
operator|.
name|obtainNewHStoreFile
argument_list|(
name|conf
argument_list|,
name|dir
argument_list|,
name|regionName
argument_list|,
name|colFamily
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|fs
operator|.
name|rename
argument_list|(
name|compactedFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|,
name|finalCompactedFile
operator|.
name|getMapFilePath
argument_list|()
argument_list|)
expr_stmt|;
comment|// Fail here?  No problem.
name|fs
operator|.
name|rename
argument_list|(
name|compactedFile
operator|.
name|getInfoFilePath
argument_list|()
argument_list|,
name|finalCompactedFile
operator|.
name|getInfoFilePath
argument_list|()
argument_list|)
expr_stmt|;
comment|// Fail here?  No worries.
name|long
name|orderVal
init|=
name|finalCompactedFile
operator|.
name|loadInfo
argument_list|(
name|fs
argument_list|)
decl_stmt|;
comment|// 6. Loading the new TreeMap.
name|LOG
operator|.
name|debug
argument_list|(
literal|"loading new TreeMap"
argument_list|)
expr_stmt|;
name|mapFiles
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
name|finalCompactedFile
argument_list|)
expr_stmt|;
name|maps
operator|.
name|put
argument_list|(
name|orderVal
argument_list|,
operator|new
name|MapFile
operator|.
name|Reader
argument_list|(
name|fs
argument_list|,
name|finalCompactedFile
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// 7. Releasing the write-lock
name|locking
operator|.
name|releaseWriteLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return all the available columns for the given key.  The key indicates a     * row and timestamp, but not a column name.    *    * The returned object should map column names to byte arrays (byte[]).    */
specifier|public
name|void
name|getFull
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|TreeMap
argument_list|<
name|Text
argument_list|,
name|byte
index|[]
argument_list|>
name|results
parameter_list|)
throws|throws
name|IOException
block|{
name|locking
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|maps
operator|.
name|values
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|maps
operator|.
name|size
argument_list|()
index|]
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|BytesWritable
name|readval
init|=
operator|new
name|BytesWritable
argument_list|()
decl_stmt|;
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
do|do
block|{
name|Text
name|readcol
init|=
name|readkey
operator|.
name|getColumn
argument_list|()
decl_stmt|;
if|if
condition|(
name|results
operator|.
name|get
argument_list|(
name|readcol
argument_list|)
operator|==
literal|null
operator|&&
name|key
operator|.
name|matchesWithoutColumn
argument_list|(
name|readkey
argument_list|)
condition|)
block|{
name|results
operator|.
name|put
argument_list|(
operator|new
name|Text
argument_list|(
name|readcol
argument_list|)
argument_list|,
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|BytesWritable
argument_list|()
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|key
operator|.
name|getRow
argument_list|()
operator|.
name|compareTo
argument_list|(
name|readkey
operator|.
name|getRow
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
break|break;
block|}
block|}
do|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
condition|)
do|;
block|}
block|}
block|}
finally|finally
block|{
name|locking
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Get the value for the indicated HStoreKey.  Grab the target value and the     * previous 'numVersions-1' values, as well.    *    * If 'numVersions' is negative, the method returns all available versions.    */
specifier|public
name|byte
index|[]
index|[]
name|get
parameter_list|(
name|HStoreKey
name|key
parameter_list|,
name|int
name|numVersions
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|numVersions
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Must request at least one value."
argument_list|)
throw|;
block|}
name|Vector
argument_list|<
name|byte
index|[]
argument_list|>
name|results
init|=
operator|new
name|Vector
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|()
decl_stmt|;
name|locking
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|MapFile
operator|.
name|Reader
index|[]
name|maparray
init|=
name|maps
operator|.
name|values
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|maps
operator|.
name|size
argument_list|()
index|]
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|maparray
operator|.
name|length
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
name|i
operator|--
control|)
block|{
name|MapFile
operator|.
name|Reader
name|map
init|=
name|maparray
index|[
name|i
index|]
decl_stmt|;
synchronized|synchronized
init|(
name|map
init|)
block|{
name|BytesWritable
name|readval
init|=
operator|new
name|BytesWritable
argument_list|()
decl_stmt|;
name|map
operator|.
name|reset
argument_list|()
expr_stmt|;
name|HStoreKey
name|readkey
init|=
operator|(
name|HStoreKey
operator|)
name|map
operator|.
name|getClosest
argument_list|(
name|key
argument_list|,
name|readval
argument_list|)
decl_stmt|;
if|if
condition|(
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
name|results
operator|.
name|add
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|BytesWritable
argument_list|()
expr_stmt|;
while|while
condition|(
name|map
operator|.
name|next
argument_list|(
name|readkey
argument_list|,
name|readval
argument_list|)
operator|&&
name|readkey
operator|.
name|matchesRowCol
argument_list|(
name|key
argument_list|)
condition|)
block|{
if|if
condition|(
name|numVersions
operator|>
literal|0
operator|&&
operator|(
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
operator|)
condition|)
block|{
break|break;
block|}
else|else
block|{
name|results
operator|.
name|add
argument_list|(
name|readval
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|readval
operator|=
operator|new
name|BytesWritable
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
if|if
condition|(
name|results
operator|.
name|size
argument_list|()
operator|>=
name|numVersions
condition|)
block|{
break|break;
block|}
block|}
if|if
condition|(
name|results
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
else|else
block|{
return|return
operator|(
name|byte
index|[]
index|[]
operator|)
name|results
operator|.
name|toArray
argument_list|(
operator|new
name|byte
index|[
name|results
operator|.
name|size
argument_list|()
index|]
index|[]
argument_list|)
return|;
block|}
block|}
finally|finally
block|{
name|locking
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Gets the size of the largest MapFile and its mid key.    *     * @param midKey      - the middle key for the largest MapFile    * @return            - size of the largest MapFile    * @throws IOException    */
specifier|public
name|long
name|getLargestFileSize
parameter_list|(
name|Text
name|midKey
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|long
name|mapIndex
init|=
literal|0L
decl_stmt|;
comment|// Iterate through all the MapFiles
for|for
control|(
name|Iterator
argument_list|<
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
argument_list|>
name|it
init|=
name|mapFiles
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|HStoreFile
argument_list|>
name|e
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|HStoreFile
name|curHSF
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|long
name|size
init|=
name|fs
operator|.
name|getLength
argument_list|(
operator|new
name|Path
argument_list|(
name|curHSF
operator|.
name|getMapFilePath
argument_list|()
argument_list|,
name|MapFile
operator|.
name|DATA_FILE_NAME
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|mapIndex
operator|=
name|e
operator|.
name|getKey
argument_list|()
expr_stmt|;
block|}
block|}
name|MapFile
operator|.
name|Reader
name|r
init|=
name|maps
operator|.
name|get
argument_list|(
name|mapIndex
argument_list|)
decl_stmt|;
synchronized|synchronized
init|(
name|r
init|)
block|{
name|midKey
operator|.
name|set
argument_list|(
operator|(
operator|(
name|HStoreKey
operator|)
name|r
operator|.
name|midKey
argument_list|()
operator|)
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|maxSize
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/** Generate a random unique filename suffix */
name|String
name|obtainFileLabel
parameter_list|(
name|Path
name|prefix
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|testsuffix
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextInt
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|Path
name|testpath
init|=
operator|new
name|Path
argument_list|(
name|prefix
operator|.
name|toString
argument_list|()
operator|+
name|testsuffix
argument_list|)
decl_stmt|;
while|while
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|testpath
argument_list|)
condition|)
block|{
name|testsuffix
operator|=
name|String
operator|.
name|valueOf
argument_list|(
name|Math
operator|.
name|abs
argument_list|(
name|rand
operator|.
name|nextInt
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|testpath
operator|=
operator|new
name|Path
argument_list|(
name|prefix
operator|.
name|toString
argument_list|()
operator|+
name|testsuffix
argument_list|)
expr_stmt|;
block|}
return|return
name|testsuffix
return|;
block|}
comment|/**    * Return a set of MapFile.Readers, one for each HStore file.    * These should be closed after the user is done with them.    */
specifier|public
name|HScannerInterface
name|getScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|Text
name|targetCols
index|[]
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|new
name|HStoreScanner
argument_list|(
name|timestamp
argument_list|,
name|targetCols
argument_list|,
name|firstRow
argument_list|)
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// This class implements the HScannerInterface.
comment|// It lets the caller scan the contents of this HStore.
comment|//////////////////////////////////////////////////////////////////////////////
class|class
name|HStoreScanner
extends|extends
name|HAbstractScanner
block|{
name|MapFile
operator|.
name|Reader
name|readers
index|[]
decl_stmt|;
name|Text
name|lastRow
init|=
literal|null
decl_stmt|;
specifier|public
name|HStoreScanner
parameter_list|(
name|long
name|timestamp
parameter_list|,
name|Text
name|targetCols
index|[]
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|timestamp
argument_list|,
name|targetCols
argument_list|)
expr_stmt|;
name|locking
operator|.
name|obtainReadLock
argument_list|()
expr_stmt|;
try|try
block|{
name|this
operator|.
name|readers
operator|=
operator|new
name|MapFile
operator|.
name|Reader
index|[
name|mapFiles
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Iterator
argument_list|<
name|HStoreFile
argument_list|>
name|it
init|=
name|mapFiles
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|it
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|HStoreFile
name|curHSF
init|=
name|it
operator|.
name|next
argument_list|()
decl_stmt|;
name|readers
index|[
name|i
operator|++
index|]
operator|=
operator|new
name|MapFile
operator|.
name|Reader
argument_list|(
name|fs
argument_list|,
name|curHSF
operator|.
name|getMapFilePath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|keys
operator|=
operator|new
name|HStoreKey
index|[
name|readers
operator|.
name|length
index|]
expr_stmt|;
name|this
operator|.
name|vals
operator|=
operator|new
name|BytesWritable
index|[
name|readers
operator|.
name|length
index|]
expr_stmt|;
comment|// Advance the readers to the first pos.
for|for
control|(
name|i
operator|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|keys
index|[
name|i
index|]
operator|=
operator|new
name|HStoreKey
argument_list|()
expr_stmt|;
name|vals
index|[
name|i
index|]
operator|=
operator|new
name|BytesWritable
argument_list|()
expr_stmt|;
if|if
condition|(
name|firstRow
operator|.
name|getLength
argument_list|()
operator|!=
literal|0
condition|)
block|{
if|if
condition|(
name|findFirstRow
argument_list|(
name|i
argument_list|,
name|firstRow
argument_list|)
condition|)
block|{
continue|continue;
block|}
block|}
while|while
condition|(
name|getNext
argument_list|(
name|i
argument_list|)
condition|)
block|{
if|if
condition|(
name|columnMatch
argument_list|(
name|i
argument_list|)
condition|)
block|{
break|break;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**      * The user didn't want to start scanning at the first row. This method      * seeks to the requested row.      *      * @param i         - which iterator to advance      * @param firstRow  - seek to this row      * @return          - true if this is the first row or if the row was not found      */
name|boolean
name|findFirstRow
parameter_list|(
name|int
name|i
parameter_list|,
name|Text
name|firstRow
parameter_list|)
throws|throws
name|IOException
block|{
name|HStoreKey
name|firstKey
init|=
operator|(
name|HStoreKey
operator|)
name|readers
index|[
name|i
index|]
operator|.
name|getClosest
argument_list|(
operator|new
name|HStoreKey
argument_list|(
name|firstRow
argument_list|)
argument_list|,
name|vals
index|[
name|i
index|]
argument_list|)
decl_stmt|;
if|if
condition|(
name|firstKey
operator|==
literal|null
condition|)
block|{
comment|// Didn't find it. Close the scanner and return TRUE
name|closeSubScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
name|keys
index|[
name|i
index|]
operator|.
name|setRow
argument_list|(
name|firstKey
operator|.
name|getRow
argument_list|()
argument_list|)
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setColumn
argument_list|(
name|firstKey
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|.
name|setVersion
argument_list|(
name|firstKey
operator|.
name|getTimestamp
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|columnMatch
argument_list|(
name|i
argument_list|)
return|;
block|}
comment|/**      * Get the next value from the specified reader.      *       * @param i - which reader to fetch next value from      * @return - true if there is more data available      */
name|boolean
name|getNext
parameter_list|(
name|int
name|i
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|readers
index|[
name|i
index|]
operator|.
name|next
argument_list|(
name|keys
index|[
name|i
index|]
argument_list|,
name|vals
index|[
name|i
index|]
argument_list|)
condition|)
block|{
name|closeSubScanner
argument_list|(
name|i
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
return|return
literal|true
return|;
block|}
comment|/** Close down the indicated reader. */
name|void
name|closeSubScanner
parameter_list|(
name|int
name|i
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
if|if
condition|(
name|readers
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
name|readers
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|readers
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|keys
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
name|vals
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|/** Shut it down! */
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|scannerClosed
condition|)
block|{
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|readers
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|readers
index|[
name|i
index|]
operator|!=
literal|null
condition|)
block|{
name|readers
index|[
name|i
index|]
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|locking
operator|.
name|releaseReadLock
argument_list|()
expr_stmt|;
name|scannerClosed
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
end_class

end_unit

