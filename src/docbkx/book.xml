<?xml version="1.0" encoding="UTF-8"?>
<!--
/**
 * Copyright 2010 The Apache Software Foundation
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<book version="5.0" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:svg="http://www.w3.org/2000/svg"
      xmlns:m="http://www.w3.org/1998/Math/MathML"
      xmlns:html="http://www.w3.org/1999/xhtml"
      xmlns:db="http://docbook.org/ns/docbook">
  <info>
    <title>The Apache <link xlink:href="http://www.hbase.org">HBase</link>
    Book</title>
      <copyright><year>2010</year><holder>Apache Software Foundation</holder></copyright>
      <abstract>
    <para>This is the official book of
    <link xlink:href="http://www.hbase.org">Apache HBase</link>,
    a distributed, versioned, column-oriented database built on top of
    Apache Hadoop <link xlink:href="http://hadoop.apache.org/">Common and HDFS</link>.
      </para>
      </abstract>

    <revhistory>
      <revision>
        <date />

        <revdescription>Adding first cuts at Configuration, Getting Started, Data Model</revdescription>
        <revnumber>
          <?eval ${project.version}?>
        </revnumber>
      </revision>
      <revision>
        <date>
        5 October 2010
        </date>
        <authorinitials>stack</authorinitials>
        <revdescription>Initial layout</revdescription>
        <revnumber>
          0.89.20100924
        </revnumber>
      </revision>
    </revhistory>
  </info>

  <preface xml:id="preface">
    <title>Preface</title>

    <para>This book aims to be the official guide for the <link
    xlink:href="http://hbase.apache.org/">HBase</link> version it ships with.
    This document describes HBase version <emphasis><?eval ${project.version}?></emphasis>.
    Herein you will find either the definitive documentation on an HBase topic
    as of its standing when the referenced HBase version shipped, or failing
    that, this book will point to the location in <link
    xlink:href="http://hbase.apache.org/docs/current/api/index.html">javadoc</link>,
    <link xlink:href="https://issues.apache.org/jira/browse/HBASE">JIRA</link>
    or <link xlink:href="http://wiki.apache.org/hadoop/Hbase">wiki</link>
    where the pertinent information can be found.</para>

    <para>This book is a work in progress. It is lacking in many areas but we
    hope to fill in the holes with time. Feel free to add to this book should
    you feel so inclined by adding a patch to an issue up in the HBase <link
    xlink:href="https://issues.apache.org/jira/browse/HBASE">JIRA</link>.</para>
  </preface>

  <chapter xml:id="getting_started">
    <title>Getting Started</title>
    <section >
      <title>Introduction</title>
      <para>
          <link linkend="quickstart">Quick Start</link> will get you up and running
          on a single-node instance of HBase using the local filesystem.
          The <link linkend="notsoquick">Not-so-quick Start Guide</link> 
          describes setup of HBase in distributed mode running on top of HDFS.
      </para>
    </section>

    <section xml:id="quickstart">
      <title>Quick Start</title>

      <para><itemizedlist>
          <para>Here is a quick guide to starting up a standalone HBase
              instance that uses the local filesystem.  It leads you
              through creating a table, inserting rows via the
          <link linkend="shell">HBase Shell</link>, and then cleaning up and shutting
          down your instance. The below exercise should take no more than
          ten minutes (not including download time).
      </para>
          
          <listitem>
            <para>Download and unpack the latest stable release.</para>

            <para>Choose a download site from this list of <link
            xlink:href="http://www.apache.org/dyn/closer.cgi/hbase/">Apache
            Download Mirrors</link>. Click on suggested top link. This will take you to a
            mirror of <emphasis>HBase Releases</emphasis>. Click on
            the folder named <filename>stable</filename> and then download the
            file that ends in <filename>.tar.gz</filename> to your local filesystem;
            e.g. <filename>hbase-<?eval ${project.version}?>.tar.gz</filename>.</para>

            <para>Decompress and untar your download and then change into the
            unpacked directory.</para>

            <para><programlisting>$ tar xfz hbase-<?eval ${project.version}?>.tar.gz
$ cd hbase-<?eval ${project.version}?>
</programlisting></para>

<para>
   At this point, you are ready to start HBase. But before starting it,
   edit <filename>conf/hbase-site.xml</filename> and set the directory
   you want HBase to write to, <varname>hbase.rootdir</varname>.
   <programlisting>
<![CDATA[
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///DIRECTORY/hbase</value>
  </property>
</configuration>
]]>
</programlisting>
Replace <varname>DIRECTORY</varname> in the above with a path to a directory where you want
HBase to store its data.  By default, <varname>hbase.rootdir</varname> is set to <filename>/tmp/hbase-${user.name}</filename> 
which means you'll lose all your data whenever your server reboots.
</para>

            <para>Now start HBase:<programlisting>$ ./bin/start-hbase.sh
starting master, logging to logs/hbase-user-master-example.org.out</programlisting></para>

            <para>You now have a running standalone HBase instance. In standalone mode, HBase runs
            all daemons in the the one JVM; i.e. the master, regionserver, and zookeeper daemons.
            Also by default, HBase in standalone mode writes data to <filename>/tmp/hbase-${USERID}</filename>.
            HBase logs can be found in the <filename>logs</filename> subdirectory. Check them
            out especially if HBase had trouble starting.</para>

            <note>
            <title>Is <application>java</application> installed?</title>
            <para>The above presumes a 1.6 version of SUN
            <application>java</application> is installed on your
            machine and available on your path; i.e. when you type
            <application>java</application>, you see output that describes the options
            the java program takes (HBase like Hadoop requires java 6).  If this is
            not the case, HBase will not start.
            Install java, edit <filename>conf/hbase-env.sh</filename>, uncommenting the
            <envar>JAVA_HOME</envar> line pointing it  to your java install.  Then,
            retry the steps above.</para>
            </note>
            
          </listitem>

          <listitem>
            <para>Connect to your running HBase via the 
          <link linkend="shell">HBase Shell</link>.</para>

            <para><programlisting>$ ./bin/hbase shell
HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.
Type "exit&lt;RETURN&gt;" to leave the HBase Shell
Version: 0.89.20100924, r1001068, Fri Sep 24 13:55:42 PDT 2010

hbase(main):001:0&gt; </programlisting></para>

            <para>Type <command>help</command> and then <command>&lt;RETURN&gt;</command>
            to see a listing of shell
            commands and options. Browse at least the paragraphs at the end of
            the help emission for the gist of how variables are entered in the
            HBase shell; in particular note how table names, rows, and
            columns, etc., must be quoted.</para>
          </listitem>

          <listitem>
            <para>Create a table named <filename>test</filename> with a single
            column family named <filename>cf.</filename>.  Verify its creation by
            listing all tables and then insert some
            values.</para>
            <para><programlisting>hbase(main):003:0&gt; create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0&gt; list 'table'
test
1 row(s) in 0.0550 seconds
hbase(main):004:0&gt; put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0&gt; put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0&gt; put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds</programlisting></para>

            <para>Above we inserted 3 values, one at a time. The first insert is at
            <varname>row1</varname>, column <varname>cf:a</varname> -- columns
            have a column family prefix delimited by the colon character --
            with a value of <varname>value1</varname>.</para>
          </listitem>

          <listitem>
            <para>Verify the table content</para>

            <para>Run a scan of the table by doing the following</para>

            <para><programlisting>hbase(main):007:0&gt; scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds</programlisting></para>

            <para>Get a single row as follows</para>

            <para><programlisting>hbase(main):008:0&gt; get 'test', 'row1'
COLUMN      CELL
cf:a        timestamp=1288380727188, value=value1
1 row(s) in 0.0400 seconds</programlisting></para>
          </listitem>

          <listitem>
            <para>Now, disable and drop your table. This will clean up all
            done above.</para>

            <para><programlisting>hbase(main):012:0&gt; disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0&gt; drop 'test'
0 row(s) in 0.0770 seconds </programlisting></para>
          </listitem>

          <listitem>
            <para>Exit the shell by typing exit.</para>

            <para><programlisting>hbase(main):014:0&gt; exit</programlisting></para>
          </listitem>

          <listitem>
            <para>Stop your hbase instance by running the stop script.</para>

            <para><programlisting>$ ./bin/stop-hbase.sh
stopping hbase...............</programlisting></para>
          </listitem>
        </itemizedlist>
      </para>
      <section><title>Where to go next
      </title>
      <para>The above described standalone setup is good for testing and experiments only.
      Move on to the next section, the <link linkend="notsoquick">Not-so-quick Start Guide</link>
      where we'll go into depth on the different HBase run modes, requirements and critical
      configurations needed setting up a distributed HBase deploy.
      </para>
      </section>
    </section>

    <section xml:id="notsoquick">
      <title>Not-so-quick Start Guide</title>
      
      <section xml:id="requirements"><title>Requirements</title>
      <para>HBase has the following requirements.  Please read the
      section below carefully and ensure that all requirements have been
      satisfied.  Failure to do so will cause you (and us) grief debugging
      strange errors and/or data loss.
      </para>

  <section xml:id="java"><title>java</title>
<para>
  Just like Hadoop, HBase requires java 6 from <link xlink:href="http://www.java.com/download/">Oracle</link>.
Usually you'll want to use the latest version available except the problematic u18  (u22 is the latest version as of this writing).</para>
</section>

  <section xml:id="hadoop"><title><link xlink:href="http://hadoop.apache.org">hadoop</link></title>
<para>This version of HBase will only run on <link xlink:href="http://hadoop.apache.org/common/releases.html">Hadoop 0.20.x</link>.
    It will not run on hadoop 0.21.x as of this writing.
    HBase will lose data unless it is running on an HDFS that has a durable <code>sync</code>.
 Currently only the <link xlink:href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/">branch-0.20-append</link>
 branch has this attribute.  No official releases have been made from this branch as of this writing
 so you will have to build your own Hadoop from the tip of this branch
 (or install Cloudera's <link xlink:href="http://archive.cloudera.com/docs/">CDH3</link> (as of this writing, it is in beta);
 CDH has the 0.20-append patches needed to add a durable sync).
 See <link xlink:href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/CHANGES.txt">CHANGES.txt</link>
 in branch-0.20-append to see list of patches involved.</para>
  </section>
<section xml:id="ssh"> <title>ssh</title>
<para><command>ssh</command> must be installed and <command>sshd</command> must be running to use Hadoop's scripts to manage remote Hadoop daemons.
   You must be able to ssh to all nodes, including your local node, using passwordless login (Google "ssh passwordless login").
  </para>
</section>
  <section><title>DNS</title>
    <para>HBase uses the local hostname to self-report it's IP address. Both forward and reverse DNS resolving should work.</para>
    <para>If your machine has multiple interfaces, HBase will use the interface that the primary hostname resolves to.</para>
    <para>If this is insufficient, you can set <varname>hbase.regionserver.dns.interface</varname> to indicate the primary interface.
    This only works if your cluster
    configuration is consistent and every host has the same network interface configuration.</para>
    <para>Another alternative is setting <varname>hbase.regionserver.dns.nameserver</varname> to choose a different nameserver than the
    system wide default.</para>
</section>
  <section><title>NTP</title>
<para>
    The clocks on cluster members should be in basic alignments. Some skew is tolerable but
    wild skew could generate odd behaviors. Run <link xlink:href="http://en.wikipedia.org/wiki/Network_Time_Protocol">NTP</link>
    on your cluster, or an equivalent.
  </para>
    <para>If you are having problems querying data, or "weird" cluster operations, check system time!</para>
</section>


      <section xml:id="ulimit">
      <title><varname>ulimit</varname></title>
      <para>HBase is a database, it uses a lot of files at the same time.
      The default ulimit -n of 1024 on *nix systems is insufficient.
      Any significant amount of loading will lead you to 
      <link xlink:href="http://wiki.apache.org/hadoop/Hbase/FAQ#A6">FAQ: Why do I see "java.io.IOException...(Too many open files)" in my logs?</link>.
      You will also notice errors like:
      <programlisting>
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
      </programlisting>
      Do yourself a favor and change the upper bound on the number of file descriptors.
      Set it to north of 10k.  See the above referenced FAQ for how.</para>
      <para>To be clear, upping the file descriptors for the user who is
      running the HBase process is an operating system configuration, not an
      HBase configuration. Also, a common mistake is that administrators
      will up the file descriptors for a user but for whatever reason,
      HBase is running as some other users.  HBase prints in its logs
      as the first line the ulimit its seeing.  Ensure its whats expected.
      </para>
        <section xml:id="ulimit_ubuntu">
          <title><varname>ulimit</varname> on Ubuntu</title>
        <para>
          If you are on Ubuntu you will need to make the following changes:</para>
        <para>
          In the file <filename>/etc/security/limits.conf</filename> add a line like:
          <programlisting>hadoop  -       nofile  32768
          </programlisting>
          Replace 'hadoop' with whatever user is running hadoop and hbase. If you have
          separate users, you will need 2 entries, one for each user.
        </para>
        <para>
          In the file <filename>/etc/pam.d/common-session</filename> add as the last line in the file:
          <programlisting>session required  pam_limits.so
          </programlisting>
          Otherwise the changes in <filename>/etc/security/limits.conf</filename> won't be applied.
        </para>
        <para>
          Don't forget to log out and back in again for the changes to take place!
        </para>
          </section>
      </section>

      <section xml:id="dfs.datanode.max.xcievers">
      <title><varname>dfs.datanode.max.xcievers</varname></title>
      <para>
      Hadoop HDFS datanodes have an upper bound on the number of files that it will serve at one same time.
      The upper bound parameter is called <varname>xcievers</varname> (yes, this is misspelled). Again, before
      doing any loading, make sure you have configured Hadoop's <filename>conf/hdfs-site.xml</filename>
      setting the <varname>xceivers</varname> value to at least the following:
      <programlisting>
      &lt;property&gt;
        &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
        &lt;value&gt;2047&lt;/value&gt;
      &lt;/property&gt;
      </programlisting>
      </para>
      <para>Be sure to restart your HDFS after making the above configuration change so its picked
      up by datanodes.</para>
      </section>

<section xml:id="windows">
<title>Windows</title>
<para>
If you are running HBase on Windows, you must install
<link xlink:href="http://cygwin.com/">Cygwin</link>
to have a *nix-like environment for the shell scripts. The full details
are explained in the <link xlink:href="cygwin.html">Windows Installation</link>
guide.
</para>
</section>

      </section>

      <section><title>HBase run modes: Standalone and Distributed</title>
          <para>HBase has two run modes: <link linkend="standalone">standalone</link>
              and <link linkend="distributed">distributed</link>.</para>

<para>Whatever your mode, define <code>${HBASE_HOME}</code> to be the location of the root of your HBase installation, e.g.
<code>/user/local/hbase</code>. Edit <code>${HBASE_HOME}/conf/hbase-env.sh</code>. In this file you can
set the heapsize for HBase, etc. At a minimum, set <code>JAVA_HOME</code> to point at the root of
your Java installation.</para>

      <section xml:id="standalone"><title>Standalone HBase</title>
        <para>This is the default mode straight out of the box. Standalone mode is
        what is described in the <link linkend="quickstart">quickstart</link>
        section.  In standalone mode, HBase does not use HDFS -- it uses the local
        filesystem instead -- and it runs all HBase daemons and a local zookeeper
        all up in the same JVM.  Zookeeper binds to a well known port so clients may
        talk to HBase.
      </para>
      </section>
      <section><title>Distributed</title>
          <para>Distributed mode can be subdivided into distributed but all daemons run on a
          single node -- i.e. <emphasis>pseudo-distributed</emphasis> mode -- AND
          <emphasis>cluster distibuted</emphasis> with daemons spread across all
          nodes in the cluster.</para>
      <para>
          Distributed modes require an instance of the
          <emphasis>Hadoop Distributed File System</emphasis> (HDFS).  See the
          Hadoop <link xlink:href="http://hadoop.apache.org/common/docs/current/api/overview-summary.html#overview_description">
          requirements and instructions</link> for how to set up a HDFS.
      </para>

      <section xml:id="pseudo"><title>Pseudo-distributed</title>
<para>A pseudo-distributed mode is simply a distributed mode run on a single host.
Use this configuration testing and prototyping on HBase.  Do not use this configuration
for production nor for evaluating HBase performance.
</para>
<para>Once you have confirmed your HDFS setup, configuring HBase for use on one host requires modification of
<filename>./conf/hbase-site.xml</filename>, which needs to be pointed at the running Hadoop HDFS instance.
Use <filename>hbase-site.xml</filename> to override the properties defined in
<filename>conf/hbase-default.xml</filename> (<filename>hbase-default.xml</filename> itself
should never be modified) and for HDFS client configurations.
At a minimum, the <varname>hbase.rootdir</varname>,
which points HBase at the Hadoop filesystem to use,
should be redefined in <filename>hbase-site.xml</filename>. For example,
adding the properties below to your <filename>hbase-site.xml</filename> says that HBase
should use the <filename>/hbase</filename> 
directory in the HDFS whose namenode is at port 9000 on your local machine, and that
it should run with one replica only (recommended for pseudo-distributed mode):</para>
<programlisting>
&lt;configuration&gt;
  ...
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;
    &lt;description&gt;The directory shared by region servers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
    &lt;description&gt;The replication count for HLog &amp; HFile storage. Should not be greater than HDFS datanode count.
    &lt;/description&gt;
  &lt;/property&gt;
  ...
&lt;/configuration&gt;
</programlisting>

<note>
<para>Let HBase create the <varname>hbase.rootdir</varname>
directory. If you don't, you'll get warning saying HBase
needs a migration run because the directory is missing files
expected by HBase (it'll
create them if you let it).</para>
</note>

<note>
<para>Above we bind to <varname>localhost</varname>.
This means that a remote client cannot
connect.  Amend accordingly, if you want to
connect from a remote location.</para>
</note>
<section>
<title>Starting extra masters and regionservers when running pseudo-distributed</title>
<para>See <link xlink:href="pseudo-distributed.html">Pseudo-distributed mode extras</link>.</para>
</section>
</section>

      <section><title>Cluster Distributed</title>


<para>For running a fully-distributed operation on more than one host, the following
configurations must be made <emphasis>in addition</emphasis> to those described in the
<link linkend="pseudo">pseudo-distributed</link> section above.</para>

<para>In <filename>hbase-site.xml</filename>, set <varname>hbase.cluster.distributed</varname> to <varname>true</varname>.</para>
<programlisting>
&lt;configuration&gt;
  ...
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    &lt;/description&gt;
  &lt;/property&gt;
  ...
&lt;/configuration&gt;
</programlisting>

<para>In fully-distributed mode, you probably want to change your <varname>hbase.rootdir</varname>
from localhost to the name of the node running the HDFS NameNode and you should set
the dfs.replication to be the number of datanodes you have in your cluster or 3, which
ever is the smaller.
</para>
<para>In addition
to <filename>hbase-site.xml</filename> changes, a fully-distributed mode requires that you
modify <filename>${HBASE_HOME}/conf/regionservers</filename>.
The <filename>regionserver</filename> file lists all hosts running <application>HRegionServer</application>s, one host per line
(This file in HBase is like the Hadoop slaves file at <filename>${HADOOP_HOME}/conf/slaves</filename>).</para>

<para>A distributed HBase depends on a running ZooKeeper cluster. All participating nodes and clients
need to be able to get to the running ZooKeeper cluster.
HBase by default manages a ZooKeeper cluster for you, or you can manage it on your own and point HBase to it.
To toggle HBase management of ZooKeeper, use the <varname>HBASE_MANAGES_ZK</varname> variable in <filename>${HBASE_HOME}/conf/hbase-env.sh</filename>.
This variable, which defaults to <varname>true</varname>, tells HBase whether to
start/stop the ZooKeeper quorum servers alongside the rest of the servers.</para>

<para>When HBase manages the ZooKeeper cluster, you can specify ZooKeeper configuration
using its canonical <filename>zoo.cfg</filename> file (see below), or 
just specify ZookKeeper options directly in the <filename>${HBASE_HOME}/conf/hbase-site.xml</filename>
(If new to ZooKeeper, go the path of specifying your configuration in HBase's hbase-site.xml).
Every ZooKeeper configuration option has a corresponding property in the HBase hbase-site.xml
XML configuration file named <varname>hbase.zookeeper.property.OPTION</varname>.
For example, the <varname>clientPort</varname> setting in ZooKeeper can be changed by
setting the <varname>hbase.zookeeper.property.clientPort</varname> property.
For the full list of available properties, see ZooKeeper's <filename>zoo.cfg</filename>.
For the default values used by HBase, see <filename>${HBASE_HOME}/conf/hbase-default.xml</filename>.</para>

<para>At minimum, you should set the list of servers that you want ZooKeeper to run
on using the <varname>hbase.zookeeper.quorum</varname> property.
This property defaults to <varname>localhost</varname> which is not suitable for a
fully distributed HBase (it binds to the local machine only and remote clients
will not be able to connect).
It is recommended to run a ZooKeeper quorum of 3, 5 or 7 machines, and give each
ZooKeeper server around 1GB of RAM, and if possible, its own dedicated disk.
For very heavily loaded clusters, run ZooKeeper servers on separate machines from the
Region Servers (DataNodes and TaskTrackers).</para>


<para>To point HBase at an existing ZooKeeper cluster, add 
a suitably configured <filename>zoo.cfg</filename> to the <filename>CLASSPATH</filename>.
HBase will see this file and use it to figure out where ZooKeeper is.
Additionally set <varname>HBASE_MANAGES_ZK</varname> in <filename>${HBASE_HOME}/conf/hbase-env.sh</filename>
to <filename>false</filename> so that HBase doesn't mess with your ZooKeeper setup:</para>
<programlisting>
   ...
  # Tell HBase whether it should manage it's own instance of Zookeeper or not.
  export HBASE_MANAGES_ZK=false
</programlisting>

<para>As an example, to have HBase manage a ZooKeeper quorum on nodes
<emphasis>rs{1,2,3,4,5}.example.com</emphasis>, bound to port 2222 (the default is 2181), use:</para>
<programlisting>
  ${HBASE_HOME}/conf/hbase-env.sh:

       ...
      # Tell HBase whether it should manage it's own instance of Zookeeper or not.
      export HBASE_MANAGES_ZK=true

  ${HBASE_HOME}/conf/hbase-site.xml:

  &lt;configuration&gt;
    ...
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
      &lt;value&gt;2222&lt;/value&gt;
      &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
      The port at which the clients will connect.
      &lt;/description&gt;
    &lt;/property&gt;
    ...
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
      &lt;value&gt;rs1.example.com,rs2.example.com,rs3.example.com,rs4.example.com,rs5.example.com&lt;/value&gt;
      &lt;description&gt;Comma separated list of servers in the ZooKeeper Quorum.
      For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
      By default this is set to localhost for local and pseudo-distributed modes
      of operation. For a fully-distributed setup, this should be set to a full
      list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh
      this is the list of servers which we will start/stop ZooKeeper on.
      &lt;/description&gt;
    &lt;/property&gt;
    ...
  &lt;/configuration&gt;
</programlisting>

<para>When HBase manages ZooKeeper, it will start/stop the ZooKeeper servers as a part
of the regular start/stop scripts. If you would like to run it yourself, you can
do:</para>
<programlisting>
${HBASE_HOME}/bin/hbase-daemons.sh {start,stop} zookeeper
</programlisting>

<para>If you do let HBase manage ZooKeeper for you, make sure you configure
where it's data is stored. By default, it will be stored in <filename>/tmp</filename> which is
sometimes cleaned in live systems. Do modify this configuration:</para>
<programlisting>
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
      &lt;value&gt;${hbase.tmp.dir}/zookeeper&lt;/value&gt;
      &lt;description>Property from ZooKeeper's config zoo.cfg.
      The directory where the snapshot is stored.
      &lt;/description&gt;
    &lt;/property&gt;
</programlisting>

<para>Note that you can use HBase in this manner to spin up a ZooKeeper cluster,
unrelated to HBase. Just make sure to set <varname>HBASE_MANAGES_ZK</varname> to
<varname>false</varname> if you want it to stay up so that when HBase shuts down it
doesn't take ZooKeeper with it.</para>

<para>For more information about setting up a ZooKeeper cluster on your own, see
the ZooKeeper <link xlink:href="http://hadoop.apache.org/zookeeper/docs/current/zookeeperStarted.html">Getting Started Guide</link>.
HBase currently uses ZooKeeper version 3.3.2, so any cluster setup with a
3.x.x version of ZooKeeper should work.</para>

<para>Of note, if you have made <emphasis>HDFS client configuration</emphasis> on your Hadoop cluster, HBase will not
see this configuration unless you do one of the following:</para>
<orderedlist>
  <listitem><para>Add a pointer to your <varname>HADOOP_CONF_DIR</varname> to <varname>CLASSPATH</varname> in <filename>hbase-env.sh</filename>.</para></listitem>
  <listitem><para>Add a copy of <filename>hdfs-site.xml</filename> (or <filename>hadoop-site.xml</filename>) to <filename>${HBASE_HOME}/conf</filename>, or</para></listitem>
  <listitem><para>if only a small set of HDFS client configurations, add them to <filename>hbase-site.xml</filename>.</para></listitem>
</orderedlist>

<para>An example of such an HDFS client configuration is <varname>dfs.replication</varname>. If for example,
you want to run with a replication factor of 5, hbase will create files with the default of 3 unless
you do the above to make the configuration available to HBase.</para>
      </section>

<section xml:id="confirm"><title>Running and Confirming Your Installation</title>
<para>If you are running in standalone, non-distributed mode, HBase by default uses the local filesystem.</para>

<para>If you are running a distributed cluster you will need to start the Hadoop DFS daemons and
ZooKeeper Quorum before starting HBase and stop the daemons after HBase has shut down.</para>

<para>Start and stop the Hadoop DFS daemons by running <filename>${HADOOP_HOME}/bin/start-dfs.sh</filename>.
You can ensure it started properly by testing the put and get of files into the Hadoop filesystem.
HBase does not normally use the mapreduce daemons.  These do not need to be started.</para>

<para>Start up your ZooKeeper cluster.</para>

<para>Start HBase with the following command:</para>
<programlisting>
${HBASE_HOME}/bin/start-hbase.sh
</programlisting>

<para>Once HBase has started, enter <filename>${HBASE_HOME}/bin/hbase shell</filename> to obtain a
shell against HBase from which you can execute commands.
Type 'help' at the shells' prompt to get a list of commands.
Test your running install by creating tables, inserting content, viewing content, and then dropping your tables.
For example:</para>
<programlisting>
hbase&gt; # Type "help" to see shell help screen
hbase&gt; help
hbase&gt; # To create a table named "mylittletable" with a column family of "mylittlecolumnfamily", type
hbase&gt; create "mylittletable", "mylittlecolumnfamily"
hbase&gt; # To see the schema for you just created "mylittletable" table and its single "mylittlecolumnfamily", type
hbase&gt; describe "mylittletable"
hbase&gt; # To add a row whose id is "myrow", to the column "mylittlecolumnfamily:x" with a value of 'v', do
hbase&gt; put "mylittletable", "myrow", "mylittlecolumnfamily:x", "v"
hbase&gt; # To get the cell just added, do
hbase&gt; get "mylittletable", "myrow"
hbase&gt; # To scan you new table, do
hbase&gt; scan "mylittletable"
</programlisting>

<para>To stop HBase, exit the HBase shell and enter:</para>
<programlisting>
${HBASE_HOME}/bin/stop-hbase.sh
</programlisting>

<para>If you are running a distributed operation, be sure to wait until HBase has shut down completely
before stopping the Hadoop daemons.</para>

<para>The default location for logs is <filename>${HBASE_HOME}/logs</filename>.</para>

<para>HBase also puts up a UI listing vital attributes. By default its deployed on the master host
at port 60010 (HBase RegionServers listen on port 60020 by default and put up an informational
http server at 60030).</para>
</section>








</section>
</section>



      <section><title>Client configuration and dependencies connecting to an HBase cluster</title>

      <para>
        Since the HBase master may move around, clients bootstrap from Zookeeper.  Thus clients
        require the Zookeeper quorum information in a <filename>hbase-site.xml</filename> that
        is on their classpath.  If you are configuring an IDE to run a HBase client, you should
        include the <filename>conf/</filename> directory on your classpath.
      </para>
        <para>
          An example basic <filename>hbase-site.xml</filename> for client only:
          <programlisting><![CDATA[
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>example1,example2,example3</value>
    <description>The directory shared by region servers.
    </description>
  </property>
</configuration>
]]>
          </programlisting>
        </para>
    </section>


      <section xml:id="upgrading">
          <title>Upgrading your HBase Install</title>
          <para>This version of 0.90.x HBase can be started on data written by
              HBase 0.20.x or HBase 0.89.x.  There is no need of a migration step.
              HBase 0.89.x and 0.90.x does write out the name of region directories
              differently -- it names them with a md5 hash of the region name rather
              than a jenkins hash -- so this means that once started, there is no
              going back to HBase 0.20.x.
          </para>
      </section>




    <section><title>Example Configurations</title>
    <para>In this section we provide a few sample configurations.</para>
    <section><title>Basic Distributed HBase Install</title>
    <para>Here is example basic configuration of a ten node cluster running in
    distributed mode.  The nodes
are named <varname>example0</varname>, <varname>example1</varname>, etc., through
node <varname>example9</varname>  in this example.  The HBase Master and the HDFS namenode 
are running on the node <varname>example0</varname>.  RegionServers run on nodes
<varname>example1</varname>-<varname>example9</varname>.
A 3-node zookeeper ensemble runs on <varname>example1</varname>, <varname>example2</varname>, and <varname>example3</varname>.
Below we show what the main configuration files
-- <filename>hbase-site.xml</filename>, <filename>regionservers</filename>, and
<filename>hbase-env.sh</filename> -- found in the <filename>conf</filename> directory
might look like.
</para>
    <section xml:id="hbase_site"><title><filename>hbase-site.xml</filename></title>
    <programlisting>
<![CDATA[
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>example1,example2,example3</value>
    <description>The directory shared by region servers.
    </description>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/export/stack/zookeeper</value>
    <description>Property from ZooKeeper's config zoo.cfg.
    The directory where the snapshot is stored.
    </description>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://example1:9000/hbase</value>
    <description>The directory shared by region servers.
    </description>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
    <description>The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    </description>
  </property>
</configuration>
]]>
    </programlisting>
    </section>

    <section xml:id="regionservers"><title><filename>regionservers</filename></title>
    <para>In this file you list the nodes that will run regionservers.  In
    our case we run regionservers on all but the head node example1 which is
    carrying the HBase master and the HDFS namenode</para>
    <programlisting>
    example1
    example3
    example4
    example5
    example6
    example7
    example8
    example9
    </programlisting>
    </section>

    <section xml:id="hbase_env"><title><filename>hbase-env.sh</filename></title>
    <para>Below we use a <command>diff</command> to show the differences from 
    default in the <filename>hbase-env.sh</filename> file. Here we are setting
the HBase heap to be 4G instead of the default 1G.
    </para>
    <programlisting>
    <![CDATA[
$ git diff hbase-env.sh
diff --git a/conf/hbase-env.sh b/conf/hbase-env.sh
index e70ebc6..96f8c27 100644
--- a/conf/hbase-env.sh
+++ b/conf/hbase-env.sh
@@ -31,7 +31,7 @@ export JAVA_HOME=/usr/lib//jvm/java-6-sun/
 # export HBASE_CLASSPATH=
 
 # The maximum amount of heap to use, in MB. Default is 1000.
-# export HBASE_HEAPSIZE=1000
+export HBASE_HEAPSIZE=4096
 
 # Extra Java runtime options.
 # Below are what we set by default.  May only work with SUN JVM.
]]>
    </programlisting>
    </section>

    </section>
    
    </section>
    </section>
  </chapter>

  <chapter xml:id="configuration">
    <title>Configuration</title>
    <para>
        HBase uses the same configuration system as Hadoop.
        To configure a deploy, edit a file of environment variables
        in <filename>conf/hbase-env.sh</filename> -- this configuration
        is used mostly by the launcher shell scripts getting the cluster
        off the ground -- and then add configuration to an xml file to
        do things like override HBase defaults, tell HBase what Filesystem to
        use, and the location of the ZooKeeper ensemble.
    </para>

    <section>
    <title><filename>hbase-site.xml</filename> and <filename>hbase-default.xml</filename></title>
    <para>What are these?
    </para>
    <para>
    Not all configuration options make it out to
    <filename>hbase-default.xml</filename>.  Configuration
    that it thought rare anyone would change can exist only
    in code; the only way to turn up the configurations is
    via a reading of the source code.
    </para>
    <!--The file hbase-default.xml is generated as part of
    the build of the hbase site.  See the hbase pom.xml.
    The generated file is a docbook section with a glossary
    in it-->
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
      href="../../target/site/hbase-default.xml" />
    </section>

      <section>
      <title><filename>hbase-env.sh</filename></title>
      <para></para>
      </section>

      <section xml:id="log4j">
      <title><filename>log4j.properties</filename></title>
      <para></para>
      </section>

      <section xml:id="important_configurations">
      <title>The Important Configurations</title>
      <para>Below we list the important Configurations.  We've divided this section into
      required configuration and worth-a-look recommended configs.
      </para>


      <section xml:id="required_configuration"><title>Required Configurations</title>
      <para>See the <link linkend="requirements">Requirements</link> section.
      It lists at least two required configurations needed running HBase bearing
      load: i.e. <link linkend="ulimit">file descriptors <varname>ulimit</varname></link> and
      <link linkend="dfs.datanode.max.xcievers"><varname>dfs.datanode.max.xcievers</varname></link>.
      </para>
      </section>

      <section xml:id="recommended_configurations"><title>Recommended Configuations</title>
      <section xml:id="big_memory">
        <title>Configuration for large memory machines</title>
        <para>
          HBase ships with a reasonable configuration that will work on nearly all
          machine types that people might want to test with. If you have larger
          machines you might the following configuration options helpful.
        </para>

      </section>
      <section xml:id="lzo">
      <title>LZO compression</title>
      <para>You should consider enabling LZO compression.  Its
      near-frictionless and in most all cases boosts performance.
      </para>
      <para>Unfortunately, HBase cannot ship with LZO because of
      the licensing issues; HBase is Apache-licensed, LZO is GPL.
      Therefore LZO install is to be done post-HBase install.
      See the <link xlink:href="http://wiki.apache.org/hadoop/UsingLzoCompression">Using LZO Compression</link>
      wiki page for how to make LZO work with HBase.
      </para>
      <para>A common problem users run into when using LZO is that while initial
      setup of the cluster runs smooth, a month goes by and some sysadmin goes to
      add a machine to the cluster only they'll have forgotten to do the LZO
      fixup on the new machine.  In versions since HBase 0.90.0, we should
      fail in a way that makes it plain what the problem is, but maybe not.
      Remember you read this paragraph<footnote><para>See
      <link linkend="hbase.regionserver.codec">hbase.regionserver.codec</link>
      for a feature to help protect against failed LZO install</para></footnote>.
      </para>
      </section>
      </section>

      </section>
  </chapter>

  <chapter xml:id="shell">
    <title>The HBase Shell</title>

    <para>
        The HBase Shell is <link xlink:href="http://jruby.org">(J)Ruby</link>'s
        IRB with some HBase particular verbs addded.  Anything you can do in
        IRB, you should be able to do in the HBase Shell.</para>
        <para>To run the HBase shell, 
        do as follows:
        <programlisting>$ ./bin/hbase shell</programlisting>
        Type <command>help</command> followed by <command>&lt;RETURN&gt;</command>
        to see a complete listing of commands available.
        Take some time to study the tail of the help screen where it
        does a synopsis of IRB syntax specifying arguments -- usually you must
        quote -- and how to write out dictionaries, etc.
    </para>

    <section><title>Scripting</title>
        <para>For examples scripting HBase, look in the
            HBase <filename>bin</filename> directory.  Look at the files
            that end in <filename>*.rb</filename>.  To run one of these
            files, do as follows:
            <programlisting>$ ./bin/hbase org.jruby.Main PATH_TO_SCRIPT</programlisting>
        </para>
    </section>

    <section xml:id="shell_tricks"><title>Shell Tricks</title>
        <section><title><filename>irbrc</filename></title>
                <para>Create an <filename>.irbrc</filename> file for yourself in your
                    home directory. Add HBase Shell customizations. A useful one is
                    command history:
                    <programlisting>
                        $ more .irbrc
                        require 'irb/ext/save-history'
                        IRB.conf[:SAVE_HISTORY] = 100
                        IRB.conf[:HISTORY_FILE] = "#{ENV['HOME']}/.irb-save-history"
                    </programlisting>
                </para>
        </section>
        <section><title>Log data to timestamp</title>
            <para>
                To convert the date '08/08/16 20:56:29' from an hbase log into a timestamp, do:
                <programlisting>
                    hbase(main):021:0> import java.text.SimpleDateFormat
                    hbase(main):022:0> import java.text.ParsePosition
                    hbase(main):023:0> SimpleDateFormat.new("yy/MM/dd HH:mm:ss").parse("08/08/16 20:56:29", ParsePosition.new(0)).getTime() => 1218920189000
                </programlisting>
            </para>
            <para>
                To go the other direction:
                <programlisting>
                    hbase(main):021:0> import java.util.Date
                    hbase(main):022:0> Date.new(1218920189000).toString() => "Sat Aug 16 20:56:29 UTC 2008"
                </programlisting>
            </para>
            <para>
                To output in a format that is exactly like hbase log format is a pain messing with
                <link xlink:href="http://download.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html">SimpleDateFormat</link>.
            </para>
        </section>
        <section><title>Debug</title>
            <section><title>Shell debug switch</title>
                <para>You can set a debug switch in the shell to see more output
                    -- e.g. more of the stack trace on exception --
                    when you run a command:
                    <programlisting>hbase> debug &lt;RETURN&gt;</programlisting>
                 </para>
            </section>
            <section><title>DEBUG log level</title>
                <para>To enable DEBUG level logging in the shell,
                    launch it with the <command>-d</command> option.
                    <programlisting>$ ./bin/hbase shell -d</programlisting>
               </para>
            </section>
         </section>
    </section>
  </chapter>

  <chapter xml:id="mapreduce">
  <title>HBase and MapReduce</title>
  <para>See <link xlink:href="apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#package_description">HBase and MapReduce</link>
  up in javadocs.</para>
  </chapter>

  <chapter xml:id="hbase_metrics">
  <title>Metrics</title>
  <para>See <link xlink:href="metrics.html">Metrics</link>.
  </para>
  </chapter>

  <chapter xml:id="cluster_replication">
  <title>Cluster Replication</title>
  <para>See <link xlink:href="replication.html">Cluster Replication</link>.
  </para>
  </chapter>

  <chapter xml:id="datamodel">
    <title>Data Model</title>
  <para>The HBase data model resembles that a traditional RDBMS.
  Applications store data into HBase <emphasis>tables</emphasis>.
      Tables are made of rows and columns. Table cells
      -- the intersection of row and column
      coordinates -- are versioned. By default, their
      <emphasis>version</emphasis> is a timestamp
      auto-assigned by HBase at the time of cell insertion. A cell’s content
      is an uninterpreted array of bytes.
  </para>
      <para>Table row keys are also byte arrays so almost anything can
      serve as a row key from strings to binary representations of longs or
      even serialized data structures. Rows in HBase tables
      are sorted by row key. The sort is byte-ordered. All table accesses are
      via the table row key -- its primary key.
</para>

    <section xml:id="table">
      <title>Table</title>

      <para></para>
    </section>

    <section xml:id="row">
      <title>Row</title>

      <para></para>
    </section>

    <section xml:id="columnfamily">
      <title>Column Family<indexterm><primary>Column Family</primary></indexterm></title>
        <para>
      Columns in HBase are grouped into <emphasis>column families</emphasis>.
      All column members of a column family have a common prefix.  For example, the
      columns <emphasis>courses:history</emphasis> and
      <emphasis>courses:math</emphasis> are both members of the
      <emphasis>courses</emphasis> column family.
          The colon character (<literal
          moreinfo="none">:</literal>) delimits the column family from the
          column family <emphasis>qualifier</emphasis>.
        The column family prefix must be composed of
      <emphasis>printable</emphasis> characters. The qualifying tail, the
      <indexterm>column family <emphasis>qualifier</emphasis><primary>Column Family Qualifier</primary></indexterm>, can be made of any
      arbitrary bytes. Column families must be declared up front
      at schema definition time whereas columns do not need to be
      defined at schema time but can be conjured on the fly while
      the table is up an running.</para>
      <para>Physically, all column family members are stored together on the
      filesystem.  Because tunings and
      storage specifications are done at the column family level, it is
      advised that all column family members have the same general access
      pattern and size characteristics.</para>

      <para></para>
    </section>

    <section xml:id="versions">
      <title>Versions<indexterm><primary>Versions</primary></indexterm></title>

      <para>A <emphasis>{row, column, version} </emphasis>tuple exactly
      specifies a <literal>cell</literal> in HBase. Its possible to have an
      unbounded number of cells where the row and column are the same but the
      cell address differs only in its version dimension.</para>

      <para>While rows and column keys are expressed as bytes, the version is
      specified using a long integer. Typically this long contains time
      instances such as those returned by
      <code>java.util.Date.getTime()</code> or
      <code>System.currentTimeMillis()</code>, that is: <quote>the difference,
      measured in milliseconds, between the current time and midnight, January
      1, 1970 UTC</quote>.</para>

      <para>The HBase version dimension is stored in decreasing order, so that
      when reading from a store file, the most recent values are found
      first.</para>

      <para>There is a lot of confusion over the semantics of
      <literal>cell</literal> versions, in HBase. In particular, a couple
      questions that often come up are:<itemizedlist>
          <listitem>
            <para>If multiple writes to a cell have the same version, are all
            versions maintained or just the last?<footnote>
                <para>Currently, only the last written is fetchable.</para>
              </footnote></para>
          </listitem>

          <listitem>
            <para>Is it OK to write cells in a non-increasing version
            order?<footnote>
                <para>Yes</para>
              </footnote></para>
          </listitem>
        </itemizedlist></para>

      <para>Below we describe how the version dimension in HBase currently
      works<footnote>
          <para>See <link
          xlink:href="https://issues.apache.org/jira/browse/HBASE-2406">HBASE-2406</link>
          for discussion of HBase versions. <link
          xlink:href="http://outerthought.org/blog/417-ot.html">Bending time
          in HBase</link> makes for a good read on the version, or time,
          dimension in HBase. It has more detail on versioning than is
          provided here. As of this writing, the limiitation
          <emphasis>Overwriting values at existing timestamps</emphasis>
          mentioned in the article no longer holds in HBase. This section is
          basically a synopsis of this article by Bruno Dumon.</para>
        </footnote>.</para>

      <section>
        <title>Versions and HBase Operations</title>

        <para>In this section we look at the behavior of the version dimension
        for each of the core HBase operations.</para>

        <section>
          <title>Get/Scan</title>

          <para>Gets are implemented on top of Scans. The below discussion of
          Get applies equally to Scans.</para>

          <para>By default, i.e. if you specify no explicit version, when
          doing a <literal>get</literal>, the cell whose version has the
          largest value is returned (which may or may not be the latest one
          written, see later). The default behavior can be modified in the
          following ways:</para>

          <itemizedlist>
            <listitem>
              <para>to return more than one version, see <link
              xlink:href="http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/client/Get.html#setMaxVersions()">Get.setMaxVersions()</link></para>
            </listitem>

            <listitem>
              <para>to return versions other than the latest, see <link
              xlink:href="???">Get.setTimeRange()</link></para>

              <para>To retrieve the latest version that is less than or equal
              to a given value, thus giving the 'latest' state of the record
              at a certain point in time, just use a range from 0 to the
              desired version and set the max versions to 1.</para>
            </listitem>
          </itemizedlist>
        </section>

        <section>
          <title>Put</title>

          <para>Doing a put always creates a new version of a
          <literal>cell</literal>, at a certain timestamp. By default the
          system uses the server's <literal>currentTimeMillis</literal>, but
          you can specify the version (= the long integer) yourself, on a
          per-column level. This means you could assign a time in the past or
          the future, or use the long value for non-time purposes.</para>

          <para>To overwrite an existing value, do a put at exactly the same
          row, column, and version as that of the cell you would
          overshadow.</para>
        </section>

        <section>
          <title>Delete</title>

          <para>When performing a delete operation in HBase, there are two
          ways to specify the versions to be deleted</para>

          <itemizedlist>
            <listitem>
              <para>Delete all versions older than a certain timestamp</para>
            </listitem>

            <listitem>
              <para>Delete the version at a specific timestamp</para>
            </listitem>
          </itemizedlist>

          <para>A delete can apply to a complete row, a complete column
          family, or to just one column. It is only in the last case that you
          can delete explicit versions. For the deletion of a row or all the
          columns within a family, it always works by deleting all cells older
          than a certain version.</para>

          <para>Deletes work by creating <emphasis>tombstone</emphasis>
          markers. For example, let's suppose we want to delete a row. For
          this you can specify a version, or else by default the
          <literal>currentTimeMillis</literal> is used. What this means is
          <quote>delete all cells where the version is less than or equal to
          this version</quote>. HBase never modifies data in place, so for
          example a delete will not immediately delete (or mark as deleted)
          the entries in the storage file that correspond to the delete
          condition. Rather, a so-called <emphasis>tombstone</emphasis> is
          written, which will mask the deleted values<footnote>
              <para>When HBase does a major compaction, the tombstones are
              processed to actually remove the dead values, together with the
              tombstones themselves.</para>
            </footnote>. If the version you specified when deleting a row is
          larger than the version of any value in the row, then you can
          consider the complete row to be deleted.</para>
        </section>
      </section>

      <section>
        <title>Current Limitations</title>

        <para>There are still some bugs (or at least 'undecided behavior')
        with the version dimension that will be addressed by later HBase
        releases.</para>

        <section>
          <title>Deletes mask Puts</title>

          <para>Deletes mask puts, even puts that happened after the delete
          was entered<footnote>
              <para><link
              xlink:href="https://issues.apache.org/jira/browse/HBASE-2256">HBASE-2256</link></para>
            </footnote>. Remember that a delete writes a tombstone, which only
          disappears after then next major compaction has run. Suppose you do
          a delete of everything &lt;= T. After this you do a new put with a
          timestamp &lt;= T. This put, even if it happened after the delete,
          will be masked by the delete tombstone. Performing the put will not
          fail, but when you do a get you will notice the put did have no
          effect. It will start working again after the major compaction has
          run. These issues should not be a problem if you use
          always-increasing versions for new puts to a row. But they can occur
          even if you do not care about time: just do delete and put
          immediately after each other, and there is some chance they happen
          within the same millisecond.</para>
        </section>

        <section>
          <title>Major compactions change query results</title>

          <para><quote>...create three cell versions at t1, t2 and t3, with a
          maximum-versions setting of 2. So when getting all versions, only
          the values at t2 and t3 will be returned. But if you delete the
          version at t2 or t3, the one at t1 will appear again. Obviously,
          once a major compaction has run, such behavior will not be the case
          anymore...<footnote>
              <para>See <emphasis>Garbage Collection</emphasis> in <link
              xlink:href="http://outerthought.org/blog/417-ot.html">Bending
              time in HBase</link> </para>
            </footnote></quote></para>
        </section>
      </section>
    </section>
  </chapter>


  <chapter xml:id="filesystem">
    <title>Filesystem Format</title>

    <subtitle>How HBase is persisted on the Filesystem</subtitle>

    <section xml:id="hfile">
      <title>HFile</title>

      <section xml:id="hfile_tool">
        <title>HFile Tool</title>

        <para>To view a textualized version of hfile content, you can do use
        the <classname>org.apache.hadoop.hbase.io.hfile.HFile
        </classname>tool. Type the following to see usage:<programlisting><code>$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile </code> </programlisting>For
        example, to view the content of the file
        <filename>hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475</filename>,
        type the following:<programlisting> <code>$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -f hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475 </code> </programlisting>If
        you leave off the option -v to see just a summary on the hfile. See
        usage for other things to do with the <classname>HFile</classname>
        tool.</para>
      </section>
    </section>
  </chapter>

  <chapter xml:id="architecture">
    <title>Architecture</title>
    <section>
    <title>Regions</title>

    <para>This chapter is all about Regions.</para>

    <note>
      <para>Does this belong in the data model chapter?</para>
    </note>

    <section>
      <title>Region Size</title>

      <para>Region size is one of those tricky things, there are a few factors
      to consider:</para>

      <itemizedlist>
        <listitem>
          <para>Regions are the basic element of availability and
          distribution.</para>
        </listitem>

        <listitem>
          <para>HBase scales by having regions across many servers. Thus if
          you have 2 regions for 16GB data, on a 20 node machine you are a net
          loss there.</para>
        </listitem>

        <listitem>
          <para>High region count has been known to make things slow, this is
          getting better, but it is probably better to have 700 regions than
          3000 for the same amount of data.</para>
        </listitem>

        <listitem>
          <para>Low region count prevents parallel scalability as per point
          #2. This really cant be stressed enough, since a common problem is
          loading 200MB data into HBase then wondering why your awesome 10
          node cluster is mostly idle.</para>
        </listitem>

        <listitem>
          <para>There is not much memory footprint difference between 1 region
          and 10 in terms of indexes, etc, held by the regionserver.</para>
        </listitem>
      </itemizedlist>

      <para>Its probably best to stick to the default, perhaps going smaller
      for hot tables (or manually split hot regions to spread the load over
      the cluster), or go with a 1GB region size if your cell sizes tend to be
      largish (100k and up).</para>
    </section>

    <section>
      <title>Region Transitions</title>

      <note>
        <para>TODO: Review all of the below to ensure it matches what was
        committed -- St.Ack 20100901</para>
      </note>

      <para>Regions only transition in a limited set of circumstances.</para>

      <section>
        <title>Cluster Startup</title>

        <para>During cluster startup, the Master will know that it is a
        cluster startup and do a bulk assignment.</para>

        <note>
          <para>This should take HDFS block locations into account.</para>
        </note>

        <itemizedlist>
          <listitem>
            <para>Master startup determines whether this is startup or
            failover by counting the number of RegionServer nodes in
            ZooKeeper.</para>
          </listitem>

          <listitem>
            <para>Master waits for the minimum number of RegionServers to be
            available to be assigned regions.</para>
          </listitem>

          <listitem>
            <para>Master clears out anything in the
            <filename>/unassigned</filename> directory in ZooKeeper.</para>
          </listitem>

          <listitem>
            <para>Master randomly assigns out <constant>-ROOT-</constant> and
            then <constant>.META.</constant>.</para>
          </listitem>

          <listitem>
            <para>Master determines a bulk assignment plan via the
            <classname>LoadBalancer</classname></para>
          </listitem>

          <listitem>
            <para>Master stores the plan in the
            <classname>AssignmentManager</classname>.</para>
          </listitem>

          <listitem>
            <para>Master creates <code>OFFLINE</code> ZooKeeper nodes in
            <filename>/unassigned</filename> for every region.</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to each RegionServer, telling them to
            <code>OPEN</code> their regions.</para>
          </listitem>
        </itemizedlist>

        <para>All special cluster startup logic ends here.</para>

        <note>
          <para>So what can go wrong?</para>

          <itemizedlist>
            <listitem>
              <para>We assume that the Master will not fail until after the
              <code>OFFLINE</code> nodes have been created in ZK.
              RegionServers can fail at any time.</para>
            </listitem>

            <listitem>
              <para>If an RS fails at some point during this process, normal
              region open/opening/opened handling will take care of it.</para>

              <para>If the RS successfully opened a region, then it will be
              taken care of in the normal RS failure handling.</para>

              <para>If the RS did not successfully open a region, the
              RegionManager or MasterPlanner will notice that the OFFLINE (or
              OPENING) node in ZK has not been updated. This will trigger a
              re-assignment to a different server. This logic is not special
              to startup, all assignments will eventually time out if the
              destination server never proceeds.</para>
            </listitem>

            <listitem>
              <para>If the Master fails (after creating the ZK nodes), the
              failed-over Master will see all of the regions in transition. It
              will handle it in the same way any failed-over Master will
              handle existing regions in transition.</para>
            </listitem>
          </itemizedlist>
        </note>
      </section>

      <section>
        <title>Load Balancing</title>

        <para>Periodically, and when there are not any regions in transition,
        a load balancer will run and move regions around to balance cluster
        load.</para>

        <itemizedlist>
          <listitem>
            <para>Periodic timer expires initializing a load balance (Load
            Balancer is an instance of <classname>Chore</classname>).</para>
          </listitem>

          <listitem>
            <para>Currently if regions in transition, load balancer goes back
            to sleep.</para>

            <note>
              <para>Should it block until there are no regions in
              transition.</para>
            </note>
          </listitem>

          <listitem>
            <para>The <classname>AssignmentManager</classname> determines a
            balancing plan via the LoadBalancer.</para>
          </listitem>

          <listitem>
            <para>Master stores the plan in the
            <classname>AssignmentMaster</classname> store of
            <classname>RegionPlan</classname>s</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to the source RSs, telling them to
            <code>CLOSE</code> the regions.</para>
          </listitem>
        </itemizedlist>

        <para>That is it for the initial part of the load balance. Further
        steps will be executed following event-triggers from ZK or timeouts if
        closes run too long. It's not clear what to do in the case of a
        long-running CLOSE besides ask again.</para>

        <itemizedlist>
          <listitem>
            <para>RS receives CLOSE RPC, changes to CLOSING, and begins
            closing the region.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now CLOSING but does
            nothing.</para>
          </listitem>

          <listitem>
            <para>RS closes region and changes ZK node to CLOSED.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now CLOSED.</para>
          </listitem>

          <listitem>
            <para>Master looks at the plan for the specified region to figure
            out the desired destination server.</para>
          </listitem>

          <listitem>
            <para>Master sends an RPC to the destination RS telling it to OPEN
            the region.</para>
          </listitem>

          <listitem>
            <para>RS receives OPEN RPC, changes to OPENING, and begins opening
            the region.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now OPENING but does
            nothing.</para>
          </listitem>

          <listitem>
            <para>RS opens region and changes ZK node to OPENED. Edits .META.
            updating the regions location.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now OPENED.</para>
          </listitem>

          <listitem>
            <para>Master removes the region from all in-memory
            structures.</para>
          </listitem>

          <listitem>
            <para>Master deletes the OPENED node from ZK.</para>
          </listitem>
        </itemizedlist>

        <para>The Master or RSs can fail during this process. There is nothing
        special about handling regions in transition due to load balancing so
        consult the descriptions below for how this is handled.</para>
      </section>

      <section>
        <title>Table Enable/Disable</title>

        <para>Users can enable and disable tables manually. This is done to
        make config changes to tables, drop tables, etc...</para>

        <note>
          <para>Because all failover logic is designed to ensure assignment of
          all regions in transition, these operations will not properly ride
          over Master or RegionServer failures. Since these are
          client-triggered operations, this should be okay for the initial
          master design. Moving forward, a special node could be put in ZK to
          denote that a enable/disable has been requested. Another option is
          to persist region movement plans into ZK instead of just in-memory.
          In that case, an empty destination would signal that the region
          should not be reopened after being closed.</para>
        </note>

        <section>
          <title>Disable</title>

          <itemizedlist>
            <listitem>
              <para>Client sends Master an RPC to disable a table.</para>
            </listitem>

            <listitem>
              <para>Master finds all regions of the table.</para>
            </listitem>

            <listitem>
              <para>Master stores the plan (do not re-open the regions once
              closed).</para>
            </listitem>

            <listitem>
              <para>Master sends RPCs to RSs to close all the regions of the
              table.</para>
            </listitem>

            <listitem>
              <para>RS receives CLOSE RPC, creates ZK node in CLOSING state,
              and begins closing the region.</para>
            </listitem>

            <listitem>
              <para>Master sees that region is now CLOSING but does
              nothing.</para>
            </listitem>

            <listitem>
              <para>RS closes region and changes ZK node to CLOSED.</para>
            </listitem>

            <listitem>
              <para>Master sees that region is now CLOSED.</para>
            </listitem>

            <listitem>
              <para>Master looks at the plan for the specified region and sees
              that it should not reopen.</para>
            </listitem>

            <listitem>
              <para>Master deletes the unassigned znode. It is no longer
              responsible for ensuring assignment/availability of this
              region.</para>
            </listitem>
          </itemizedlist>

          <section>
            <title>Enable</title>

            <itemizedlist>
              <listitem>
                <para>Client sends Master an RPC to disable a table.</para>
              </listitem>

              <listitem>
                <para>Master finds all regions of the table.</para>
              </listitem>

              <listitem>
                <para>Master creates an unassigned node in an OFFLINE state
                for each region.</para>
              </listitem>

              <listitem>
                <para>Master sends RPCs to RSs to open all the regions of the
                table.</para>
              </listitem>

              <listitem>
                <para>RS receives OPEN RPC, transitions ZK node to OPENING
                state, and begins opening the region.</para>
              </listitem>

              <listitem>
                <para>Master sees that region is now OPENING but does
                nothing.</para>
              </listitem>

              <listitem>
                <para>RS opens region and changes ZK node to OPENED.</para>
              </listitem>

              <listitem>
                <para>Master sees that region is now OPENED.</para>
              </listitem>

              <listitem>
                <para>Master deletes the unassigned znode.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>
      </section>

      <section>
        <title>RegionServer Failure</title>

        <itemizedlist>
          <listitem>
            <para>Master is alerted via ZK that an RS ephemeral node is
            gone.</para>
          </listitem>

          <listitem>
            <para>Master begins RS failure process.</para>
          </listitem>

          <listitem>
            <para>Master determines which regions need to be handled.</para>
          </listitem>

          <listitem>
            <para>Master in-memory state shows all regions currently assigned
            to the dead RS.</para>
          </listitem>

          <listitem>
            <para>Master in-memory plans show any regions that were in
            transitioning to the dead RS.</para>
          </listitem>

          <listitem>
            <para>With list of regions, Master now forces assignment of all
            regions to other RSs.</para>
          </listitem>

          <listitem>
            <para>Master creates or force updates all existing ZK unassigned
            nodes to be OFFLINE.</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to RSs to open all the regions.</para>
          </listitem>

          <listitem>
            <para>Normal operations from here on.</para>
          </listitem>
        </itemizedlist>

        <para>There are some complexities here. For regions in transition that
        were somehow involved with the dead RS, these could be in any of the 5
        states in ZK.</para>

        <itemizedlist>
          <listitem>
            <para><code>OFFLINE</code> Generate a new assignment and send an
            OPEN RPC.</para>
          </listitem>

          <listitem>
            <para><code>CLOSING</code> If the failed RS is the source, we
            overwrite the state to OFFLINE, generate a new assignment, and
            send an OPEN RPC. If the failed RS is the destination, we
            overwrite the state to OFFLINE and send an OPEN RPC to the
            original destination. If for some reason we don't have an existing
            plan (concurrent Master failure), generate a new assignment and
            send an OPEN RPC.</para>
          </listitem>

          <listitem>
            <para><code>CLOSED</code> If the failed RS is the source, we can
            safely ignore this. The normal ZK event handling should deal with
            this. If the failed RS is the destination, we generate a new
            assignment and send an OPEN RPC.</para>
          </listitem>

          <listitem>
            <para>OPENING or OPENED If the failed RS was the original source,
            ignore. If the failed RS is the destination, we overwrite the
            state to OFFLINE, generate a new assignment, and send an OPEN
            RPC.</para>
          </listitem>
        </itemizedlist>

        <para>In all of these cases, it is important to note that the
        transitions on the RS side ensure only a single RS ever successfully
        completes a transition. This is done by reading the current state,
        verifying it is expected, and then issuing the update with the version
        number of the read value. If multiple RSs are attempting this
        operation, exactly one can succeed.</para>
      </section>

      <section>
        <title>Master Failover</title>

        <itemizedlist>
          <listitem>
            <para>Master initializes and finds out that he is a failed-over
            Master.</para>
          </listitem>

          <listitem>
            <para>Before Master starts up the normal handlers for region
            transitions he grabs all nodes in /unassigned.</para>
          </listitem>

          <listitem>
            <para>If no regions are in transition, failover is done and he
            continues.</para>
          </listitem>

          <listitem>
            <para>If regions are in transition, each will be handled according
            to the current region state in ZK.</para>
          </listitem>

          <listitem>
            <para>Before processing the regions in transition, the normal
            handlers start to ensure we don't miss any transitions. The
            handling of opens on the RS side ensures we don't dupe assign even
            if things have changed before we finish acting on
            them.<itemizedlist>
                <listitem>
                  <para>OFFLINE Generate a new assignment and send an OPEN
                  RPC.</para>
                </listitem>

                <listitem>
                  <para>CLOSING Nothing to be done. Normal handlers take care
                  of timeouts.</para>
                </listitem>

                <listitem>
                  <para>CLOSED Generate a new assignment and send an OPEN
                  RPC.</para>
                </listitem>

                <listitem>
                  <para>OPENING Nothing to be done. Normal handlers take care
                  of timeouts.</para>
                </listitem>

                <listitem>
                  <para>OPENED Delete the node from ZK. Region was
                  successfully opened but the previous Master did not
                  acknowledge it.</para>
                </listitem>
              </itemizedlist></para>
          </listitem>

          <listitem>
            <para>Once this is done, everything further is dealt with as
            normal by the RegionManager.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Summary of Region Transition States</title>

        <note>
          <para>Check below is complete -- St.Ack 20100901</para>
        </note>

        <section>
          <title>Master</title>

          <itemizedlist>
            <listitem>
              <para>Master creates an unassigned node as OFFLINE.</para>

              <para>Cluster startup and table enabling.</para>
            </listitem>

            <listitem>
              <para>Master forces an existing unassigned node to
              OFFLINE.</para>

              <para>RegionServer failure.</para>

              <para>Allows transitions from all states to OFFLINE.</para>
            </listitem>

            <listitem>
              <para>Master deletes an unassigned node that was in a OPENED
              state.</para>

              <para>Normal region transitions. Besides cluster startup, no
              other deletions of unassigned nodes is allowed.</para>
            </listitem>

            <listitem>
              <para>Master deletes all unassigned nodes regardless of
              state.</para>

              <para>Cluster startup before any assignment happens.</para>
            </listitem>
          </itemizedlist>
        </section>

        <section>
          <title>RegionServer</title>

          <itemizedlist>
            <listitem>
              <para>RegionServer creates an unassigned node as CLOSING.</para>

              <para>All region closes will do this in response to a CLOSE RPC
              from Master.</para>

              <para>A node can never be transitioned to CLOSING, only
              created.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from CLOSING
              to CLOSED.</para>

              <para>Normal region closes. CAS operation.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from OFFLINE
              to OPENING.</para>

              <para>All region opens will do this in response to an OPEN RPC
              from the Master.</para>

              <para>Normal region opens. CAS operation.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from OPENING
              to OPENED.</para>

              <para>Normal region opens. CAS operation.</para>
            </listitem>
          </itemizedlist>
        </section>
      </section>

      <section>
        <title>Region Splits</title>

        <para>Splits run unaided on the RegionServer; i.e. the Master does not
        participate. The RegionServer splits a region, offlines the split
        region and then adds the daughter regions to META, opens daughters on
        the parent's hosting RegionServer and then reports the split to the
        master.</para>
      </section>
    </section>
    </section>
  </chapter>

  <chapter xml:id="wal">
    <title >The WAL</title>

    <subtitle>HBase's<link
    xlink:href="http://en.wikipedia.org/wiki/Write-ahead_logging"> Write-Ahead
    Log</link></subtitle>

    <para>Each RegionServer adds updates to its Write-ahead Log (WAL)
    first, and then to memory.</para>

    <section>
      <title>What is the purpose of the HBase WAL</title>

      <para>The HBase WAL is...</para>
    </section>

    <section xml:id="wal_splitting">
      <title>WAL splitting</title>

      <subtitle>How edits are recovered from a crashed RegionServer</subtitle>

      <para>When a RegionServer crashes, it will lose its ephemeral lease in
      ZooKeeper...TODO</para>

      <section>
        <title><varname>hbase.hlog.split.skip.errors</varname></title>

        <para>When set to <constant>true</constant>, the default, any error
        encountered splitting will be logged, the problematic WAL will be
        moved into the <filename>.corrupt</filename> directory under the hbase
        <varname>rootdir</varname>, and processing will continue. If set to
        <constant>false</constant>, the exception will be propagated and the
        split logged as failed.<footnote>
            <para>See <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-2958">HBASE-2958
            When hbase.hlog.split.skip.errors is set to false, we fail the
            split but thats it</link>. We need to do more than just fail split
            if this flag is set.</para>
          </footnote></para>
      </section>

      <section>
        <title>How EOFExceptions are treated when splitting a crashed
        RegionServers' WALs</title>

        <para>If we get an EOF while splitting logs, we proceed with the split
        even when <varname>hbase.hlog.split.skip.errors</varname> ==
        <constant>false</constant>. An EOF while reading the last log in the
        set of files to split is near-guaranteed since the RegionServer likely
        crashed mid-write of a record. But we'll continue even if we got an
        EOF reading other than the last file in the set.<footnote>
            <para>For background, see <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-2643">HBASE-2643
            Figure how to deal with eof splitting logs</link></para>
          </footnote></para>
      </section>
    </section>

  </chapter>

  <chapter xml:id="blooms">
    <title>Bloom Filters</title>

    <para>Bloom filters were developed over in <link
    xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200
    Add bloomfilters</link>.<footnote>
        <para>For description of the development process -- why static blooms
        rather than dynamic -- and for an overview of the unique properties
        that pertain to blooms in HBase, as well as possible future
        directions, see the <emphasis>Development Process</emphasis> section
        of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> attached to <link
        xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200</link>.</para>
      </footnote><footnote>
        <para>The bloom filters described here are actually version two of
        blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom
        option based on work done by the <link
        xlink:href="http://www.one-lab.org">European Commission One-Lab
        Project 034819</link>. The core of the HBase bloom work was later
        pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile.
        Version 1 of HBase blooms never worked that well. Version 2 is a
        rewrite from scratch though again it starts with the one-lab
        work.</para>
      </footnote></para>

    <section>
      <title>Configurations</title>

      <para>Blooms are enabled by specifying options on a column family in the
      HBase shell or in java code as specification on
      <classname>org.apache.hadoop.hbase.HColumnDescriptor</classname>.</para>

      <section>
        <title><code>HColumnDescriptor</code> option</title>

        <para>Use <code>HColumnDescriptor.setBloomFilterType(NONE | ROW |
        ROWCOL)</code> to enable blooms per Column Family. Default =
        <varname>NONE</varname> for no bloom filters. If
        <varname>ROW</varname>, the hash of the row will be added to the bloom
        on each insert. If <varname>ROWCOL</varname>, the hash of the row +
        column family + column family qualifier will be added to the bloom on
        each key insert.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.enabled</varname> global kill
        switch</title>

        <para><code>io.hfile.bloom.enabled</code> in
        <classname>Configuration</classname> serves as the kill switch in case
        something goes wrong. Default = <varname>true</varname>.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.error.rate</varname></title>

        <para><varname>io.hfile.bloom.error.rate</varname> = average false
        positive rate. Default = 1%. Decrease rate by ½ (e.g. to .5%) == +1
        bit per bloom entry.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.max.fold</varname></title>

        <para><varname>io.hfile.bloom.max.fold</varname> = guaranteed minimum
        fold rate. Most people should leave this alone. Default = 7, or can
        collapse to at least 1/128th of original size. See the
        <emphasis>Development Process</emphasis> section of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> for more on what this option means.</para>
      </section>
    </section>

    <section xml:id="bloom_footprint">
      <title>Bloom StoreFile footprint</title>

      <para>Bloom filters add an entry to the <classname>StoreFile</classname>
      general <classname>FileInfo</classname> data structure and then two
      extra entries to the <classname>StoreFile</classname> metadata
      section.</para>

      <section>
        <title>BloomFilter in the <classname>StoreFile</classname>
        <classname>FileInfo</classname> data structure</title>

        <section>
          <title><varname>BLOOM_FILTER_TYPE</varname></title>

          <para><classname>FileInfo</classname> has a
          <varname>BLOOM_FILTER_TYPE</varname> entry which is set to
          <varname>NONE</varname>, <varname>ROW</varname> or
          <varname>ROWCOL.</varname></para>
        </section>
      </section>

      <section>
        <title>BloomFilter entries in <classname>StoreFile</classname>
        metadata</title>

        <section>
          <title><varname>BLOOM_FILTER_META</varname></title>

          <para><varname>BLOOM_FILTER_META</varname> holds Bloom Size, Hash
          Function used, etc. Its small in size and is cached on
          <classname>StoreFile.Reader</classname> load</para>
        </section>

        <section>
          <title><varname>BLOOM_FILTER_DATA</varname></title>

          <para><varname>BLOOM_FILTER_DATA</varname> is the actual bloomfilter
          data. Obtained on-demand. Stored in the LRU cache, if it is enabled
          (Its enabled by default).</para>
        </section>
      </section>
    </section>
  </chapter>

  <appendix xml:id="tools">
    <title >Tools</title>

    <para>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</para>
    <section xml:id="hbck">
        <title>HBase <application>hbck</application></title>
        <subtitle>An <emphasis>fsck</emphasis> for your HBase install</subtitle>
        <para>To run <application>hbck</application> against your HBase cluster run
        <programlisting>$ ./bin/hbase hbck</programlisting>
        At the end of the commands output it prints <emphasis>OK</emphasis>
        or <emphasis>INCONSISTENCY</emphasis>. If your cluster reports
        inconsistencies, pass <command>-details</command> to see more detail emitted.
        If inconsistencies, run <command>hbck</command> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <command>-fix</command> may correct the inconsistency (This latter
        is an experimental feature).
        </para>
    </section>
    <section><title>HFile Tool</title>
        <para>See <link linkend="hfile_tool" >HFile Tool</link>.</para>
    </section>
    <section xml:id="wal_tools">
      <title>WAL Tools</title>

      <section xml:id="hlog_tool">
        <title><classname>HLog</classname> tool</title>

        <para>The main method on <classname>HLog</classname> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <filename>recovered.edits</filename>. directory.</para>

        <para>You can get a textual dump of a WAL file content by doing the
        following:<programlisting> <code>$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --dump hdfs://example.org:9000/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </programlisting>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <varname>STDOUT</varname> to
        <code>/dev/null</code> and testing the program return.</para>

        <para>Similarily you can force a split of a log file directory by
        doing:<programlisting> $ ./<code>bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --split hdfs://example.org:9000/hbase/.logs/example.org,60020,1283516293161/</code></programlisting></para>
      </section>
    </section>
  </appendix>
  <appendix xml:id="compression">
    <title >Compression</title>

    <para>TODO: Compression in hbase...</para>
    <section>
    <title>
    LZO
    </title>
    <para>
    Running with LZO enabled is recommended though HBase does not ship with
    LZO because of licensing issues.  To install LZO and verify its installation
    and that its available to HBase, do the following...
    </para>
    </section>

    <section id="hbase.regionserver.codec">
    <title>
    <varname>
    hbase.regionserver.codec
    </varname>
    </title>
    <para>
    To have a RegionServer test a set of codecs and fail-to-start if any
    code is missing or misinstalled, add the configuration
    <varname>
    hbase.regionserver.codec
    </varname>
    to your <filename>hbase-site.xml</filename> with a value of
    codecs to test on startup.  For example if the 
    <varname>
    hbase.regionserver.codec
    </varname> value is <code>lzo,gz</code> and if lzo is not present
    or improperly installed, the misconfigured RegionServer will fail
    to start.
    </para>
    <para>
    Administrators might make use of this facility to guard against
    the case where a new server is added to cluster but the cluster
    requires install of a particular coded.
    </para>

    </section>
  </appendix>

  <appendix xml:id="faq">
    <title >FAQ</title>
    <qandaset defaultlabel='faq'>
        <qandadiv><title>General</title>
        <qandaentry>
                <question><para>Are there other HBase FAQs?</para></question>
            <answer>
                <para>
              See the FAQ that is up on the wiki, <link xlink:href="http://wiki.apache.org/hadoop/Hbase/FAQ">HBase Wiki FAQ</link>
              as well as the <link xlink:href="http://wiki.apache.org/hadoop/Hbase/Troubleshooting">Troubleshooting</link> page and
              the <link xlink:href="http://wiki.apache.org/hadoop/Hbase/FrequentlySeenErrors">Frequently Seen Errors</link> page.
                </para>
            </answer>
        </qandaentry>
    </qandadiv>
    <qandadiv xml:id="ec2"><title>EC2</title>
        <qandaentry>
            <question><para>
            Why doesn't my remote java connection into my ec2 cluster work?
            </para></question>
            <answer>
                <para>
          See Andrew's answer here, up on the user list: <link xlink:href="http://search-hadoop.com/m/sPdqNFAwyg2">Remote Java client connection into EC2 instance</link>.
                </para>
            </answer>
        </qandaentry>
    </qandadiv>
        <qandadiv><title>Building HBase</title>
        <qandaentry>
            <question><para>
When I build, why do I always get <code>Unable to find resource 'VM_global_library.vm'</code>?
            </para></question>
            <answer>
                <para>
                    Ignore it.  Its not an error.  It is <link xlink:href="http://jira.codehaus.org/browse/MSITE-286">officially ugly</link> though.
                </para>
            </answer>
        </qandaentry>
    </qandadiv>
    </qandaset>
  </appendix>


  <index xml:id="book_index">
  <title>Index</title>
  </index>
</book>
