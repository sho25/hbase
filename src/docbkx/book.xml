<?xml version="1.0" encoding="UTF-8"?>
<book version="5.0" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:svg="http://www.w3.org/2000/svg"
      xmlns:m="http://www.w3.org/1998/Math/MathML"
      xmlns:html="http://www.w3.org/1999/xhtml"
      xmlns:db="http://docbook.org/ns/docbook">
  <info>
    <title>The <link xlink:href="http://www.hbase.org">HBase</link>
    Book</title>

    <revhistory>
      <revision>
        <date />

        <revdescription>Initial layout</revdescription>

        <revnumber>
          <?eval ${project.version}?>
        </revnumber>
      </revision>
    </revhistory>
  </info>

  <chapter xml:id="getting_started">
    <title>Getting Started</title>

    <section>
      <title>Requirements</title>

      <para>First...</para>
    </section>
  </chapter>

  <chapter>
    <title>The HBase Shell</title>

    <para></para>
  </chapter>

  <chapter xml:id="filesystem">
    <title>Filesystem Format</title>

    <subtitle>How HBase is persisted on the Filesystem</subtitle>

    <section>
      <title>HFile</title>

      <section>
        <title>HFile Tool</title>

        <para>To view a textualized version of hfile content, you can do use
        the <classname>org.apache.hadoop.hbase.io.hfile.HFile
        </classname>tool. Type the following to see usage:<programlisting><code>$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile </code> </programlisting>For
        example, to view the content of the file
        <filename>hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475</filename>,
        type the following:<programlisting> <code>$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -f hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475 </code> </programlisting>If
        you leave off the option -v to see just a summary on the hfile. See
        usage for other things to do with the <classname>HFile</classname>
        tool.</para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>Regions</title>

    <para>This chapter is all about Regions.</para>


    <section>
      <title>Region Size</title>

      <para>Region size is one of those tricky things, there are a few factors
      to consider:</para>

      <itemizedlist>
        <listitem>
          <para>Regions are the basic element of availability and
          distribution.</para>
        </listitem>

        <listitem>
          <para>HBase scales by having regions across many servers. Thus if
          you have 2 regions for 16GB data, on a 20 node machine you are a net
          loss there.</para>
        </listitem>

        <listitem>
          <para>High region count has been known to make things slow, this is
          getting better, but it is probably better to have 700 regions than
          3000 for the same amount of data.</para>
        </listitem>

        <listitem>
          <para>Low region count prevents parallel scalability as per point
          #2. This really cant be stressed enough, since a common problem is
          loading 200MB data into HBase then wondering why your awesome 10
          node cluster is mostly idle.</para>
        </listitem>

        <listitem>
          <para>There is not much memory footprint difference between 1 region
          and 10 in terms of indexes, etc, held by the regionserver.</para>
        </listitem>
      </itemizedlist>

      <para>Its probably best to stick to the default, perhaps going smaller
      for hot tables (or manually split hot regions to spread the load over
      the cluster), or go with a 1GB region size if your cell sizes tend to be
      largish (100k and up).</para>
    </section>

    <section>
      <title>Region Transitions</title>
    <note>
      <para>TODO: Review all of the below to ensure it matches what was
      committed -- St.Ack 20100901</para>
    </note>

      <para>Regions only transition in a limited set of circumstances.</para>

      <section>
        <title>Cluster Startup</title>

        <para>During cluster startup, the Master will know that it is a
        cluster startup and do a bulk assignment.</para>

        <note>
          <para>This should take HDFS block locations into account.</para>
        </note>

        <itemizedlist>
          <listitem>
            <para>Master startup determines whether this is startup or
            failover by counting the number of RegionServer nodes in
            ZooKeeper.</para>
          </listitem>

          <listitem>
            <para>Master waits for the minimum number of RegionServers to be
            available to be assigned regions.</para>
          </listitem>

          <listitem>
            <para>Master clears out anything in the
            <filename>/unassigned</filename> directory in ZooKeeper.</para>
          </listitem>

          <listitem>
            <para>Master randomly assigns out <constant>-ROOT-</constant> and
            then <constant>.META.</constant>.</para>
          </listitem>

          <listitem>
            <para>Master determines a bulk assignment plan via the
            <classname>LoadBalancer</classname></para>
          </listitem>

          <listitem>
            <para>Master stores the plan in the
            <classname>AssignmentManager</classname>.</para>
          </listitem>

          <listitem>
            <para>Master creates <code>OFFLINE</code> ZooKeeper nodes in
            <filename>/unassigned</filename> for every region.</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to each RegionServer, telling them to
            <code>OPEN</code> their regions.</para>
          </listitem>
        </itemizedlist>

        <para>All special cluster startup logic ends here.</para>

        <note>
          <para>So what can go wrong?</para>

          <itemizedlist>
            <listitem>
              <para>We assume that the Master will not fail until after the
              <code>OFFLINE</code> nodes have been created in ZK.
              RegionServers can fail at any time.</para>
            </listitem>

            <listitem>
              <para>If an RS fails at some point during this process, normal
              region open/opening/opened handling will take care of it.</para>

              <para>If the RS successfully opened a region, then it will be
              taken care of in the normal RS failure handling.</para>

              <para>If the RS did not successfully open a region, the
              RegionManager or MasterPlanner will notice that the OFFLINE (or
              OPENING) node in ZK has not been updated. This will trigger a
              re-assignment to a different server. This logic is not special
              to startup, all assignments will eventually time out if the
              destination server never proceeds.</para>
            </listitem>

            <listitem>
              <para>If the Master fails (after creating the ZK nodes), the
              failed-over Master will see all of the regions in transition. It
              will handle it in the same way any failed-over Master will
              handle existing regions in transition.</para>
            </listitem>
          </itemizedlist>
        </note>
      </section>

      <section>
        <title>Load Balancing</title>

        <para>Periodically, and when there are not any regions in transition,
        a load balancer will run and move regions around to balance cluster
        load.</para>

        <itemizedlist>
          <listitem>
            <para>Periodic timer expires initializing a load balance (Load
            Balancer is an instance of <classname>Chore</classname>).</para>
          </listitem>

          <listitem>
            <para>Currently if regions in transition, load balancer goes back
            to sleep.</para>

            <note>
              <para>Should it block until there are no regions in
              transition.</para>
            </note>
          </listitem>

          <listitem>
            <para>The <classname>AssignmentManager</classname> determines a
            balancing plan via the LoadBalancer.</para>
          </listitem>

          <listitem>
            <para>Master stores the plan in the
            <classname>AssignmentMaster</classname> store of
            <classname>RegionPlan</classname>s</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to the source RSs, telling them to
            <code>CLOSE</code> the regions.</para>
          </listitem>
        </itemizedlist>

        <para>That is it for the initial part of the load balance. Further
        steps will be executed following event-triggers from ZK or timeouts if
        closes run too long. It's not clear what to do in the case of a
        long-running CLOSE besides ask again.</para>

        <itemizedlist>
          <listitem>
            <para>RS receives CLOSE RPC, changes to CLOSING, and begins
            closing the region.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now CLOSING but does
            nothing.</para>
          </listitem>

          <listitem>
            <para>RS closes region and changes ZK node to CLOSED.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now CLOSED.</para>
          </listitem>

          <listitem>
            <para>Master looks at the plan for the specified region to figure
            out the desired destination server.</para>
          </listitem>

          <listitem>
            <para>Master sends an RPC to the destination RS telling it to OPEN
            the region.</para>
          </listitem>

          <listitem>
            <para>RS receives OPEN RPC, changes to OPENING, and begins opening
            the region.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now OPENING but does
            nothing.</para>
          </listitem>

          <listitem>
            <para>RS opens region and changes ZK node to OPENED. Edits .META.
            updating the regions location.</para>
          </listitem>

          <listitem>
            <para>Master sees that region is now OPENED.</para>
          </listitem>

          <listitem>
            <para>Master removes the region from all in-memory
            structures.</para>
          </listitem>

          <listitem>
            <para>Master deletes the OPENED node from ZK.</para>
          </listitem>
        </itemizedlist>

        <para>The Master or RSs can fail during this process. There is nothing
        special about handling regions in transition due to load balancing so
        consult the descriptions below for how this is handled.</para>
      </section>

      <section>
        <title>Table Enable/Disable</title>

        <para>Users can enable and disable tables manually. This is done to
        make config changes to tables, drop tables, etc...</para>

        <note>
          <para>Because all failover logic is designed to ensure assignment of
          all regions in transition, these operations will not properly ride
          over Master or RegionServer failures. Since these are
          client-triggered operations, this should be okay for the initial
          master design. Moving forward, a special node could be put in ZK to
          denote that a enable/disable has been requested. Another option is
          to persist region movement plans into ZK instead of just in-memory.
          In that case, an empty destination would signal that the region
          should not be reopened after being closed.</para>
        </note>

        <section>
          <title>Disable</title>

          <itemizedlist>
            <listitem>
              <para>Client sends Master an RPC to disable a table.</para>
            </listitem>

            <listitem>
              <para>Master finds all regions of the table.</para>
            </listitem>

            <listitem>
              <para>Master stores the plan (do not re-open the regions once
              closed).</para>
            </listitem>

            <listitem>
              <para>Master sends RPCs to RSs to close all the regions of the
              table.</para>
            </listitem>

            <listitem>
              <para>RS receives CLOSE RPC, creates ZK node in CLOSING state,
              and begins closing the region.</para>
            </listitem>

            <listitem>
              <para>Master sees that region is now CLOSING but does
              nothing.</para>
            </listitem>

            <listitem>
              <para>RS closes region and changes ZK node to CLOSED.</para>
            </listitem>

            <listitem>
              <para>Master sees that region is now CLOSED.</para>
            </listitem>

            <listitem>
              <para>Master looks at the plan for the specified region and sees
              that it should not reopen.</para>
            </listitem>

            <listitem>
              <para>Master deletes the unassigned znode. It is no longer
              responsible for ensuring assignment/availability of this
              region.</para>
            </listitem>
          </itemizedlist>

          <section>
            <title>Enable</title>

            <itemizedlist>
              <listitem>
                <para>Client sends Master an RPC to disable a table.</para>
              </listitem>

              <listitem>
                <para>Master finds all regions of the table.</para>
              </listitem>

              <listitem>
                <para>Master creates an unassigned node in an OFFLINE state
                for each region.</para>
              </listitem>

              <listitem>
                <para>Master sends RPCs to RSs to open all the regions of the
                table.</para>
              </listitem>

              <listitem>
                <para>RS receives OPEN RPC, transitions ZK node to OPENING
                state, and begins opening the region.</para>
              </listitem>

              <listitem>
                <para>Master sees that region is now OPENING but does
                nothing.</para>
              </listitem>

              <listitem>
                <para>RS opens region and changes ZK node to OPENED.</para>
              </listitem>

              <listitem>
                <para>Master sees that region is now OPENED.</para>
              </listitem>

              <listitem>
                <para>Master deletes the unassigned znode.</para>
              </listitem>
            </itemizedlist>
          </section>
        </section>
      </section>

      <section>
        <title>RegionServer Failure</title>

        <itemizedlist>
          <listitem>
            <para>Master is alerted via ZK that an RS ephemeral node is
            gone.</para>
          </listitem>

          <listitem>
            <para>Master begins RS failure process.</para>
          </listitem>

          <listitem>
            <para>Master determines which regions need to be handled.</para>
          </listitem>

          <listitem>
            <para>Master in-memory state shows all regions currently assigned
            to the dead RS.</para>
          </listitem>

          <listitem>
            <para>Master in-memory plans show any regions that were in
            transitioning to the dead RS.</para>
          </listitem>

          <listitem>
            <para>With list of regions, Master now forces assignment of all
            regions to other RSs.</para>
          </listitem>

          <listitem>
            <para>Master creates or force updates all existing ZK unassigned
            nodes to be OFFLINE.</para>
          </listitem>

          <listitem>
            <para>Master sends RPCs to RSs to open all the regions.</para>
          </listitem>

          <listitem>
            <para>Normal operations from here on.</para>
          </listitem>
        </itemizedlist>

        <para>There are some complexities here. For regions in transition that
        were somehow involved with the dead RS, these could be in any of the 5
        states in ZK.</para>

        <itemizedlist>
          <listitem>
            <para><code>OFFLINE</code> Generate a new assignment and send an
            OPEN RPC.</para>
          </listitem>

          <listitem>
            <para><code>CLOSING</code> If the failed RS is the source, we
            overwrite the state to OFFLINE, generate a new assignment, and
            send an OPEN RPC. If the failed RS is the destination, we
            overwrite the state to OFFLINE and send an OPEN RPC to the
            original destination. If for some reason we don't have an existing
            plan (concurrent Master failure), generate a new assignment and
            send an OPEN RPC.</para>
          </listitem>

          <listitem>
            <para><code>CLOSED</code> If the failed RS is the source, we can
            safely ignore this. The normal ZK event handling should deal with
            this. If the failed RS is the destination, we generate a new
            assignment and send an OPEN RPC.</para>
          </listitem>

          <listitem>
            <para>OPENING or OPENED If the failed RS was the original source,
            ignore. If the failed RS is the destination, we overwrite the
            state to OFFLINE, generate a new assignment, and send an OPEN
            RPC.</para>
          </listitem>
        </itemizedlist>

        <para>In all of these cases, it is important to note that the
        transitions on the RS side ensure only a single RS ever successfully
        completes a transition. This is done by reading the current state,
        verifying it is expected, and then issuing the update with the version
        number of the read value. If multiple RSs are attempting this
        operation, exactly one can succeed.</para>
      </section>

      <section>
        <title>Master Failover</title>

        <itemizedlist>
          <listitem>
            <para>Master initializes and finds out that he is a failed-over
            Master.</para>
          </listitem>

          <listitem>
            <para>Before Master starts up the normal handlers for region
            transitions he grabs all nodes in /unassigned.</para>
          </listitem>

          <listitem>
            <para>If no regions are in transition, failover is done and he
            continues.</para>
          </listitem>

          <listitem>
            <para>If regions are in transition, each will be handled according
            to the current region state in ZK.</para>
          </listitem>

          <listitem>
            <para>Before processing the regions in transition, the normal
            handlers start to ensure we don't miss any transitions. The
            handling of opens on the RS side ensures we don't dupe assign even
            if things have changed before we finish acting on
            them.<itemizedlist>
                <listitem>
                  <para>OFFLINE Generate a new assignment and send an OPEN
                  RPC.</para>
                </listitem>

                <listitem>
                  <para>CLOSING Nothing to be done. Normal handlers take care
                  of timeouts.</para>
                </listitem>

                <listitem>
                  <para>CLOSED Generate a new assignment and send an OPEN
                  RPC.</para>
                </listitem>

                <listitem>
                  <para>OPENING Nothing to be done. Normal handlers take care
                  of timeouts.</para>
                </listitem>

                <listitem>
                  <para>OPENED Delete the node from ZK. Region was
                  successfully opened but the previous Master did not
                  acknowledge it.</para>
                </listitem>
              </itemizedlist></para>
          </listitem>

          <listitem>
            <para>Once this is done, everything further is dealt with as
            normal by the RegionManager.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Summary of Region Transition States</title>

        <note>
          <para>Check below is complete -- St.Ack 20100901</para>
        </note>

        <section>
          <title>Master</title>

          <itemizedlist>
            <listitem>
              <para>Master creates an unassigned node as OFFLINE.</para>

              <para>Cluster startup and table enabling.</para>
            </listitem>

            <listitem>
              <para>Master forces an existing unassigned node to
              OFFLINE.</para>

              <para>RegionServer failure.</para>

              <para>Allows transitions from all states to OFFLINE.</para>
            </listitem>

            <listitem>
              <para>Master deletes an unassigned node that was in a OPENED
              state.</para>

              <para>Normal region transitions. Besides cluster startup, no
              other deletions of unassigned nodes is allowed.</para>
            </listitem>

            <listitem>
              <para>Master deletes all unassigned nodes regardless of
              state.</para>

              <para>Cluster startup before any assignment happens.</para>
            </listitem>
          </itemizedlist>
        </section>

        <section>
          <title>RegionServer</title>

          <itemizedlist>
            <listitem>
              <para>RegionServer creates an unassigned node as CLOSING.</para>

              <para>All region closes will do this in response to a CLOSE RPC
              from Master.</para>

              <para>A node can never be transitioned to CLOSING, only
              created.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from CLOSING
              to CLOSED.</para>

              <para>Normal region closes. CAS operation.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from OFFLINE
              to OPENING.</para>

              <para>All region opens will do this in response to an OPEN RPC
              from the Master.</para>

              <para>Normal region opens. CAS operation.</para>
            </listitem>

            <listitem>
              <para>RegionServer transitions an unassigned node from OPENING
              to OPENED.</para>

              <para>Normal region opens. CAS operation.</para>
            </listitem>
          </itemizedlist>
        </section>
      </section>
      <section>
      <title>Region Splits</title>
      <para>Splits run unaided on the RegionServer; i.e. the Master does not
      participate. The RegionServer splits
      a region, offlines the split region and then adds the daughter regions
      to META, opens daughters on the parent's hosting RegionServer and then
      reports the split to the master.
      </para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>The WAL</title>

    <subtitle>HBase's<link
    xlink:href="http://en.wikipedia.org/wiki/Write-ahead_logging"> Write-Ahead
    Log</link></subtitle>

    <para>Each RegionServer adds updates to its <link linkend="???">WAL</link>
    first, and then to memory.</para>

    <section>
      <title>What is the purpose of the HBase WAL</title>

      <para>The HBase WAL is...</para>
    </section>

    <section>
      <title>WAL splitting</title>

      <subtitle>How edits are recovered from a crashed RegionServer</subtitle>

      <para>When a RegionServer crashes, it will lose its ephemeral lease in
      ZooKeeper...TODO</para>

      <section>
        <title><varname>hbase.hlog.split.skip.errors</varname></title>

        <para>When set to <constant>true</constant>, the default, any error
        encountered splitting will be logged, the problematic WAL will be
        moved into the <filename>.corrupt</filename> directory under the hbase
        <varname>rootdir</varname>, and processing will continue. If set to
        <constant>false</constant>, the exception will be propagated and the
        split logged as failed.<footnote>
            <para>See <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-2958">HBASE-2958
            When hbase.hlog.split.skip.errors is set to false, we fail the
            split but thats it</link>. We need to do more than just fail split
            if this flag is set.</para>
          </footnote></para>
      </section>

      <section>
        <title>How EOFExceptions are treated when splitting a crashed
        RegionServers' WALs</title>

        <para>If we get an EOF while splitting logs, we proceed with the split
        even when <varname>hbase.hlog.split.skip.errors</varname> ==
        <constant>false</constant>. An EOF while reading the last log in the
        set of files to split is near-guaranteed since the RegionServer likely
        crashed mid-write of a record. But we'll continue even if we got an
        EOF reading other than the last file in the set.<footnote>
            <para>For background, see <link
            xlink:href="https://issues.apache.org/jira/browse/HBASE-2643">HBASE-2643
            Figure how to deal with eof splitting logs</link></para>
          </footnote></para>
      </section>
    </section>

    <section>
      <title>WAL Tools</title>

      <section>
        <title><classname>HLog</classname> tool</title>

        <para>The main method on <classname>HLog</classname> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <filename>recovered.edits</filename>. directory.</para>

        <para>You can get a textual dump of a WAL file content by doing the
        following:<programlisting> <code>$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --dump hdfs://example.org:9000/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </programlisting>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <varname>STDOUT</varname> to
        <code>/dev/null</code> and testing the program return.</para>

        <para>Similarily you can force a split of a log file directory by
        doing:<programlisting> $ ./<code>bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --split hdfs://example.org:9000/hbase/.logs/example.org,60020,1283516293161/</code></programlisting></para>
      </section>
    </section>
  </chapter>

  <chapter>
    <title>Bloom Filters</title>

    <para>Bloom filters were developed over in <link
    xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200
    Add bloomfilters</link>.<footnote>
        <para>For description of the development process -- why static blooms
        rather than dynamic -- and for an overview of the unique properties
        that pertain to blooms in HBase, as well as possible future
        directions, see the <emphasis>Development Process</emphasis> section
        of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> attached to <link
        xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200</link>.</para>
      </footnote><footnote>
        <para>The bloom filters described here are actually version two of
        blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom
        option based on work done by the <link
        xlink:href="http://www.one-lab.org">European Commission One-Lab
        Project 034819</link>. The core of the HBase bloom work was later
        pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile.
        Version 1 of HBase blooms never worked that well. Version 2 is a
        rewrite from scratch though again it starts with the one-lab
        work.</para>
      </footnote></para>

    <section>
      <title>Configurations</title>

      <para>Blooms are enabled by specifying options on a column family in the
      HBase shell or in java code as specification on <classname>org.apache.hadoop.hbase.HColumnDescriptor</classname>.</para>

      <section>
        <title><code>HColumnDescriptor</code> option</title>

        <para>Use <code>HColumnDescriptor.setBloomFilterType(NONE | ROW |
        ROWCOL)</code> to enable blooms per Column Family. Default =
        <varname>NONE</varname> for no bloom filters. If
        <varname>ROW</varname>, the hash of the row will be added to the bloom
        on each insert. If <varname>ROWCOL</varname>, the hash of the row +
        column family + column family qualifier will be added to the bloom on
        each key insert.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.enabled</varname> global kill
        switch</title>

        <para><code>io.hfile.bloom.enabled</code> in
        <classname>Configuration</classname> serves as the kill switch in case
        something goes wrong. Default = <varname>true</varname>.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.error.rate</varname></title>

        <para><varname>io.hfile.bloom.error.rate</varname> = average false
        positive rate. Default = 1%. Decrease rate by ½ (e.g. to .5%) == +1
        bit per bloom entry.</para>
      </section>

      <section>
        <title><varname>io.hfile.bloom.max.fold</varname></title>

        <para><varname>io.hfile.bloom.max.fold</varname> = guaranteed minimum
        fold rate. Most people should leave this alone. Default = 7, or can
        collapse to at least 1/128th of original size. See the
        <emphasis>Development Process</emphasis> section of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> for more on what this option means.</para>
      </section>
    </section>

    <section>
      <title>Bloom StoreFile footprint</title>

      <para>Bloom filters add an entry to the <classname>StoreFile</classname>
      general <classname>FileInfo</classname> data structure and then two
      extra entries to the <classname>StoreFile</classname> metadata
      section.</para>

      <section>
        <title>BloomFilter in the <classname>StoreFile</classname>
        <classname>FileInfo</classname> data structure</title>

        <section>
          <title><varname>BLOOM_FILTER_TYPE</varname></title>

          <para><classname>FileInfo</classname> has a
          <varname>BLOOM_FILTER_TYPE</varname> entry which is set to
          <varname>NONE</varname>, <varname>ROW</varname> or
          <varname>ROWCOL.</varname></para>
        </section>
      </section>

      <section>
        <title>BloomFilter entries in <classname>StoreFile</classname>
        metadata</title>

        <section>
          <title><varname>BLOOM_FILTER_META</varname></title>

          <para><varname>BLOOM_FILTER_META</varname> holds Bloom Size, Hash
          Function used, etc. Its small in size and is cached on
          <classname>StoreFile.Reader</classname> load</para>
        </section>

        <section>
          <title><varname>BLOOM_FILTER_DATA</varname></title>

          <para><varname>BLOOM_FILTER_DATA</varname> is the actual bloomfilter
          data. Obtained on-demand. Stored in the LRU cache, if it is enabled
          (Its enabled by default).</para>
        </section>
      </section>
    </section>
  </chapter>

  <appendix>
    <title>Tools</title>

    <para>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</para>
  </appendix>
</book>
