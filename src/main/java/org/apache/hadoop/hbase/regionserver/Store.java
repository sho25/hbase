begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2010 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CopyOnWriteArraySet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|locks
operator|.
name|ReentrantReadWriteLock
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HRegionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
operator|.
name|KeyComparator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|RemoteExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Get
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Scan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|HeapSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|Compression
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFile
operator|.
name|Reader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFileScanner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ClassSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|FSUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|ImmutableList
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Iterables
import|;
end_import

begin_comment
comment|/**   * A Store holds a column family in a Region.  Its a memstore and a set of zero   * or more StoreFiles, which stretch backwards over time.   *   *<p>There's no reason to consider append-logging at this level; all logging   * and locking is handled at the HRegion level.  Store just provides   * services to manage sets of StoreFiles.  One of the most important of those   * services is compaction services where files are aggregated once they pass   * a configurable threshold.  *  *<p>Locking and transactions are handled at a higher level.  This API should  * not be called directly but by an HRegion manager.  */
end_comment

begin_class
specifier|public
class|class
name|Store
implements|implements
name|HeapSize
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|Store
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|protected
specifier|final
name|MemStore
name|memstore
decl_stmt|;
comment|// This stores directory in the filesystem.
specifier|private
specifier|final
name|Path
name|homedir
decl_stmt|;
specifier|private
specifier|final
name|HRegion
name|region
decl_stmt|;
specifier|private
specifier|final
name|HColumnDescriptor
name|family
decl_stmt|;
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|final
name|Configuration
name|conf
decl_stmt|;
comment|// ttl in milliseconds.
specifier|protected
name|long
name|ttl
decl_stmt|;
specifier|private
name|long
name|majorCompactionTime
decl_stmt|;
specifier|private
name|int
name|maxFilesToCompact
decl_stmt|;
specifier|private
specifier|final
name|long
name|desiredMaxFileSize
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|storeSize
init|=
literal|0L
decl_stmt|;
specifier|private
specifier|final
name|Object
name|flushLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|final
name|ReentrantReadWriteLock
name|lock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|String
name|storeNameStr
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|inMemory
decl_stmt|;
comment|/*    * List of store files inside this store. This is an immutable list that    * is atomically replaced when its contents change.    */
specifier|private
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|storefiles
init|=
literal|null
decl_stmt|;
comment|// All access must be synchronized.
specifier|private
specifier|final
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
name|changedReaderObservers
init|=
operator|new
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|Path
name|regionCompactionDir
decl_stmt|;
specifier|private
specifier|final
name|Object
name|compactLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|int
name|compactionThreshold
decl_stmt|;
specifier|private
specifier|final
name|int
name|blocksize
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|blockcache
decl_stmt|;
specifier|private
specifier|final
name|Compression
operator|.
name|Algorithm
name|compression
decl_stmt|;
comment|// Comparing KeyValues
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|comparator
decl_stmt|;
comment|/**    * Constructor    * @param basedir qualified path under which the region directory lives;    * generally the table subdirectory    * @param region    * @param family HColumnDescriptor for this column    * @param fs file system object    * @param conf configuration object    * failed.  Can be null.    * @throws IOException    */
specifier|protected
name|Store
parameter_list|(
name|Path
name|basedir
parameter_list|,
name|HRegion
name|region
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|HRegionInfo
name|info
init|=
name|region
operator|.
name|regionInfo
decl_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|homedir
operator|=
name|getStoreHomedir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|mkdirs
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
condition|)
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Failed create of: "
operator|+
name|this
operator|.
name|homedir
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
name|this
operator|.
name|region
operator|=
name|region
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|blockcache
operator|=
name|family
operator|.
name|isBlockCacheEnabled
argument_list|()
expr_stmt|;
name|this
operator|.
name|blocksize
operator|=
name|family
operator|.
name|getBlocksize
argument_list|()
expr_stmt|;
name|this
operator|.
name|compression
operator|=
name|family
operator|.
name|getCompression
argument_list|()
expr_stmt|;
name|this
operator|.
name|comparator
operator|=
name|info
operator|.
name|getComparator
argument_list|()
expr_stmt|;
comment|// getTimeToLive returns ttl in seconds.  Convert to milliseconds.
name|this
operator|.
name|ttl
operator|=
name|family
operator|.
name|getTimeToLive
argument_list|()
expr_stmt|;
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
condition|)
block|{
comment|// default is unlimited ttl.
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|ttl
operator|==
operator|-
literal|1
condition|)
block|{
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
else|else
block|{
comment|// second -> ms adjust for user data
name|this
operator|.
name|ttl
operator|*=
literal|1000
expr_stmt|;
block|}
name|this
operator|.
name|memstore
operator|=
operator|new
name|MemStore
argument_list|(
name|this
operator|.
name|comparator
argument_list|)
expr_stmt|;
name|this
operator|.
name|regionCompactionDir
operator|=
operator|new
name|Path
argument_list|(
name|HRegion
operator|.
name|getCompactionDir
argument_list|(
name|basedir
argument_list|)
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeNameStr
operator|=
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
comment|// By default, we compact if an HStore has more than
comment|// MIN_COMMITS_FOR_COMPACTION map files
name|this
operator|.
name|compactionThreshold
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
expr_stmt|;
comment|// Check if this is in-memory store
name|this
operator|.
name|inMemory
operator|=
name|family
operator|.
name|isInMemory
argument_list|()
expr_stmt|;
comment|// By default we split region if a file> HConstants.DEFAULT_MAX_FILE_SIZE.
name|long
name|maxFileSize
init|=
name|info
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getMaxFileSize
argument_list|()
decl_stmt|;
if|if
condition|(
name|maxFileSize
operator|==
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
condition|)
block|{
name|maxFileSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hregion.max.filesize"
argument_list|,
name|HConstants
operator|.
name|DEFAULT_MAX_FILE_SIZE
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|desiredMaxFileSize
operator|=
name|maxFileSize
expr_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|,
literal|86400000
argument_list|)
expr_stmt|;
if|if
condition|(
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|String
name|strCompactionTime
init|=
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
decl_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
operator|(
operator|new
name|Long
argument_list|(
name|strCompactionTime
argument_list|)
operator|)
operator|.
name|longValue
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|maxFilesToCompact
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.max"
argument_list|,
literal|10
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|=
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|loadStoreFiles
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|HColumnDescriptor
name|getFamily
parameter_list|()
block|{
return|return
name|this
operator|.
name|family
return|;
block|}
comment|/**    * @return The maximum sequence id in all store files.    */
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|this
operator|.
name|getStorefiles
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * @param tabledir    * @param encodedName Encoded region name.    * @param family    * @return Path to family/Store home directory.    */
specifier|public
specifier|static
name|Path
name|getStoreHomedir
parameter_list|(
specifier|final
name|Path
name|tabledir
parameter_list|,
specifier|final
name|String
name|encodedName
parameter_list|,
specifier|final
name|byte
index|[]
name|family
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|tabledir
argument_list|,
operator|new
name|Path
argument_list|(
name|encodedName
argument_list|,
operator|new
name|Path
argument_list|(
name|Bytes
operator|.
name|toString
argument_list|(
name|family
argument_list|)
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
comment|/*    * Creates a series of StoreFile loaded from the given directory.    * @throws IOException    */
specifier|private
name|List
argument_list|<
name|StoreFile
argument_list|>
name|loadStoreFiles
parameter_list|()
throws|throws
name|IOException
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|FileStatus
name|files
index|[]
init|=
name|this
operator|.
name|fs
operator|.
name|listStatus
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|files
operator|!=
literal|null
operator|&&
name|i
operator|<
name|files
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
comment|// Skip directories.
if|if
condition|(
name|files
index|[
name|i
index|]
operator|.
name|isDir
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|Path
name|p
init|=
name|files
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Check for empty file.  Should never be the case but can happen
comment|// after data loss in hdfs for whatever reason (upgrade, etc.): HBASE-646
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|p
argument_list|)
operator|.
name|getLen
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Skipping "
operator|+
name|p
operator|+
literal|" because its empty. HBASE-646 DATA LOSS?"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|StoreFile
name|curfile
init|=
literal|null
decl_stmt|;
try|try
block|{
name|curfile
operator|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|p
argument_list|,
name|blockcache
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|inMemory
argument_list|)
expr_stmt|;
name|curfile
operator|.
name|createReader
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed open of "
operator|+
name|p
operator|+
literal|"; presumption is that file was "
operator|+
literal|"corrupted at flush and lost edits picked up by commit log replay. "
operator|+
literal|"Verify!"
argument_list|,
name|ioe
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|length
init|=
name|curfile
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|length
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded "
operator|+
name|curfile
operator|.
name|toStringDetailed
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|results
operator|.
name|add
argument_list|(
name|curfile
argument_list|)
expr_stmt|;
block|}
name|Collections
operator|.
name|sort
argument_list|(
name|results
argument_list|,
name|StoreFile
operator|.
name|Comparators
operator|.
name|FLUSH_TIME
argument_list|)
expr_stmt|;
return|return
name|results
return|;
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|add
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|add
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|delete
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|delete
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return All store files.    */
name|List
argument_list|<
name|StoreFile
argument_list|>
name|getStorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
return|;
block|}
specifier|public
name|void
name|bulkLoadHFile
parameter_list|(
name|String
name|srcPathStr
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|srcPath
init|=
operator|new
name|Path
argument_list|(
name|srcPathStr
argument_list|)
decl_stmt|;
name|HFile
operator|.
name|Reader
name|reader
init|=
literal|null
decl_stmt|;
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Validating hfile at "
operator|+
name|srcPath
operator|+
literal|" for inclusion in "
operator|+
literal|"store "
operator|+
name|this
operator|+
literal|" region "
operator|+
name|this
operator|.
name|region
argument_list|)
expr_stmt|;
name|reader
operator|=
operator|new
name|HFile
operator|.
name|Reader
argument_list|(
name|srcPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|srcPath
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|reader
operator|.
name|loadFileInfo
argument_list|()
expr_stmt|;
name|byte
index|[]
name|firstKey
init|=
name|reader
operator|.
name|getFirstRowKey
argument_list|()
decl_stmt|;
name|byte
index|[]
name|lastKey
init|=
name|reader
operator|.
name|getLastRowKey
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"HFile bounds: first="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|firstKey
argument_list|)
operator|+
literal|" last="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|lastKey
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Region bounds: first="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|region
operator|.
name|getStartKey
argument_list|()
argument_list|)
operator|+
literal|" last="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|region
operator|.
name|getEndKey
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|HRegionInfo
name|hri
init|=
name|region
operator|.
name|getRegionInfo
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|hri
operator|.
name|containsRange
argument_list|(
name|firstKey
argument_list|,
name|lastKey
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|WrongRegionException
argument_list|(
literal|"Bulk load file "
operator|+
name|srcPathStr
operator|+
literal|" does not fit inside region "
operator|+
name|this
operator|.
name|region
argument_list|)
throw|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Move the file if it's on another filesystem
name|FileSystem
name|srcFs
init|=
name|srcPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|srcFs
operator|.
name|equals
argument_list|(
name|fs
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"File "
operator|+
name|srcPath
operator|+
literal|" on different filesystem than "
operator|+
literal|"destination store - moving to this filesystem."
argument_list|)
expr_stmt|;
name|Path
name|tmpDir
init|=
operator|new
name|Path
argument_list|(
name|homedir
argument_list|,
literal|"_tmp"
argument_list|)
decl_stmt|;
name|Path
name|tmpPath
init|=
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|tmpDir
argument_list|)
decl_stmt|;
name|FileUtil
operator|.
name|copy
argument_list|(
name|srcFs
argument_list|,
name|srcPath
argument_list|,
name|fs
argument_list|,
name|tmpPath
argument_list|,
literal|false
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Copied to temporary path on dst filesystem: "
operator|+
name|tmpPath
argument_list|)
expr_stmt|;
name|srcPath
operator|=
name|tmpPath
expr_stmt|;
block|}
name|Path
name|dstPath
init|=
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|homedir
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Renaming bulk load file "
operator|+
name|srcPath
operator|+
literal|" to "
operator|+
name|dstPath
argument_list|)
expr_stmt|;
name|StoreFile
operator|.
name|rename
argument_list|(
name|fs
argument_list|,
name|srcPath
argument_list|,
name|dstPath
argument_list|)
expr_stmt|;
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|dstPath
argument_list|,
name|blockcache
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|inMemory
argument_list|)
decl_stmt|;
name|sf
operator|.
name|createReader
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Moved hfile "
operator|+
name|srcPath
operator|+
literal|" into store directory "
operator|+
name|homedir
operator|+
literal|" - updating store file list."
argument_list|)
expr_stmt|;
comment|// Append the new storefile into the list
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newFiles
operator|.
name|add
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|=
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|newFiles
argument_list|)
expr_stmt|;
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Successfully loaded store file "
operator|+
name|srcPath
operator|+
literal|" into store "
operator|+
name|this
operator|+
literal|" (new location: "
operator|+
name|dstPath
operator|+
literal|")"
argument_list|)
expr_stmt|;
block|}
comment|/**    * Close all the readers    *    * We don't need to worry about subsequent requests because the HRegion holds    * a write lock that will prevent any more reads or writes.    *    * @throws IOException    */
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|result
init|=
name|storefiles
decl_stmt|;
comment|// Clear so metrics doesn't find them.
name|storefiles
operator|=
name|ImmutableList
operator|.
name|of
argument_list|()
expr_stmt|;
for|for
control|(
name|StoreFile
name|f
range|:
name|result
control|)
block|{
name|f
operator|.
name|closeReader
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"closed "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Snapshot this stores memstore.  Call before running    * {@link #flushCache(long, SortedSet<KeyValue>)} so it has some work to do.    */
name|void
name|snapshot
parameter_list|()
block|{
name|this
operator|.
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
block|}
comment|/**    * Write out current snapshot.  Presumes {@link #snapshot()} has been called    * previously.    * @param logCacheFlushId flush sequence number    * @return true if a compaction is needed    * @throws IOException    */
specifier|private
name|StoreFile
name|flushCache
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If an exception happens flushing, we let it out without clearing
comment|// the memstore snapshot.  The old snapshot will be returned when we say
comment|// 'snapshot', the next time flush comes around.
return|return
name|internalFlushCache
argument_list|(
name|snapshot
argument_list|,
name|logCacheFlushId
argument_list|)
return|;
block|}
comment|/*    * @param cache    * @param logCacheFlushId    * @return StoreFile created.    * @throws IOException    */
specifier|private
name|StoreFile
name|internalFlushCache
parameter_list|(
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|,
specifier|final
name|long
name|logCacheFlushId
parameter_list|)
throws|throws
name|IOException
block|{
name|StoreFile
operator|.
name|Writer
name|writer
init|=
literal|null
decl_stmt|;
name|long
name|flushed
init|=
literal|0
decl_stmt|;
comment|// Don't flush if there are no entries.
if|if
condition|(
name|set
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
name|long
name|oldestTimestamp
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|-
name|ttl
decl_stmt|;
comment|// TODO:  We can fail in the below block before we complete adding this
comment|// flush to list of store files.  Add cleanup of anything put on filesystem
comment|// if we fail.
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
comment|// A. Write the map out to the disk
name|writer
operator|=
name|createWriter
argument_list|(
name|this
operator|.
name|homedir
argument_list|,
name|set
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|int
name|entries
init|=
literal|0
decl_stmt|;
try|try
block|{
for|for
control|(
name|KeyValue
name|kv
range|:
name|set
control|)
block|{
if|if
condition|(
operator|!
name|isExpired
argument_list|(
name|kv
argument_list|,
name|oldestTimestamp
argument_list|)
condition|)
block|{
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
name|entries
operator|++
expr_stmt|;
name|flushed
operator|+=
name|this
operator|.
name|memstore
operator|.
name|heapSizeChange
argument_list|(
name|kv
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
comment|// Write out the log sequence number that corresponds to this output
comment|// hfile.  The hfile is current up to and including logCacheFlushId.
name|writer
operator|.
name|appendMetadata
argument_list|(
name|logCacheFlushId
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|writer
operator|.
name|getPath
argument_list|()
argument_list|,
name|blockcache
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|inMemory
argument_list|)
decl_stmt|;
name|Reader
name|r
init|=
name|sf
operator|.
name|createReader
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Added "
operator|+
name|sf
operator|+
literal|", entries="
operator|+
name|r
operator|.
name|getEntries
argument_list|()
operator|+
literal|", sequenceid="
operator|+
name|logCacheFlushId
operator|+
literal|", memsize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|flushed
argument_list|)
operator|+
literal|", filesize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|r
operator|.
name|length
argument_list|()
argument_list|)
operator|+
literal|" to "
operator|+
name|this
operator|.
name|region
operator|.
name|regionInfo
operator|.
name|getRegionNameAsString
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|sf
return|;
block|}
comment|/*    * @return Writer for this store.    * @param basedir Directory to put writer in.    * @throws IOException    */
specifier|private
name|StoreFile
operator|.
name|Writer
name|createWriter
parameter_list|(
specifier|final
name|Path
name|basedir
parameter_list|,
name|int
name|maxKeyCount
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|StoreFile
operator|.
name|createWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|basedir
argument_list|,
name|this
operator|.
name|blocksize
argument_list|,
name|this
operator|.
name|compression
argument_list|,
name|this
operator|.
name|comparator
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|maxKeyCount
argument_list|)
return|;
block|}
comment|/*    * Change storefiles adding into place the Reader produced by this new flush.    * @param sf    * @param set That was used to make the passed file<code>p</code>.    * @throws IOException    * @return Whether compaction is required.    */
specifier|private
name|boolean
name|updateStorefiles
parameter_list|(
specifier|final
name|StoreFile
name|sf
parameter_list|,
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|storefiles
operator|=
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|newList
argument_list|)
expr_stmt|;
name|this
operator|.
name|memstore
operator|.
name|clearSnapshot
argument_list|(
name|set
argument_list|)
expr_stmt|;
comment|// Tell listeners of the change in readers.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>=
name|this
operator|.
name|compactionThreshold
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Notify all observers that set of Readers has changed.    * @throws IOException    */
specifier|private
name|void
name|notifyChangedReadersObservers
parameter_list|()
throws|throws
name|IOException
block|{
for|for
control|(
name|ChangedReadersObserver
name|o
range|:
name|this
operator|.
name|changedReaderObservers
control|)
block|{
name|o
operator|.
name|updateReaders
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * @param o Observer who wants to know about changes in set of Readers    */
name|void
name|addChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
name|this
operator|.
name|changedReaderObservers
operator|.
name|add
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|/*    * @param o Observer no longer interested in changes in set of Readers.    */
name|void
name|deleteChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|changedReaderObservers
operator|.
name|remove
argument_list|(
name|o
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Not in set"
operator|+
name|o
argument_list|)
expr_stmt|;
block|}
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the StoreFiles.  This method may take some time, so the calling    * thread must be able to block for long periods.    *    *<p>During this time, the Store can work as usual, getting values from    * StoreFiles and writing new StoreFiles from the memstore.    *    * Existing StoreFiles are not destroyed until the new compacted StoreFile is    * completely written-out to disk.    *    *<p>The compactLock prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *    *<p>We don't want to hold the structureLock for the whole time, as a compact()    * can be lengthy and we want to allow cache-flushes during this period.    *    * @param mc True to force a major compaction regardless of thresholds    * @return row to split around if a split is needed, null otherwise    * @throws IOException    */
name|StoreSize
name|compact
parameter_list|(
specifier|final
name|boolean
name|mc
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|forceSplit
init|=
name|this
operator|.
name|region
operator|.
name|shouldSplit
argument_list|(
literal|false
argument_list|)
decl_stmt|;
name|boolean
name|majorcompaction
init|=
name|mc
decl_stmt|;
synchronized|synchronized
init|(
name|compactLock
init|)
block|{
comment|// filesToCompact are sorted oldest to newest.
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
name|this
operator|.
name|storefiles
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|this
operator|.
name|storeNameStr
operator|+
literal|": no store files to compact"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// Max-sequenceID is the last key of the storefiles TreeMap
name|long
name|maxId
init|=
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
comment|// Check to see if we need to do a major compaction on this region.
comment|// If so, change doMajorCompaction to true to skip the incremental
comment|// compacting below. Only check if doMajorCompaction is not true.
if|if
condition|(
operator|!
name|majorcompaction
condition|)
block|{
name|majorcompaction
operator|=
name|isMajorCompaction
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
name|boolean
name|references
init|=
name|hasReferences
argument_list|(
name|filesToCompact
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|references
operator|&&
operator|(
name|forceSplit
operator|||
operator|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|compactionThreshold
operator|)
operator|)
condition|)
block|{
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
operator|&&
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Mkdir on "
operator|+
name|this
operator|.
name|regionCompactionDir
operator|.
name|toString
argument_list|()
operator|+
literal|" failed"
argument_list|)
expr_stmt|;
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
comment|// HBASE-745, preparing all store file sizes for incremental compacting
comment|// selection.
name|int
name|countOfFiles
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
name|totalSize
init|=
literal|0
decl_stmt|;
name|long
index|[]
name|fileSizes
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
name|long
name|skipped
init|=
literal|0
decl_stmt|;
name|int
name|point
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|countOfFiles
condition|;
name|i
operator|++
control|)
block|{
name|StoreFile
name|file
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|Path
name|path
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Path is null for "
operator|+
name|file
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|Reader
name|r
init|=
name|file
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|file
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|long
name|len
init|=
name|file
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|fileSizes
index|[
name|i
index|]
operator|=
name|len
expr_stmt|;
name|totalSize
operator|+=
name|len
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|references
condition|)
block|{
comment|// Here we select files for incremental compaction.
comment|// The rule is: if the largest(oldest) one is more than twice the
comment|// size of the second, skip the largest, and continue to next...,
comment|// until we meet the compactionThreshold limit.
comment|// A problem with the above heuristic is that we could go through all of
comment|// filesToCompact and the above condition could hold for all files and
comment|// we'd end up with nothing to compact.  To protect against this, we'll
comment|// compact the tail -- up to the last 4 files -- of filesToCompact
comment|// regardless.
name|int
name|tail
init|=
name|Math
operator|.
name|min
argument_list|(
name|countOfFiles
argument_list|,
literal|4
argument_list|)
decl_stmt|;
for|for
control|(
name|point
operator|=
literal|0
init|;
name|point
operator|<
operator|(
name|countOfFiles
operator|-
name|tail
operator|)
condition|;
name|point
operator|++
control|)
block|{
if|if
condition|(
operator|(
operator|(
name|fileSizes
index|[
name|point
index|]
operator|<
name|fileSizes
index|[
name|point
operator|+
literal|1
index|]
operator|*
literal|2
operator|)
operator|&&
operator|(
name|countOfFiles
operator|-
name|point
operator|)
operator|<=
name|maxFilesToCompact
operator|)
condition|)
block|{
break|break;
block|}
name|skipped
operator|+=
name|fileSizes
index|[
name|point
index|]
expr_stmt|;
block|}
name|filesToCompact
operator|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|filesToCompact
operator|.
name|subList
argument_list|(
name|point
argument_list|,
name|countOfFiles
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<=
literal|1
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipped compaction of 1 file; compaction size of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|": "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|"; Skipped "
operator|+
name|point
operator|+
literal|" files, size: "
operator|+
name|skipped
argument_list|)
expr_stmt|;
block|}
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Compaction size of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|": "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|"; Skipped "
operator|+
name|point
operator|+
literal|" file(s), size: "
operator|+
name|skipped
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Ready to go.  Have list of files to compact.
name|LOG
operator|.
name|info
argument_list|(
literal|"Started compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" of "
operator|+
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|getRegionNameAsString
argument_list|()
operator|+
operator|(
name|references
condition|?
literal|", hasReferences=true,"
else|:
literal|" "
operator|)
operator|+
literal|" into "
operator|+
name|FSUtils
operator|.
name|getPath
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|)
operator|+
literal|", seqid="
operator|+
name|maxId
argument_list|)
expr_stmt|;
name|HFile
operator|.
name|Writer
name|writer
init|=
name|compact
argument_list|(
name|filesToCompact
argument_list|,
name|majorcompaction
argument_list|,
name|maxId
argument_list|)
decl_stmt|;
comment|// Move the compaction into place.
name|StoreFile
name|sf
init|=
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|writer
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Completed"
operator|+
operator|(
name|majorcompaction
condition|?
literal|" major "
else|:
literal|" "
operator|)
operator|+
literal|"compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" of "
operator|+
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|getRegionNameAsString
argument_list|()
operator|+
literal|"; new storefile is "
operator|+
operator|(
name|sf
operator|==
literal|null
condition|?
literal|"none"
else|:
name|sf
operator|.
name|toString
argument_list|()
operator|)
operator|+
literal|"; store size is "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|storeSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|checkSplit
argument_list|(
name|forceSplit
argument_list|)
return|;
block|}
comment|/*    * @param files    * @return True if any of the files in<code>files</code> are References.    */
specifier|private
name|boolean
name|hasReferences
parameter_list|(
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|StoreFile
name|hsf
range|:
name|files
control|)
block|{
if|if
condition|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/*    * Gets lowest timestamp from files in a dir    *    * @param fs    * @param dir    * @throws IOException    */
specifier|private
specifier|static
name|long
name|getLowestTimestamp
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
name|FileStatus
index|[]
name|stats
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|)
decl_stmt|;
if|if
condition|(
name|stats
operator|==
literal|null
operator|||
name|stats
operator|.
name|length
operator|==
literal|0
condition|)
block|{
return|return
literal|0l
return|;
block|}
name|long
name|lowTimestamp
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|stats
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|long
name|timestamp
init|=
name|stats
index|[
name|i
index|]
operator|.
name|getModificationTime
argument_list|()
decl_stmt|;
if|if
condition|(
name|timestamp
operator|<
name|lowTimestamp
condition|)
block|{
name|lowTimestamp
operator|=
name|timestamp
expr_stmt|;
block|}
block|}
return|return
name|lowTimestamp
return|;
block|}
comment|/*    * @return True if we should run a major compaction.    */
name|boolean
name|isMajorCompaction
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|isMajorCompaction
argument_list|(
name|storefiles
argument_list|)
return|;
block|}
comment|/*    * @param filesToCompact Files to compact. Can be null.    * @return True if we should run a major compaction.    */
specifier|private
name|boolean
name|isMajorCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|==
literal|null
operator|||
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
operator|||
name|majorCompactionTime
operator|==
literal|0
condition|)
block|{
return|return
name|result
return|;
block|}
name|long
name|lowTimestamp
init|=
name|getLowestTimestamp
argument_list|(
name|fs
argument_list|,
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
argument_list|)
decl_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
if|if
condition|(
name|lowTimestamp
operator|>
literal|0l
operator|&&
name|lowTimestamp
operator|<
operator|(
name|now
operator|-
name|this
operator|.
name|majorCompactionTime
operator|)
condition|)
block|{
comment|// Major compaction time has elapsed.
name|long
name|elapsedTime
init|=
name|now
operator|-
name|lowTimestamp
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
literal|1
operator|&&
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|isMajorCompaction
argument_list|()
operator|&&
operator|(
name|this
operator|.
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|elapsedTime
operator|<
name|this
operator|.
name|ttl
operator|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping major compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" because one (major) compacted file only and elapsedTime "
operator|+
name|elapsedTime
operator|+
literal|"ms is< ttl="
operator|+
name|this
operator|.
name|ttl
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Major compaction triggered on store "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|"; time since last major compaction "
operator|+
operator|(
name|now
operator|-
name|lowTimestamp
operator|)
operator|+
literal|"ms"
argument_list|)
expr_stmt|;
block|}
name|result
operator|=
literal|true
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
comment|/**    * Do a minor/major compaction.  Uses the scan infrastructure to make it easy.    *    * @param filesToCompact which files to compact    * @param majorCompaction true to major compact (prune all deletes, max versions, etc)    * @param maxId Readers maximum sequence id.    * @return Product of compaction or null if all cells expired or deleted and    * nothing made it through the compaction.    * @throws IOException    */
specifier|private
name|HFile
operator|.
name|Writer
name|compact
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|,
specifier|final
name|boolean
name|majorCompaction
parameter_list|,
specifier|final
name|long
name|maxId
parameter_list|)
throws|throws
name|IOException
block|{
comment|// calculate maximum key count after compaction (for blooms)
name|int
name|maxKeyCount
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|file
range|:
name|filesToCompact
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|file
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|!=
literal|null
condition|)
block|{
comment|// NOTE: getFilterEntries could cause under-sized blooms if the user
comment|//       switches bloom type (e.g. from ROW to ROWCOL)
name|maxKeyCount
operator|+=
operator|(
name|r
operator|.
name|getBloomFilterType
argument_list|()
operator|==
name|family
operator|.
name|getBloomFilterType
argument_list|()
operator|)
condition|?
name|r
operator|.
name|getFilterEntries
argument_list|()
else|:
name|r
operator|.
name|getEntries
argument_list|()
expr_stmt|;
block|}
block|}
comment|// For each file, obtain a scanner:
name|List
argument_list|<
name|StoreFileScanner
argument_list|>
name|scanners
init|=
name|StoreFileScanner
operator|.
name|getScannersForStoreFiles
argument_list|(
name|filesToCompact
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|// Make the instantiation lazy in case compaction produces no product; i.e.
comment|// where all source cells are expired or deleted.
name|StoreFile
operator|.
name|Writer
name|writer
init|=
literal|null
decl_stmt|;
try|try
block|{
if|if
condition|(
name|majorCompaction
condition|)
block|{
name|InternalScanner
name|scanner
init|=
literal|null
decl_stmt|;
try|try
block|{
name|Scan
name|scan
init|=
operator|new
name|Scan
argument_list|()
decl_stmt|;
name|scan
operator|.
name|setMaxVersions
argument_list|(
name|family
operator|.
name|getMaxVersions
argument_list|()
argument_list|)
expr_stmt|;
name|scanner
operator|=
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|scanners
argument_list|)
expr_stmt|;
comment|// since scanner.next() can return 'false' but still be delivering data,
comment|// we have to use a do/while loop.
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
name|kvs
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
while|while
condition|(
name|scanner
operator|.
name|next
argument_list|(
name|kvs
argument_list|)
condition|)
block|{
comment|// output to writer:
for|for
control|(
name|KeyValue
name|kv
range|:
name|kvs
control|)
block|{
if|if
condition|(
name|writer
operator|==
literal|null
condition|)
block|{
name|writer
operator|=
name|createWriter
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|,
name|maxKeyCount
argument_list|)
expr_stmt|;
block|}
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
block|}
name|kvs
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|scanner
operator|!=
literal|null
condition|)
block|{
name|scanner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|MinorCompactingStoreScanner
name|scanner
init|=
literal|null
decl_stmt|;
try|try
block|{
name|scanner
operator|=
operator|new
name|MinorCompactingStoreScanner
argument_list|(
name|this
argument_list|,
name|scanners
argument_list|)
expr_stmt|;
name|writer
operator|=
name|createWriter
argument_list|(
name|this
operator|.
name|regionCompactionDir
argument_list|,
name|maxKeyCount
argument_list|)
expr_stmt|;
while|while
condition|(
name|scanner
operator|.
name|next
argument_list|(
name|writer
argument_list|)
condition|)
block|{
comment|// Nothing to do
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|scanner
operator|!=
literal|null
condition|)
name|scanner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|writer
operator|!=
literal|null
condition|)
block|{
name|writer
operator|.
name|appendMetadata
argument_list|(
name|maxId
argument_list|,
name|majorCompaction
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|writer
return|;
block|}
comment|/*    * It's assumed that the compactLock  will be acquired prior to calling this    * method!  Otherwise, it is not thread-safe!    *    *<p>It works by processing a compaction that's been written to disk.    *    *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *    *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Moving the new compacted StoreFile into place    * 2) Unload all replaced StoreFile, close and collect list to delete.    * 3) Loading the new TreeMap.    * 4) Compute new store size    *</pre>    *    * @param compactedFiles list of files that were compacted    * @param compactedFile StoreFile that is the result of the compaction    * @return StoreFile created. May be null.    * @throws IOException    */
specifier|private
name|StoreFile
name|completeCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|compactedFiles
parameter_list|,
specifier|final
name|HFile
operator|.
name|Writer
name|compactedFile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// 1. Moving the new files into place -- if there is a new file (may not
comment|// be if all cells were expired or deleted).
name|StoreFile
name|result
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|compactedFile
operator|!=
literal|null
condition|)
block|{
name|Path
name|p
init|=
literal|null
decl_stmt|;
try|try
block|{
name|p
operator|=
name|StoreFile
operator|.
name|rename
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|this
operator|.
name|homedir
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|result
operator|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|p
argument_list|,
name|blockcache
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|this
operator|.
name|inMemory
argument_list|)
expr_stmt|;
name|result
operator|.
name|createReader
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
try|try
block|{
comment|// 2. Unloading
comment|// 3. Loading the new TreeMap.
comment|// Change this.storefiles so it reflects new state but do not
comment|// delete old store files until we have sent out notification of
comment|// change in case old files are still being accessed by outstanding
comment|// scanners.
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newStoreFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|StoreFile
name|sf
range|:
name|storefiles
control|)
block|{
if|if
condition|(
operator|!
name|compactedFiles
operator|.
name|contains
argument_list|(
name|sf
argument_list|)
condition|)
block|{
name|newStoreFiles
operator|.
name|add
argument_list|(
name|sf
argument_list|)
expr_stmt|;
block|}
block|}
comment|// If a StoreFile result, move it into place.  May be null.
if|if
condition|(
name|result
operator|!=
literal|null
condition|)
block|{
name|newStoreFiles
operator|.
name|add
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|storefiles
operator|=
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|newStoreFiles
argument_list|)
expr_stmt|;
comment|// Tell observers that list of StoreFiles has changed.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
comment|// Finally, delete old store files.
for|for
control|(
name|StoreFile
name|hsf
range|:
name|compactedFiles
control|)
block|{
name|hsf
operator|.
name|deleteReader
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|=
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files in "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|". Compacted file is "
operator|+
operator|(
name|result
operator|==
literal|null
condition|?
literal|"none"
else|:
name|result
operator|.
name|toString
argument_list|()
operator|)
operator|+
literal|".  Files replaced "
operator|+
name|compactedFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// 4. Compute new store size
name|this
operator|.
name|storeSize
operator|=
literal|0L
expr_stmt|;
for|for
control|(
name|StoreFile
name|hsf
range|:
name|this
operator|.
name|storefiles
control|)
block|{
name|Reader
name|r
init|=
name|hsf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|hsf
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
comment|// ////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * @return the number of files in this store    */
specifier|public
name|int
name|getNumberOfstorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/*    * @param wantedVersions How many versions were asked for.    * @return wantedVersions or this families' {@link HConstants#VERSIONS}.    */
name|int
name|versionsToReturn
parameter_list|(
specifier|final
name|int
name|wantedVersions
parameter_list|)
block|{
if|if
condition|(
name|wantedVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
comment|// Make sure we do not return more than maximum versions for this store.
name|int
name|maxVersions
init|=
name|this
operator|.
name|family
operator|.
name|getMaxVersions
argument_list|()
decl_stmt|;
return|return
name|wantedVersions
operator|>
name|maxVersions
condition|?
name|maxVersions
else|:
name|wantedVersions
return|;
block|}
specifier|static
name|void
name|expiredOrDeleted
parameter_list|(
specifier|final
name|Set
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|,
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|boolean
name|b
init|=
name|set
operator|.
name|remove
argument_list|(
name|kv
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|kv
operator|.
name|toString
argument_list|()
operator|+
literal|" expired: "
operator|+
name|b
argument_list|)
expr_stmt|;
block|}
block|}
specifier|static
name|boolean
name|isExpired
parameter_list|(
specifier|final
name|KeyValue
name|key
parameter_list|,
specifier|final
name|long
name|oldestTimestamp
parameter_list|)
block|{
return|return
name|key
operator|.
name|getTimestamp
argument_list|()
operator|<
name|oldestTimestamp
return|;
block|}
comment|/**    * Find the key that matches<i>row</i> exactly, or the one that immediately    * preceeds it. WARNING: Only use this method on a table where writes occur    * with strictly increasing timestamps. This method assumes this pattern of    * writes in order to make it reasonably performant.  Also our search is    * dependent on the axiom that deletes are for cells that are in the container    * that follows whether a memstore snapshot or a storefile, not for the    * current container: i.e. we'll see deletes before we come across cells we    * are to delete. Presumption is that the memstore#kvset is processed before    * memstore#snapshot and so on.    * @param kv First possible item on targeted row; i.e. empty columns, latest    * timestamp and maximum type.    * @return Found keyvalue or null if none found.    * @throws IOException    */
name|KeyValue
name|getRowKeyAtOrBefore
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
throws|throws
name|IOException
block|{
name|GetClosestRowBeforeTracker
name|state
init|=
operator|new
name|GetClosestRowBeforeTracker
argument_list|(
name|this
operator|.
name|comparator
argument_list|,
name|kv
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// First go to the memstore.  Pick up deletes and candidates.
name|this
operator|.
name|memstore
operator|.
name|getRowKeyAtOrBefore
argument_list|(
name|state
argument_list|)
expr_stmt|;
comment|// Check if match, if we got a candidate on the asked for 'kv' row.
comment|// Process each store file. Run through from newest to oldest.
for|for
control|(
name|StoreFile
name|sf
range|:
name|Iterables
operator|.
name|reverse
argument_list|(
name|storefiles
argument_list|)
control|)
block|{
comment|// Update the candidate keys from the current map file
name|rowAtOrBeforeFromStoreFile
argument_list|(
name|sf
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
return|return
name|state
operator|.
name|getCandidate
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Check an individual MapFile for the row at or before a given row.    * @param f    * @param state    * @throws IOException    */
specifier|private
name|void
name|rowAtOrBeforeFromStoreFile
parameter_list|(
specifier|final
name|StoreFile
name|f
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|Reader
name|r
init|=
name|f
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|f
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// TODO: Cache these keys rather than make each time?
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
name|KeyValue
name|firstOnRow
init|=
name|state
operator|.
name|getTargetKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|lastKV
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
block|{
comment|// If last key in file is not of the target table, no candidates in this
comment|// file.  Return.
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|lastKV
argument_list|)
condition|)
return|return;
comment|// If the row we're looking for is past the end of file, set search key to
comment|// last key. TODO: Cache last and first key rather than make each time.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|lastKV
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
block|}
comment|// Get a scanner that caches blocks and that uses pread.
name|HFileScanner
name|scanner
init|=
name|r
operator|.
name|getScanner
argument_list|(
literal|true
argument_list|,
literal|true
argument_list|)
decl_stmt|;
comment|// Seek scanner.  If can't seek it, return.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
return|return;
comment|// If we found candidate on firstOnRow, just return. THIS WILL NEVER HAPPEN!
comment|// Unlikely that there'll be an instance of actual first row in table.
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
return|return;
comment|// If here, need to start backing up.
while|while
condition|(
name|scanner
operator|.
name|seekBefore
argument_list|(
name|firstOnRow
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyLength
argument_list|()
argument_list|)
condition|)
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
if|if
condition|(
operator|!
name|state
operator|.
name|isBetterCandidate
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
comment|// Make new first on row.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|kv
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
comment|// Seek scanner.  If can't seek it, break.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
break|break;
comment|// If we find something, break;
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
break|break;
block|}
block|}
comment|/*    * Seek the file scanner to firstOnRow or first entry in file.    * @param scanner    * @param firstOnRow    * @param firstKV    * @return True if we successfully seeked scanner.    * @throws IOException    */
specifier|private
name|boolean
name|seekToScanner
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|KeyValue
name|firstKV
parameter_list|)
throws|throws
name|IOException
block|{
name|KeyValue
name|kv
init|=
name|firstOnRow
decl_stmt|;
comment|// If firstOnRow< firstKV, set to firstKV
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|firstKV
argument_list|,
name|firstOnRow
argument_list|)
operator|==
literal|0
condition|)
name|kv
operator|=
name|firstKV
expr_stmt|;
name|int
name|result
init|=
name|scanner
operator|.
name|seekTo
argument_list|(
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|result
operator|>=
literal|0
return|;
block|}
comment|/*    * When we come in here, we are probably at the kv just before we break into    * the row that firstOnRow is on.  Usually need to increment one time to get    * on to the row we are interested in.    * @param scanner    * @param firstOnRow    * @param state    * @return True we found a candidate.    * @throws IOException    */
specifier|private
name|boolean
name|walkForwardInSingleRow
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|foundCandidate
init|=
literal|false
decl_stmt|;
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
comment|// If we are not in the row, skip.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
continue|continue;
comment|// Did we go beyond the target row? If so break.
if|if
condition|(
name|state
operator|.
name|isTooFar
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
condition|)
break|break;
if|if
condition|(
name|state
operator|.
name|isExpired
argument_list|(
name|kv
argument_list|)
condition|)
block|{
continue|continue;
block|}
comment|// If we added something, this row is a contender. break.
if|if
condition|(
name|state
operator|.
name|handle
argument_list|(
name|kv
argument_list|)
condition|)
block|{
name|foundCandidate
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
return|return
name|foundCandidate
return|;
block|}
comment|/**    * Determines if HStore can be split    * @param force Whether to force a split or not.    * @return a StoreSize if store can be split, null otherwise.    */
name|StoreSize
name|checkSplit
parameter_list|(
specifier|final
name|boolean
name|force
parameter_list|)
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Iterate through all store files
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
operator|!
name|force
operator|&&
operator|(
name|storeSize
operator|<
name|this
operator|.
name|desiredMaxFileSize
operator|)
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
condition|)
block|{
if|if
condition|(
name|force
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Cannot split meta regions in HBase 0.20"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|// Not splitable if we find a reference store file present in the store.
name|boolean
name|splitable
init|=
literal|true
decl_stmt|;
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|StoreFile
name|largestSf
init|=
literal|null
decl_stmt|;
for|for
control|(
name|StoreFile
name|sf
range|:
name|storefiles
control|)
block|{
if|if
condition|(
name|splitable
condition|)
block|{
name|splitable
operator|=
operator|!
name|sf
operator|.
name|isReference
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|splitable
condition|)
block|{
comment|// RETURN IN MIDDLE OF FUNCTION!!! If not splitable, just return.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|sf
operator|+
literal|" is not splittable"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
block|}
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|sf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|size
init|=
name|r
operator|.
name|length
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|largestSf
operator|=
name|sf
expr_stmt|;
block|}
block|}
name|HFile
operator|.
name|Reader
name|r
init|=
name|largestSf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|largestSf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// Get first, last, and mid keys.  Midkey is the key that starts block
comment|// in middle of hfile.  Has column and timestamp.  Need to return just
comment|// the row we want to split on as midkey.
name|byte
index|[]
name|midkey
init|=
name|r
operator|.
name|midkey
argument_list|()
decl_stmt|;
if|if
condition|(
name|midkey
operator|!=
literal|null
condition|)
block|{
name|KeyValue
name|mk
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|midkey
argument_list|,
literal|0
argument_list|,
name|midkey
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
comment|// if the midkey is the same as the first and last keys, then we cannot
comment|// (ever) split this region.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|firstKey
argument_list|)
operator|==
literal|0
operator|&&
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|lastKey
argument_list|)
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"cannot split because midkey is the same as first or "
operator|+
literal|"last row"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
return|return
operator|new
name|StoreSize
argument_list|(
name|maxSize
argument_list|,
name|mk
operator|.
name|getRow
argument_list|()
argument_list|)
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed getting store size for "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** @return aggregate size of HStore */
specifier|public
name|long
name|getSize
parameter_list|()
block|{
return|return
name|storeSize
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return a scanner for both the memstore and the HStore files    * @throws IOException    */
specifier|protected
name|KeyValueScanner
name|getScanner
parameter_list|(
name|Scan
name|scan
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|targetCols
parameter_list|)
throws|throws
name|IOException
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|targetCols
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|this
operator|.
name|storeNameStr
return|;
block|}
comment|/**    * @return Count of store files    */
name|int
name|getStorefilesCount
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/**    * @return The size of the store files, in bytes.    */
name|long
name|getStorefilesSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * @return The size of the store file indexes, in bytes.    */
name|long
name|getStorefilesIndexSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|indexSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * Datastructure that holds size and row to split a file around.    * TODO: Take a KeyValue rather than row.    */
specifier|static
class|class
name|StoreSize
block|{
specifier|private
specifier|final
name|long
name|size
decl_stmt|;
specifier|private
specifier|final
name|byte
index|[]
name|row
decl_stmt|;
name|StoreSize
parameter_list|(
name|long
name|size
parameter_list|,
name|byte
index|[]
name|row
parameter_list|)
block|{
name|this
operator|.
name|size
operator|=
name|size
expr_stmt|;
name|this
operator|.
name|row
operator|=
name|row
expr_stmt|;
block|}
comment|/* @return the size */
name|long
name|getSize
parameter_list|()
block|{
return|return
name|size
return|;
block|}
name|byte
index|[]
name|getSplitRow
parameter_list|()
block|{
return|return
name|this
operator|.
name|row
return|;
block|}
block|}
name|HRegion
name|getHRegion
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
return|;
block|}
name|HRegionInfo
name|getHRegionInfo
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
operator|.
name|regionInfo
return|;
block|}
comment|/**    * Convenience method that implements the old MapFile.getClosest on top of    * HFile Scanners.  getClosest used seek to the asked-for key or just after    * (HFile seeks to the key or just before).    * @param s Scanner to use    * @param kv Key to find.    * @return True if we were able to seek the scanner to<code>b</code> or to    * the key just after.    * @throws IOException    */
specifier|static
name|boolean
name|getClosest
parameter_list|(
specifier|final
name|HFileScanner
name|s
parameter_list|,
specifier|final
name|KeyValue
name|kv
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Pass offsets to key content of a KeyValue; thats whats in the hfile index.
name|int
name|result
init|=
name|s
operator|.
name|seekTo
argument_list|(
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|result
operator|<
literal|0
condition|)
block|{
comment|// Not in file.  Will the first key do?
if|if
condition|(
operator|!
name|s
operator|.
name|seekTo
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
elseif|else
if|if
condition|(
name|result
operator|>
literal|0
condition|)
block|{
comment|// Less than what was asked for but maybe< because we're asking for
comment|// r/c/HConstants.LATEST_TIMESTAMP -- what was returned was r/c-1/SOME_TS...
comment|// A next will get us a r/c/SOME_TS.
if|if
condition|(
operator|!
name|s
operator|.
name|next
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Retrieve results from this store given the specified Get parameters.    * @param get Get operation    * @param columns List of columns to match, can be empty (not null)    * @param result List to add results to    * @throws IOException    */
specifier|public
name|void
name|get
parameter_list|(
name|Get
name|get
parameter_list|,
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|KeyValue
argument_list|>
name|result
parameter_list|)
throws|throws
name|IOException
block|{
name|KeyComparator
name|keyComparator
init|=
name|this
operator|.
name|comparator
operator|.
name|getRawComparator
argument_list|()
decl_stmt|;
comment|// Column matching and version enforcement
name|QueryMatcher
name|matcher
init|=
operator|new
name|QueryMatcher
argument_list|(
name|get
argument_list|,
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|,
name|columns
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|keyComparator
argument_list|,
name|versionsToReturn
argument_list|(
name|get
operator|.
name|getMaxVersions
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Read from memstore
if|if
condition|(
name|this
operator|.
name|memstore
operator|.
name|get
argument_list|(
name|matcher
argument_list|,
name|result
argument_list|)
condition|)
block|{
comment|// Received early-out from memstore
return|return;
block|}
comment|// Check if we even have storefiles
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return;
block|}
comment|// Get storefiles for this store
name|List
argument_list|<
name|HFileScanner
argument_list|>
name|storefileScanners
init|=
operator|new
name|ArrayList
argument_list|<
name|HFileScanner
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|StoreFile
name|sf
range|:
name|Iterables
operator|.
name|reverse
argument_list|(
name|this
operator|.
name|storefiles
argument_list|)
control|)
block|{
name|HFile
operator|.
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|sf
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|// Get a scanner that caches the block and uses pread
name|storefileScanners
operator|.
name|add
argument_list|(
name|r
operator|.
name|getScanner
argument_list|(
literal|true
argument_list|,
literal|true
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// StoreFileGetScan will handle reading this store's storefiles
name|StoreFileGetScan
name|scanner
init|=
operator|new
name|StoreFileGetScan
argument_list|(
name|storefileScanners
argument_list|,
name|matcher
argument_list|)
decl_stmt|;
comment|// Run a GET scan and put results into the specified list
name|scanner
operator|.
name|get
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Increments the value for the given row/family/qualifier.    *    * This function will always be seen as atomic by other readers    * because it only puts a single KV to memstore. Thus no    * read/write control necessary.    *    * @param row    * @param f    * @param qualifier    * @param newValue the new value to set into memstore    * @return memstore size delta    * @throws IOException    */
specifier|public
name|long
name|updateColumnValue
parameter_list|(
name|byte
index|[]
name|row
parameter_list|,
name|byte
index|[]
name|f
parameter_list|,
name|byte
index|[]
name|qualifier
parameter_list|,
name|long
name|newValue
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|KeyValue
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
name|KeyComparator
name|keyComparator
init|=
name|this
operator|.
name|comparator
operator|.
name|getRawComparator
argument_list|()
decl_stmt|;
name|KeyValue
name|kv
init|=
literal|null
decl_stmt|;
comment|// Setting up the QueryMatcher
name|Get
name|get
init|=
operator|new
name|Get
argument_list|(
name|row
argument_list|)
decl_stmt|;
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|qualifiers
init|=
operator|new
name|TreeSet
argument_list|<
name|byte
index|[]
argument_list|>
argument_list|(
name|Bytes
operator|.
name|BYTES_COMPARATOR
argument_list|)
decl_stmt|;
name|qualifiers
operator|.
name|add
argument_list|(
name|qualifier
argument_list|)
expr_stmt|;
name|QueryMatcher
name|matcher
init|=
operator|new
name|QueryMatcher
argument_list|(
name|get
argument_list|,
name|f
argument_list|,
name|qualifiers
argument_list|,
name|this
operator|.
name|ttl
argument_list|,
name|keyComparator
argument_list|,
literal|1
argument_list|)
decl_stmt|;
comment|// lock memstore snapshot for this critical section:
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
name|memstore
operator|.
name|readLockLock
argument_list|()
expr_stmt|;
try|try
block|{
name|int
name|memstoreCode
init|=
name|this
operator|.
name|memstore
operator|.
name|getWithCode
argument_list|(
name|matcher
argument_list|,
name|result
argument_list|)
decl_stmt|;
if|if
condition|(
name|memstoreCode
operator|!=
literal|0
condition|)
block|{
comment|// was in memstore (or snapshot)
name|kv
operator|=
name|result
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|clone
argument_list|()
expr_stmt|;
name|byte
index|[]
name|buffer
init|=
name|kv
operator|.
name|getBuffer
argument_list|()
decl_stmt|;
name|int
name|valueOffset
init|=
name|kv
operator|.
name|getValueOffset
argument_list|()
decl_stmt|;
name|Bytes
operator|.
name|putBytes
argument_list|(
name|buffer
argument_list|,
name|valueOffset
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|newValue
argument_list|)
argument_list|,
literal|0
argument_list|,
name|Bytes
operator|.
name|SIZEOF_LONG
argument_list|)
expr_stmt|;
if|if
condition|(
name|memstoreCode
operator|==
literal|2
condition|)
block|{
comment|// from snapshot, assign new TS
name|long
name|currTs
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
if|if
condition|(
name|currTs
operator|==
name|kv
operator|.
name|getTimestamp
argument_list|()
condition|)
block|{
name|currTs
operator|++
expr_stmt|;
comment|// unlikely but catastrophic
block|}
name|Bytes
operator|.
name|putBytes
argument_list|(
name|buffer
argument_list|,
name|kv
operator|.
name|getTimestampOffset
argument_list|()
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|currTs
argument_list|)
argument_list|,
literal|0
argument_list|,
name|Bytes
operator|.
name|SIZEOF_LONG
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|kv
operator|=
operator|new
name|KeyValue
argument_list|(
name|row
argument_list|,
name|f
argument_list|,
name|qualifier
argument_list|,
name|System
operator|.
name|currentTimeMillis
argument_list|()
argument_list|,
name|Bytes
operator|.
name|toBytes
argument_list|(
name|newValue
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|add
argument_list|(
name|kv
argument_list|)
return|;
comment|// end lock
block|}
finally|finally
block|{
name|memstore
operator|.
name|readLockUnlock
argument_list|()
expr_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|public
name|StoreFlusher
name|getStoreFlusher
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
return|return
operator|new
name|StoreFlusherImpl
argument_list|(
name|cacheFlushId
argument_list|)
return|;
block|}
specifier|private
class|class
name|StoreFlusherImpl
implements|implements
name|StoreFlusher
block|{
specifier|private
name|long
name|cacheFlushId
decl_stmt|;
specifier|private
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
decl_stmt|;
specifier|private
name|StoreFile
name|storeFile
decl_stmt|;
specifier|private
name|StoreFlusherImpl
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
name|this
operator|.
name|cacheFlushId
operator|=
name|cacheFlushId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|prepare
parameter_list|()
block|{
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
name|this
operator|.
name|snapshot
operator|=
name|memstore
operator|.
name|getSnapshot
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|flushCache
parameter_list|()
throws|throws
name|IOException
block|{
name|storeFile
operator|=
name|Store
operator|.
name|this
operator|.
name|flushCache
argument_list|(
name|cacheFlushId
argument_list|,
name|snapshot
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|commit
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|storeFile
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Add new file to store files.  Clear snapshot too while we have
comment|// the Store write lock.
return|return
name|Store
operator|.
name|this
operator|.
name|updateStorefiles
argument_list|(
name|storeFile
argument_list|,
name|snapshot
argument_list|)
return|;
block|}
block|}
comment|/**    * See if there's too much store files in this store    * @return true if number of store files is greater than    *  the number defined in compactionThreshold    */
specifier|public
name|boolean
name|hasTooManyStoreFiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|>
name|this
operator|.
name|compactionThreshold
return|;
block|}
specifier|public
specifier|static
specifier|final
name|long
name|FIXED_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|ClassSize
operator|.
name|OBJECT
operator|+
operator|(
literal|15
operator|*
name|ClassSize
operator|.
name|REFERENCE
operator|)
operator|+
operator|(
literal|4
operator|*
name|Bytes
operator|.
name|SIZEOF_LONG
operator|)
operator|+
operator|(
literal|3
operator|*
name|Bytes
operator|.
name|SIZEOF_INT
operator|)
operator|+
operator|(
name|Bytes
operator|.
name|SIZEOF_BOOLEAN
operator|*
literal|2
operator|)
argument_list|)
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|long
name|DEEP_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|FIXED_OVERHEAD
operator|+
name|ClassSize
operator|.
name|OBJECT
operator|+
name|ClassSize
operator|.
name|REENTRANT_LOCK
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP_ENTRY
operator|+
name|ClassSize
operator|.
name|OBJECT
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|long
name|heapSize
parameter_list|()
block|{
return|return
name|DEEP_OVERHEAD
operator|+
name|this
operator|.
name|memstore
operator|.
name|heapSize
argument_list|()
return|;
block|}
block|}
end_class

end_unit

