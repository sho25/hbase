begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Copyright 2010 The Apache Software Foundation  *  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InterruptedIOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|SortedSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CopyOnWriteArraySet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|locks
operator|.
name|ReentrantReadWriteLock
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|HRegionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|KeyValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|RemoteExceptionHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|client
operator|.
name|Scan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|HeapSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|CacheConfig
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|Compression
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|io
operator|.
name|hfile
operator|.
name|HFileScanner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|monitoring
operator|.
name|MonitoredTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|compactions
operator|.
name|CompactionProgress
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|regionserver
operator|.
name|compactions
operator|.
name|CompactionRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|Bytes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|ClassSize
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|CollectionBackedScanner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hbase
operator|.
name|util
operator|.
name|EnvironmentEdgeManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|ImmutableList
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_comment
comment|/**  * A Store holds a column family in a Region.  Its a memstore and a set of zero  * or more StoreFiles, which stretch backwards over time.  *  *<p>There's no reason to consider append-logging at this level; all logging  * and locking is handled at the HRegion level.  Store just provides  * services to manage sets of StoreFiles.  One of the most important of those  * services is compaction services where files are aggregated once they pass  * a configurable threshold.  *  *<p>The only thing having to do with logs that Store needs to deal with is  * the reconstructionLog.  This is a segment of an HRegion's log that might  * NOT be present upon startup.  If the param is NULL, there's nothing to do.  * If the param is non-NULL, we need to process the log to reconstruct  * a TreeMap that might not have been written to disk before the process  * died.  *  *<p>It's assumed that after this constructor returns, the reconstructionLog  * file will be deleted (by whoever has instantiated the Store).  *  *<p>Locking and transactions are handled at a higher level.  This API should  * not be called directly but by an HRegion manager.  */
end_comment

begin_class
specifier|public
class|class
name|Store
implements|implements
name|HeapSize
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|Store
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|protected
specifier|final
name|MemStore
name|memstore
decl_stmt|;
comment|// This stores directory in the filesystem.
specifier|private
specifier|final
name|Path
name|homedir
decl_stmt|;
specifier|private
specifier|final
name|HRegion
name|region
decl_stmt|;
specifier|private
specifier|final
name|HColumnDescriptor
name|family
decl_stmt|;
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|final
name|CacheConfig
name|cacheConf
decl_stmt|;
comment|// ttl in milliseconds.
specifier|protected
name|long
name|ttl
decl_stmt|;
specifier|protected
name|int
name|minVersions
decl_stmt|;
specifier|protected
name|int
name|maxVersions
decl_stmt|;
name|long
name|majorCompactionTime
decl_stmt|;
specifier|private
specifier|final
name|int
name|minFilesToCompact
decl_stmt|;
specifier|private
specifier|final
name|int
name|maxFilesToCompact
decl_stmt|;
specifier|private
specifier|final
name|long
name|minCompactSize
decl_stmt|;
specifier|private
specifier|final
name|long
name|maxCompactSize
decl_stmt|;
comment|// compactRatio: double on purpose!  Float.MAX< Long.MAX< Double.MAX
comment|// With float, java will downcast your long to float for comparisons (bad)
specifier|private
name|double
name|compactRatio
decl_stmt|;
specifier|private
name|long
name|lastCompactSize
init|=
literal|0
decl_stmt|;
specifier|volatile
name|boolean
name|forceMajor
init|=
literal|false
decl_stmt|;
comment|/* how many bytes to write between status checks */
specifier|static
name|int
name|closeCheckInterval
init|=
literal|0
decl_stmt|;
specifier|private
specifier|final
name|int
name|blockingStoreFileCount
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|storeSize
init|=
literal|0L
decl_stmt|;
specifier|private
specifier|volatile
name|long
name|totalUncompressedBytes
init|=
literal|0L
decl_stmt|;
specifier|private
specifier|final
name|Object
name|flushLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
specifier|final
name|ReentrantReadWriteLock
name|lock
init|=
operator|new
name|ReentrantReadWriteLock
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|String
name|storeNameStr
decl_stmt|;
specifier|private
name|CompactionProgress
name|progress
decl_stmt|;
specifier|private
specifier|final
name|int
name|compactionKVMax
decl_stmt|;
comment|/*    * List of store files inside this store. This is an immutable list that    * is atomically replaced when its contents change.    */
specifier|private
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|storefiles
init|=
literal|null
decl_stmt|;
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesCompacting
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
comment|// All access must be synchronized.
specifier|private
specifier|final
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
name|changedReaderObservers
init|=
operator|new
name|CopyOnWriteArraySet
argument_list|<
name|ChangedReadersObserver
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|int
name|blocksize
decl_stmt|;
comment|/** Compression algorithm for flush files and minor compaction */
specifier|private
specifier|final
name|Compression
operator|.
name|Algorithm
name|compression
decl_stmt|;
comment|/** Compression algorithm for major compaction */
specifier|private
specifier|final
name|Compression
operator|.
name|Algorithm
name|compactionCompression
decl_stmt|;
comment|// Comparing KeyValues
specifier|final
name|KeyValue
operator|.
name|KVComparator
name|comparator
decl_stmt|;
comment|/**    * Constructor    * @param basedir qualified path under which the region directory lives;    * generally the table subdirectory    * @param region    * @param family HColumnDescriptor for this column    * @param fs file system object    * @param conf configuration object    * failed.  Can be null.    * @throws IOException    */
specifier|protected
name|Store
parameter_list|(
name|Path
name|basedir
parameter_list|,
name|HRegion
name|region
parameter_list|,
name|HColumnDescriptor
name|family
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|HRegionInfo
name|info
init|=
name|region
operator|.
name|regionInfo
decl_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|homedir
operator|=
name|getStoreHomedir
argument_list|(
name|basedir
argument_list|,
name|info
operator|.
name|getEncodedName
argument_list|()
argument_list|,
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|exists
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|fs
operator|.
name|mkdirs
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
condition|)
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Failed create of: "
operator|+
name|this
operator|.
name|homedir
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
name|this
operator|.
name|region
operator|=
name|region
expr_stmt|;
name|this
operator|.
name|family
operator|=
name|family
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|blocksize
operator|=
name|family
operator|.
name|getBlocksize
argument_list|()
expr_stmt|;
name|this
operator|.
name|compression
operator|=
name|family
operator|.
name|getCompression
argument_list|()
expr_stmt|;
comment|// avoid overriding compression setting for major compactions if the user
comment|// has not specified it separately
name|this
operator|.
name|compactionCompression
operator|=
operator|(
name|family
operator|.
name|getCompactionCompression
argument_list|()
operator|!=
name|Compression
operator|.
name|Algorithm
operator|.
name|NONE
operator|)
condition|?
name|family
operator|.
name|getCompactionCompression
argument_list|()
else|:
name|this
operator|.
name|compression
expr_stmt|;
name|this
operator|.
name|comparator
operator|=
name|info
operator|.
name|getComparator
argument_list|()
expr_stmt|;
comment|// getTimeToLive returns ttl in seconds.  Convert to milliseconds.
name|this
operator|.
name|ttl
operator|=
name|family
operator|.
name|getTimeToLive
argument_list|()
expr_stmt|;
if|if
condition|(
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
condition|)
block|{
comment|// default is unlimited ttl.
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|ttl
operator|==
operator|-
literal|1
condition|)
block|{
name|ttl
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
block|}
else|else
block|{
comment|// second -> ms adjust for user data
name|this
operator|.
name|ttl
operator|*=
literal|1000
expr_stmt|;
block|}
name|this
operator|.
name|minVersions
operator|=
name|family
operator|.
name|getMinVersions
argument_list|()
expr_stmt|;
name|this
operator|.
name|maxVersions
operator|=
name|family
operator|.
name|getMaxVersions
argument_list|()
expr_stmt|;
name|this
operator|.
name|memstore
operator|=
operator|new
name|MemStore
argument_list|(
name|conf
argument_list|,
name|this
operator|.
name|comparator
argument_list|)
expr_stmt|;
name|this
operator|.
name|storeNameStr
operator|=
name|Bytes
operator|.
name|toString
argument_list|(
name|this
operator|.
name|family
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
comment|// By default, compact if storefile.count>= minFilesToCompact
name|this
operator|.
name|minFilesToCompact
operator|=
name|Math
operator|.
name|max
argument_list|(
literal|2
argument_list|,
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.min"
argument_list|,
comment|/*old name*/
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compactionThreshold"
argument_list|,
literal|3
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
comment|// Setting up cache configuration for this family
name|this
operator|.
name|cacheConf
operator|=
operator|new
name|CacheConfig
argument_list|(
name|conf
argument_list|,
name|family
argument_list|)
expr_stmt|;
name|this
operator|.
name|blockingStoreFileCount
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.blockingStoreFiles"
argument_list|,
literal|7
argument_list|)
expr_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
name|getNextMajorCompactTime
argument_list|()
expr_stmt|;
name|this
operator|.
name|maxFilesToCompact
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.max"
argument_list|,
literal|10
argument_list|)
expr_stmt|;
name|this
operator|.
name|minCompactSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hstore.compaction.min.size"
argument_list|,
name|this
operator|.
name|region
operator|.
name|memstoreFlushSize
argument_list|)
expr_stmt|;
name|this
operator|.
name|maxCompactSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"hbase.hstore.compaction.max.size"
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
expr_stmt|;
name|this
operator|.
name|compactRatio
operator|=
name|conf
operator|.
name|getFloat
argument_list|(
literal|"hbase.hstore.compaction.ratio"
argument_list|,
literal|1.2F
argument_list|)
expr_stmt|;
name|this
operator|.
name|compactionKVMax
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.compaction.kv.max"
argument_list|,
literal|10
argument_list|)
expr_stmt|;
if|if
condition|(
name|Store
operator|.
name|closeCheckInterval
operator|==
literal|0
condition|)
block|{
name|Store
operator|.
name|closeCheckInterval
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"hbase.hstore.close.check.interval"
argument_list|,
literal|10
operator|*
literal|1000
operator|*
literal|1000
comment|/* 10 MB */
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|loadStoreFiles
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|public
name|HColumnDescriptor
name|getFamily
parameter_list|()
block|{
return|return
name|this
operator|.
name|family
return|;
block|}
comment|/**    * @return The maximum sequence id in all store files.    */
name|long
name|getMaxSequenceId
parameter_list|()
block|{
return|return
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|this
operator|.
name|getStorefiles
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * @param tabledir    * @param encodedName Encoded region name.    * @param family    * @return Path to family/Store home directory.    */
specifier|public
specifier|static
name|Path
name|getStoreHomedir
parameter_list|(
specifier|final
name|Path
name|tabledir
parameter_list|,
specifier|final
name|String
name|encodedName
parameter_list|,
specifier|final
name|byte
index|[]
name|family
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|tabledir
argument_list|,
operator|new
name|Path
argument_list|(
name|encodedName
argument_list|,
operator|new
name|Path
argument_list|(
name|Bytes
operator|.
name|toString
argument_list|(
name|family
argument_list|)
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Return the directory in which this store stores its    * StoreFiles    */
specifier|public
name|Path
name|getHomedir
parameter_list|()
block|{
return|return
name|homedir
return|;
block|}
comment|/*    * Creates an unsorted list of StoreFile loaded from the given directory.    * @throws IOException    */
specifier|private
name|List
argument_list|<
name|StoreFile
argument_list|>
name|loadStoreFiles
parameter_list|()
throws|throws
name|IOException
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|()
decl_stmt|;
name|FileStatus
name|files
index|[]
init|=
name|this
operator|.
name|fs
operator|.
name|listStatus
argument_list|(
name|this
operator|.
name|homedir
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|files
operator|!=
literal|null
operator|&&
name|i
operator|<
name|files
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
comment|// Skip directories.
if|if
condition|(
name|files
index|[
name|i
index|]
operator|.
name|isDir
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|Path
name|p
init|=
name|files
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Check for empty file.  Should never be the case but can happen
comment|// after data loss in hdfs for whatever reason (upgrade, etc.): HBASE-646
if|if
condition|(
name|this
operator|.
name|fs
operator|.
name|getFileStatus
argument_list|(
name|p
argument_list|)
operator|.
name|getLen
argument_list|()
operator|<=
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Skipping "
operator|+
name|p
operator|+
literal|" because its empty. HBASE-646 DATA LOSS?"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|StoreFile
name|curfile
init|=
literal|null
decl_stmt|;
try|try
block|{
name|curfile
operator|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|p
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|)
expr_stmt|;
name|curfile
operator|.
name|createReader
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed open of "
operator|+
name|p
operator|+
literal|"; presumption is that file was "
operator|+
literal|"corrupted at flush and lost edits picked up by commit log replay. "
operator|+
literal|"Verify!"
argument_list|,
name|ioe
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|length
init|=
name|curfile
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|length
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|+=
name|curfile
operator|.
name|getReader
argument_list|()
operator|.
name|getTotalUncompressedBytes
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"loaded "
operator|+
name|curfile
operator|.
name|toStringDetailed
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|results
operator|.
name|add
argument_list|(
name|curfile
argument_list|)
expr_stmt|;
block|}
return|return
name|results
return|;
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|add
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|add
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Adds a value to the memstore    *    * @param kv    * @return memstore size delta    */
specifier|protected
name|long
name|delete
parameter_list|(
specifier|final
name|KeyValue
name|kv
parameter_list|)
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
name|this
operator|.
name|memstore
operator|.
name|delete
argument_list|(
name|kv
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return All store files.    */
name|List
argument_list|<
name|StoreFile
argument_list|>
name|getStorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
return|;
block|}
specifier|public
name|void
name|bulkLoadHFile
parameter_list|(
name|String
name|srcPathStr
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|srcPath
init|=
operator|new
name|Path
argument_list|(
name|srcPathStr
argument_list|)
decl_stmt|;
name|HFile
operator|.
name|Reader
name|reader
init|=
literal|null
decl_stmt|;
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Validating hfile at "
operator|+
name|srcPath
operator|+
literal|" for inclusion in "
operator|+
literal|"store "
operator|+
name|this
operator|+
literal|" region "
operator|+
name|this
operator|.
name|region
argument_list|)
expr_stmt|;
name|reader
operator|=
name|HFile
operator|.
name|createReader
argument_list|(
name|srcPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|srcPath
argument_list|,
name|cacheConf
argument_list|)
expr_stmt|;
name|reader
operator|.
name|loadFileInfo
argument_list|()
expr_stmt|;
name|byte
index|[]
name|firstKey
init|=
name|reader
operator|.
name|getFirstRowKey
argument_list|()
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|reader
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|byte
index|[]
name|lastKey
init|=
operator|(
name|lk
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|)
operator|.
name|getRow
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"HFile bounds: first="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|firstKey
argument_list|)
operator|+
literal|" last="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|lastKey
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Region bounds: first="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|region
operator|.
name|getStartKey
argument_list|()
argument_list|)
operator|+
literal|" last="
operator|+
name|Bytes
operator|.
name|toStringBinary
argument_list|(
name|region
operator|.
name|getEndKey
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|HRegionInfo
name|hri
init|=
name|region
operator|.
name|getRegionInfo
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|hri
operator|.
name|containsRange
argument_list|(
name|firstKey
argument_list|,
name|lastKey
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|WrongRegionException
argument_list|(
literal|"Bulk load file "
operator|+
name|srcPathStr
operator|+
literal|" does not fit inside region "
operator|+
name|this
operator|.
name|region
argument_list|)
throw|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Move the file if it's on another filesystem
name|FileSystem
name|srcFs
init|=
name|srcPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|srcFs
operator|.
name|equals
argument_list|(
name|fs
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"File "
operator|+
name|srcPath
operator|+
literal|" on different filesystem than "
operator|+
literal|"destination store - moving to this filesystem."
argument_list|)
expr_stmt|;
name|Path
name|tmpPath
init|=
name|getTmpPath
argument_list|()
decl_stmt|;
name|FileUtil
operator|.
name|copy
argument_list|(
name|srcFs
argument_list|,
name|srcPath
argument_list|,
name|fs
argument_list|,
name|tmpPath
argument_list|,
literal|false
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Copied to temporary path on dst filesystem: "
operator|+
name|tmpPath
argument_list|)
expr_stmt|;
name|srcPath
operator|=
name|tmpPath
expr_stmt|;
block|}
name|Path
name|dstPath
init|=
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|homedir
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Renaming bulk load file "
operator|+
name|srcPath
operator|+
literal|" to "
operator|+
name|dstPath
argument_list|)
expr_stmt|;
name|StoreFile
operator|.
name|rename
argument_list|(
name|fs
argument_list|,
name|srcPath
argument_list|,
name|dstPath
argument_list|)
expr_stmt|;
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|fs
argument_list|,
name|dstPath
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|)
decl_stmt|;
name|sf
operator|.
name|createReader
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Moved hfile "
operator|+
name|srcPath
operator|+
literal|" into store directory "
operator|+
name|homedir
operator|+
literal|" - updating store file list."
argument_list|)
expr_stmt|;
comment|// Append the new storefile into the list
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newFiles
operator|.
name|add
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|this
operator|.
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|newFiles
argument_list|)
expr_stmt|;
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Successfully loaded store file "
operator|+
name|srcPath
operator|+
literal|" into store "
operator|+
name|this
operator|+
literal|" (new location: "
operator|+
name|dstPath
operator|+
literal|")"
argument_list|)
expr_stmt|;
block|}
comment|/**    * Get a temporary path in this region. These temporary files    * will get cleaned up when the region is re-opened if they are    * still around.    */
specifier|private
name|Path
name|getTmpPath
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|region
operator|.
name|getTmpDir
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Close all the readers    *    * We don't need to worry about subsequent requests because the HRegion holds    * a write lock that will prevent any more reads or writes.    *    * @throws IOException    */
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|result
init|=
name|storefiles
decl_stmt|;
comment|// Clear so metrics doesn't find them.
name|storefiles
operator|=
name|ImmutableList
operator|.
name|of
argument_list|()
expr_stmt|;
for|for
control|(
name|StoreFile
name|f
range|:
name|result
control|)
block|{
name|f
operator|.
name|closeReader
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"closed "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Snapshot this stores memstore.  Call before running    * {@link #flushCache(long, SortedSet<KeyValue>)} so it has some work to do.    */
name|void
name|snapshot
parameter_list|()
block|{
name|this
operator|.
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
block|}
comment|/**    * Write out current snapshot.  Presumes {@link #snapshot()} has been called    * previously.    * @param logCacheFlushId flush sequence number    * @param snapshot    * @param snapshotTimeRangeTracker    * @return true if a compaction is needed    * @throws IOException    */
specifier|private
name|StoreFile
name|flushCache
parameter_list|(
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
parameter_list|,
name|TimeRangeTracker
name|snapshotTimeRangeTracker
parameter_list|,
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If an exception happens flushing, we let it out without clearing
comment|// the memstore snapshot.  The old snapshot will be returned when we say
comment|// 'snapshot', the next time flush comes around.
return|return
name|internalFlushCache
argument_list|(
name|snapshot
argument_list|,
name|logCacheFlushId
argument_list|,
name|snapshotTimeRangeTracker
argument_list|,
name|status
argument_list|)
return|;
block|}
comment|/*    * @param cache    * @param logCacheFlushId    * @return StoreFile created.    * @throws IOException    */
specifier|private
name|StoreFile
name|internalFlushCache
parameter_list|(
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|,
specifier|final
name|long
name|logCacheFlushId
parameter_list|,
name|TimeRangeTracker
name|snapshotTimeRangeTracker
parameter_list|,
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
name|StoreFile
operator|.
name|Writer
name|writer
init|=
literal|null
decl_stmt|;
name|long
name|flushed
init|=
literal|0
decl_stmt|;
comment|// Don't flush if there are no entries.
if|if
condition|(
name|set
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Scan
name|scan
init|=
operator|new
name|Scan
argument_list|()
decl_stmt|;
name|scan
operator|.
name|setMaxVersions
argument_list|(
name|maxVersions
argument_list|)
expr_stmt|;
comment|// Use a store scanner to find which rows to flush.
comment|// Note that we need to retain deletes, hence
comment|// pass true as the StoreScanner's retainDeletesInOutput argument.
name|InternalScanner
name|scanner
init|=
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|Collections
operator|.
name|singletonList
argument_list|(
operator|new
name|CollectionBackedScanner
argument_list|(
name|set
argument_list|,
name|this
operator|.
name|comparator
argument_list|)
argument_list|)
argument_list|,
literal|true
argument_list|)
decl_stmt|;
try|try
block|{
comment|// TODO:  We can fail in the below block before we complete adding this
comment|// flush to list of store files.  Add cleanup of anything put on filesystem
comment|// if we fail.
synchronized|synchronized
init|(
name|flushLock
init|)
block|{
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": creating writer"
argument_list|)
expr_stmt|;
comment|// A. Write the map out to the disk
name|writer
operator|=
name|createWriterInTmp
argument_list|(
name|set
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|writer
operator|.
name|setTimeRangeTracker
argument_list|(
name|snapshotTimeRangeTracker
argument_list|)
expr_stmt|;
try|try
block|{
name|List
argument_list|<
name|KeyValue
argument_list|>
name|kvs
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|hasMore
decl_stmt|;
do|do
block|{
name|hasMore
operator|=
name|scanner
operator|.
name|next
argument_list|(
name|kvs
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|kvs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
for|for
control|(
name|KeyValue
name|kv
range|:
name|kvs
control|)
block|{
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
name|flushed
operator|+=
name|this
operator|.
name|memstore
operator|.
name|heapSizeChange
argument_list|(
name|kv
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|kvs
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
do|while
condition|(
name|hasMore
condition|)
do|;
block|}
finally|finally
block|{
comment|// Write out the log sequence number that corresponds to this output
comment|// hfile.  The hfile is current up to and including logCacheFlushId.
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": appending metadata"
argument_list|)
expr_stmt|;
name|writer
operator|.
name|appendMetadata
argument_list|(
name|logCacheFlushId
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": closing flushed file"
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
name|scanner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|// Write-out finished successfully, move into the right spot
name|Path
name|dstPath
init|=
name|StoreFile
operator|.
name|getUniqueFile
argument_list|(
name|fs
argument_list|,
name|homedir
argument_list|)
decl_stmt|;
name|String
name|msg
init|=
literal|"Renaming flushed file at "
operator|+
name|writer
operator|.
name|getPath
argument_list|()
operator|+
literal|" to "
operator|+
name|dstPath
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
name|msg
argument_list|)
expr_stmt|;
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": "
operator|+
name|msg
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|writer
operator|.
name|getPath
argument_list|()
argument_list|,
name|dstPath
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to rename "
operator|+
name|writer
operator|.
name|getPath
argument_list|()
operator|+
literal|" to "
operator|+
name|dstPath
argument_list|)
expr_stmt|;
block|}
name|status
operator|.
name|setStatus
argument_list|(
literal|"Flushing "
operator|+
name|this
operator|+
literal|": reopening flushed file"
argument_list|)
expr_stmt|;
name|StoreFile
name|sf
init|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|dstPath
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|)
decl_stmt|;
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|sf
operator|.
name|createReader
argument_list|()
decl_stmt|;
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|+=
name|r
operator|.
name|getTotalUncompressedBytes
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Added "
operator|+
name|sf
operator|+
literal|", entries="
operator|+
name|r
operator|.
name|getEntries
argument_list|()
operator|+
literal|", sequenceid="
operator|+
name|logCacheFlushId
operator|+
literal|", memsize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|flushed
argument_list|)
operator|+
literal|", filesize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|r
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|sf
return|;
block|}
comment|/*    * @param maxKeyCount    * @return Writer for a new StoreFile in the tmp dir.    */
specifier|private
name|StoreFile
operator|.
name|Writer
name|createWriterInTmp
parameter_list|(
name|int
name|maxKeyCount
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|createWriterInTmp
argument_list|(
name|maxKeyCount
argument_list|,
name|this
operator|.
name|compression
argument_list|)
return|;
block|}
comment|/*    * @param maxKeyCount    * @param compression Compression algorithm to use    * @return Writer for a new StoreFile in the tmp dir.    */
specifier|private
name|StoreFile
operator|.
name|Writer
name|createWriterInTmp
parameter_list|(
name|int
name|maxKeyCount
parameter_list|,
name|Compression
operator|.
name|Algorithm
name|compression
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|StoreFile
operator|.
name|createWriter
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|region
operator|.
name|getTmpDir
argument_list|()
argument_list|,
name|this
operator|.
name|blocksize
argument_list|,
name|compression
argument_list|,
name|this
operator|.
name|comparator
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|,
name|maxKeyCount
argument_list|)
return|;
block|}
comment|/*    * Change storefiles adding into place the Reader produced by this new flush.    * @param sf    * @param set That was used to make the passed file<code>p</code>.    * @throws IOException    * @return Whether compaction is required.    */
specifier|private
name|boolean
name|updateStorefiles
parameter_list|(
specifier|final
name|StoreFile
name|sf
parameter_list|,
specifier|final
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|set
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|sf
argument_list|)
expr_stmt|;
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|newList
argument_list|)
expr_stmt|;
name|this
operator|.
name|memstore
operator|.
name|clearSnapshot
argument_list|(
name|set
argument_list|)
expr_stmt|;
comment|// Tell listeners of the change in readers.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
return|return
name|needsCompaction
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Notify all observers that set of Readers has changed.    * @throws IOException    */
specifier|private
name|void
name|notifyChangedReadersObservers
parameter_list|()
throws|throws
name|IOException
block|{
for|for
control|(
name|ChangedReadersObserver
name|o
range|:
name|this
operator|.
name|changedReaderObservers
control|)
block|{
name|o
operator|.
name|updateReaders
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * @param o Observer who wants to know about changes in set of Readers    */
name|void
name|addChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
name|this
operator|.
name|changedReaderObservers
operator|.
name|add
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|/*    * @param o Observer no longer interested in changes in set of Readers.    */
name|void
name|deleteChangedReaderObserver
parameter_list|(
name|ChangedReadersObserver
name|o
parameter_list|)
block|{
comment|// We don't check if observer present; it may not be (legitimately)
name|this
operator|.
name|changedReaderObservers
operator|.
name|remove
argument_list|(
name|o
argument_list|)
expr_stmt|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// Compaction
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Compact the StoreFiles.  This method may take some time, so the calling    * thread must be able to block for long periods.    *    *<p>During this time, the Store can work as usual, getting values from    * StoreFiles and writing new StoreFiles from the memstore.    *    * Existing StoreFiles are not destroyed until the new compacted StoreFile is    * completely written-out to disk.    *    *<p>The compactLock prevents multiple simultaneous compactions.    * The structureLock prevents us from interfering with other write operations.    *    *<p>We don't want to hold the structureLock for the whole time, as a compact()    * can be lengthy and we want to allow cache-flushes during this period.    *    * @param CompactionRequest    *          compaction details obtained from requestCompaction()    * @throws IOException    */
name|void
name|compact
parameter_list|(
name|CompactionRequest
name|cr
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|cr
operator|==
literal|null
operator|||
name|cr
operator|.
name|getFiles
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return;
block|}
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|cr
operator|.
name|getStore
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|equals
argument_list|(
name|this
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
name|cr
operator|.
name|getFiles
argument_list|()
decl_stmt|;
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
comment|// sanity check: we're compacting files that this store knows about
comment|// TODO: change this to LOG.error() after more debugging
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|filesCompacting
operator|.
name|containsAll
argument_list|(
name|filesToCompact
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// Max-sequenceID is the last key in the files we're compacting
name|long
name|maxId
init|=
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|filesToCompact
argument_list|)
decl_stmt|;
comment|// Ready to go. Have list of files to compact.
name|LOG
operator|.
name|info
argument_list|(
literal|"Starting compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" of "
operator|+
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|getRegionNameAsString
argument_list|()
operator|+
literal|" into "
operator|+
name|region
operator|.
name|getTmpDir
argument_list|()
operator|+
literal|", seqid="
operator|+
name|maxId
operator|+
literal|", totalSize="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|cr
operator|.
name|getSize
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|StoreFile
name|sf
init|=
literal|null
decl_stmt|;
try|try
block|{
name|StoreFile
operator|.
name|Writer
name|writer
init|=
name|compactStore
argument_list|(
name|filesToCompact
argument_list|,
name|cr
operator|.
name|isMajor
argument_list|()
argument_list|,
name|maxId
argument_list|)
decl_stmt|;
comment|// Move the compaction into place.
name|sf
operator|=
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|writer
argument_list|)
expr_stmt|;
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|postCompact
argument_list|(
name|this
argument_list|,
name|sf
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Completed"
operator|+
operator|(
name|cr
operator|.
name|isMajor
argument_list|()
condition|?
literal|" major "
else|:
literal|" "
operator|)
operator|+
literal|"compaction of "
operator|+
name|filesToCompact
operator|.
name|size
argument_list|()
operator|+
literal|" file(s) in "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" of "
operator|+
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|getRegionNameAsString
argument_list|()
operator|+
literal|"; new storefile name="
operator|+
operator|(
name|sf
operator|==
literal|null
condition|?
literal|"none"
else|:
name|sf
operator|.
name|toString
argument_list|()
operator|)
operator|+
literal|", size="
operator|+
operator|(
name|sf
operator|==
literal|null
condition|?
literal|"none"
else|:
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
argument_list|)
operator|)
operator|+
literal|"; total size for store is "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|storeSize
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/*    * Compact the most recent N files. Essentially a hook for testing.    */
specifier|protected
name|void
name|compactRecent
parameter_list|(
name|int
name|N
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
decl_stmt|;
name|long
name|maxId
decl_stmt|;
name|boolean
name|isMajor
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesToCompact
operator|=
name|Lists
operator|.
name|newArrayList
argument_list|(
name|storefiles
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|filesCompacting
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// exclude all files older than the newest file we're currently
comment|// compacting. this allows us to preserve contiguity (HBASE-2856)
name|StoreFile
name|last
init|=
name|filesCompacting
operator|.
name|get
argument_list|(
name|filesCompacting
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|idx
init|=
name|filesToCompact
operator|.
name|indexOf
argument_list|(
name|last
argument_list|)
decl_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|idx
operator|!=
operator|-
literal|1
argument_list|)
expr_stmt|;
name|filesToCompact
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|idx
operator|+
literal|1
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
name|int
name|count
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
decl_stmt|;
if|if
condition|(
name|N
operator|>
name|count
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Not enough files"
argument_list|)
throw|;
block|}
name|filesToCompact
operator|=
name|filesToCompact
operator|.
name|subList
argument_list|(
name|count
operator|-
name|N
argument_list|,
name|count
argument_list|)
expr_stmt|;
name|maxId
operator|=
name|StoreFile
operator|.
name|getMaxSequenceIdInList
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
name|isMajor
operator|=
operator|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
name|storefiles
operator|.
name|size
argument_list|()
operator|)
expr_stmt|;
name|filesCompacting
operator|.
name|addAll
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
name|Collections
operator|.
name|sort
argument_list|(
name|filesCompacting
argument_list|,
name|StoreFile
operator|.
name|Comparators
operator|.
name|FLUSH_TIME
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
try|try
block|{
comment|// Ready to go. Have list of files to compact.
name|StoreFile
operator|.
name|Writer
name|writer
init|=
name|compactStore
argument_list|(
name|filesToCompact
argument_list|,
name|isMajor
argument_list|,
name|maxId
argument_list|)
decl_stmt|;
comment|// Move the compaction into place.
name|StoreFile
name|sf
init|=
name|completeCompaction
argument_list|(
name|filesToCompact
argument_list|,
name|writer
argument_list|)
decl_stmt|;
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|postCompact
argument_list|(
name|this
argument_list|,
name|sf
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|boolean
name|hasReferences
parameter_list|()
block|{
return|return
name|hasReferences
argument_list|(
name|this
operator|.
name|storefiles
argument_list|)
return|;
block|}
comment|/*    * @param files    * @return True if any of the files in<code>files</code> are References.    */
specifier|private
name|boolean
name|hasReferences
parameter_list|(
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|StoreFile
name|hsf
range|:
name|files
control|)
block|{
if|if
condition|(
name|hsf
operator|.
name|isReference
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/*    * Gets lowest timestamp from candidate StoreFiles    *    * @param fs    * @param dir    * @throws IOException    */
specifier|public
specifier|static
name|long
name|getLowestTimestamp
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|minTs
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|;
for|for
control|(
name|StoreFile
name|storeFile
range|:
name|candidates
control|)
block|{
name|minTs
operator|=
name|Math
operator|.
name|min
argument_list|(
name|minTs
argument_list|,
name|storeFile
operator|.
name|getModificationTimeStamp
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|minTs
return|;
block|}
comment|/** getter for CompactionProgress object    * @return CompactionProgress object    */
specifier|public
name|CompactionProgress
name|getCompactionProgress
parameter_list|()
block|{
return|return
name|this
operator|.
name|progress
return|;
block|}
comment|/*    * @return True if we should run a major compaction.    */
name|boolean
name|isMajorCompaction
parameter_list|()
throws|throws
name|IOException
block|{
for|for
control|(
name|StoreFile
name|sf
range|:
name|this
operator|.
name|storefiles
control|)
block|{
if|if
condition|(
name|sf
operator|.
name|getReader
argument_list|()
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"StoreFile "
operator|+
name|sf
operator|+
literal|" has null Reader"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
block|}
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|this
operator|.
name|storefiles
argument_list|)
decl_stmt|;
comment|// exclude files above the max compaction threshold
comment|// except: save all references. we MUST compact them
name|int
name|pos
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|<
name|candidates
operator|.
name|size
argument_list|()
operator|&&
name|candidates
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
operator|>
name|this
operator|.
name|maxCompactSize
operator|&&
operator|!
name|candidates
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|isReference
argument_list|()
condition|)
operator|++
name|pos
expr_stmt|;
name|candidates
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|pos
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
return|return
name|isMajorCompaction
argument_list|(
name|candidates
argument_list|)
return|;
block|}
comment|/*    * @param filesToCompact Files to compact. Can be null.    * @return True if we should run a major compaction.    */
specifier|private
name|boolean
name|isMajorCompaction
parameter_list|(
specifier|final
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|filesToCompact
operator|==
literal|null
operator|||
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
operator|||
name|majorCompactionTime
operator|==
literal|0
condition|)
block|{
return|return
name|result
return|;
block|}
comment|// TODO: Use better method for determining stamp of last major (HBASE-2990)
name|long
name|lowTimestamp
init|=
name|getLowestTimestamp
argument_list|(
name|filesToCompact
argument_list|)
decl_stmt|;
name|long
name|now
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
if|if
condition|(
name|lowTimestamp
operator|>
literal|0l
operator|&&
name|lowTimestamp
operator|<
operator|(
name|now
operator|-
name|this
operator|.
name|majorCompactionTime
operator|)
condition|)
block|{
comment|// Major compaction time has elapsed.
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
comment|// Single file
name|StoreFile
name|sf
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|long
name|oldest
init|=
operator|(
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|timeRangeTracker
operator|==
literal|null
operator|)
condition|?
name|Long
operator|.
name|MIN_VALUE
else|:
name|now
operator|-
name|sf
operator|.
name|getReader
argument_list|()
operator|.
name|timeRangeTracker
operator|.
name|minimumTimestamp
decl_stmt|;
if|if
condition|(
name|sf
operator|.
name|isMajorCompaction
argument_list|()
operator|&&
operator|(
name|this
operator|.
name|ttl
operator|==
name|HConstants
operator|.
name|FOREVER
operator|||
name|oldest
operator|<
name|this
operator|.
name|ttl
operator|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping major compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|" because one (major) compacted file only and oldestTime "
operator|+
name|oldest
operator|+
literal|"ms is< ttl="
operator|+
name|this
operator|.
name|ttl
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|this
operator|.
name|ttl
operator|!=
name|HConstants
operator|.
name|FOREVER
operator|&&
name|oldest
operator|>
name|this
operator|.
name|ttl
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Major compaction triggered on store "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|", because keyvalues outdated; time since last major compaction "
operator|+
operator|(
name|now
operator|-
name|lowTimestamp
operator|)
operator|+
literal|"ms"
argument_list|)
expr_stmt|;
name|result
operator|=
literal|true
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Major compaction triggered on store "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|"; time since last major compaction "
operator|+
operator|(
name|now
operator|-
name|lowTimestamp
operator|)
operator|+
literal|"ms"
argument_list|)
expr_stmt|;
block|}
name|result
operator|=
literal|true
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
name|long
name|getNextMajorCompactTime
parameter_list|()
block|{
comment|// default = 24hrs
name|long
name|ret
init|=
name|conf
operator|.
name|getLong
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|,
literal|1000
operator|*
literal|60
operator|*
literal|60
operator|*
literal|24
argument_list|)
decl_stmt|;
if|if
condition|(
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|String
name|strCompactionTime
init|=
name|family
operator|.
name|getValue
argument_list|(
name|HConstants
operator|.
name|MAJOR_COMPACTION_PERIOD
argument_list|)
decl_stmt|;
name|ret
operator|=
operator|(
operator|new
name|Long
argument_list|(
name|strCompactionTime
argument_list|)
operator|)
operator|.
name|longValue
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|ret
operator|>
literal|0
condition|)
block|{
comment|// default = 20% = +/- 4.8 hrs
name|double
name|jitterPct
init|=
name|conf
operator|.
name|getFloat
argument_list|(
literal|"hbase.hregion.majorcompaction.jitter"
argument_list|,
literal|0.20F
argument_list|)
decl_stmt|;
if|if
condition|(
name|jitterPct
operator|>
literal|0
condition|)
block|{
name|long
name|jitter
init|=
name|Math
operator|.
name|round
argument_list|(
name|ret
operator|*
name|jitterPct
argument_list|)
decl_stmt|;
name|ret
operator|+=
name|jitter
operator|-
name|Math
operator|.
name|round
argument_list|(
literal|2L
operator|*
name|jitter
operator|*
name|Math
operator|.
name|random
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|ret
return|;
block|}
specifier|public
name|CompactionRequest
name|requestCompaction
parameter_list|()
block|{
comment|// don't even select for compaction if writes are disabled
if|if
condition|(
operator|!
name|this
operator|.
name|region
operator|.
name|areWritesEnabled
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
name|CompactionRequest
name|ret
init|=
literal|null
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
comment|// candidates = all storefiles not already in compaction queue
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
init|=
name|Lists
operator|.
name|newArrayList
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|filesCompacting
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// exclude all files older than the newest file we're currently
comment|// compacting. this allows us to preserve contiguity (HBASE-2856)
name|StoreFile
name|last
init|=
name|filesCompacting
operator|.
name|get
argument_list|(
name|filesCompacting
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|idx
init|=
name|candidates
operator|.
name|indexOf
argument_list|(
name|last
argument_list|)
decl_stmt|;
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|idx
operator|!=
operator|-
literal|1
argument_list|)
expr_stmt|;
name|candidates
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|idx
operator|+
literal|1
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
name|boolean
name|override
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|override
operator|=
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|preCompactSelection
argument_list|(
name|this
argument_list|,
name|candidates
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
decl_stmt|;
if|if
condition|(
name|override
condition|)
block|{
comment|// coprocessor is overriding normal file selection
name|filesToCompact
operator|=
name|candidates
expr_stmt|;
block|}
else|else
block|{
name|filesToCompact
operator|=
name|compactSelection
argument_list|(
name|candidates
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|postCompactSelection
argument_list|(
name|this
argument_list|,
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|filesToCompact
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// no files to compact
if|if
condition|(
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
comment|// basic sanity check: do not try to compact the same StoreFile twice.
if|if
condition|(
operator|!
name|Collections
operator|.
name|disjoint
argument_list|(
name|filesCompacting
argument_list|,
name|filesToCompact
argument_list|)
condition|)
block|{
comment|// TODO: change this from an IAE to LOG.error after sufficient testing
name|Preconditions
operator|.
name|checkArgument
argument_list|(
literal|false
argument_list|,
literal|"%s overlaps with %s"
argument_list|,
name|filesToCompact
argument_list|,
name|filesCompacting
argument_list|)
expr_stmt|;
block|}
name|filesCompacting
operator|.
name|addAll
argument_list|(
name|filesToCompact
argument_list|)
expr_stmt|;
name|Collections
operator|.
name|sort
argument_list|(
name|filesCompacting
argument_list|,
name|StoreFile
operator|.
name|Comparators
operator|.
name|FLUSH_TIME
argument_list|)
expr_stmt|;
comment|// major compaction iff all StoreFiles are included
name|boolean
name|isMajor
init|=
operator|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|==
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
operator|)
decl_stmt|;
if|if
condition|(
name|isMajor
condition|)
block|{
comment|// since we're enqueuing a major, update the compaction wait interval
name|this
operator|.
name|forceMajor
operator|=
literal|false
expr_stmt|;
name|this
operator|.
name|majorCompactionTime
operator|=
name|getNextMajorCompactTime
argument_list|()
expr_stmt|;
block|}
comment|// everything went better than expected. create a compaction request
name|int
name|pri
init|=
name|getCompactPriority
argument_list|()
decl_stmt|;
name|ret
operator|=
operator|new
name|CompactionRequest
argument_list|(
name|region
argument_list|,
name|this
argument_list|,
name|filesToCompact
argument_list|,
name|isMajor
argument_list|,
name|pri
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Compaction Request failed for region "
operator|+
name|region
operator|+
literal|", store "
operator|+
name|this
argument_list|,
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|ex
argument_list|)
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
name|ret
return|;
block|}
specifier|public
name|void
name|finishRequest
parameter_list|(
name|CompactionRequest
name|cr
parameter_list|)
block|{
synchronized|synchronized
init|(
name|filesCompacting
init|)
block|{
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|cr
operator|.
name|getFiles
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Algorithm to choose which files to compact    *    * Configuration knobs:    *  "hbase.hstore.compaction.ratio"    *    normal case: minor compact when file<= sum(smaller_files) * ratio    *  "hbase.hstore.compaction.min.size"    *    unconditionally compact individual files below this size    *  "hbase.hstore.compaction.max.size"    *    never compact individual files above this size (unless splitting)    *  "hbase.hstore.compaction.min"    *    min files needed to minor compact    *  "hbase.hstore.compaction.max"    *    max files to compact at once (avoids OOM)    *    * @param candidates candidate files, ordered from oldest to newest    * @return subset copy of candidate list that meets compaction criteria    * @throws IOException    */
name|List
argument_list|<
name|StoreFile
argument_list|>
name|compactSelection
parameter_list|(
name|List
argument_list|<
name|StoreFile
argument_list|>
name|candidates
parameter_list|)
throws|throws
name|IOException
block|{
comment|// ASSUMPTION!!! filesCompacting is locked when calling this function
comment|/* normal skew:      *      *         older ----> newer      *     _      *    | |   _      *    | |  | |   _      *  --|-|- |-|- |-|---_-------_-------  minCompactSize      *    | |  | |  | |  | |  _  | |      *    | |  | |  | |  | | | | | |      *    | |  | |  | |  | | | | | |      */
name|List
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
init|=
operator|new
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
argument_list|(
name|candidates
argument_list|)
decl_stmt|;
name|boolean
name|forcemajor
init|=
name|this
operator|.
name|forceMajor
operator|&&
name|filesCompacting
operator|.
name|isEmpty
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|forcemajor
condition|)
block|{
comment|// do not compact old files above a configurable threshold
comment|// save all references. we MUST compact them
name|int
name|pos
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|<
name|filesToCompact
operator|.
name|size
argument_list|()
operator|&&
name|filesToCompact
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
operator|>
name|maxCompactSize
operator|&&
operator|!
name|filesToCompact
operator|.
name|get
argument_list|(
name|pos
argument_list|)
operator|.
name|isReference
argument_list|()
condition|)
operator|++
name|pos
expr_stmt|;
name|filesToCompact
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|pos
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|filesToCompact
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|this
operator|.
name|storeNameStr
operator|+
literal|": no store files to compact"
argument_list|)
expr_stmt|;
return|return
name|filesToCompact
return|;
block|}
comment|// major compact on user action or age (caveat: we have too many files)
name|boolean
name|majorcompaction
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|this
operator|.
name|maxFilesToCompact
operator|&&
operator|(
name|forcemajor
operator|||
name|isMajorCompaction
argument_list|(
name|filesToCompact
argument_list|)
operator|)
decl_stmt|;
if|if
condition|(
operator|!
name|majorcompaction
operator|&&
operator|!
name|hasReferences
argument_list|(
name|filesToCompact
argument_list|)
condition|)
block|{
comment|// we're doing a minor compaction, let's see what files are applicable
name|int
name|start
init|=
literal|0
decl_stmt|;
name|double
name|r
init|=
name|this
operator|.
name|compactRatio
decl_stmt|;
comment|// skip selection algorithm if we don't have enough files
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|this
operator|.
name|minFilesToCompact
condition|)
block|{
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
comment|/* TODO: add sorting + unit test back in when HBASE-2856 is fixed       // Sort files by size to correct when normal skew is altered by bulk load.       Collections.sort(filesToCompact, StoreFile.Comparators.FILE_SIZE);        */
comment|// get store file sizes for incremental compacting selection.
name|int
name|countOfFiles
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
index|[]
name|fileSizes
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
name|long
index|[]
name|sumSize
init|=
operator|new
name|long
index|[
name|countOfFiles
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|countOfFiles
operator|-
literal|1
init|;
name|i
operator|>=
literal|0
condition|;
operator|--
name|i
control|)
block|{
name|StoreFile
name|file
init|=
name|filesToCompact
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|fileSizes
index|[
name|i
index|]
operator|=
name|file
operator|.
name|getReader
argument_list|()
operator|.
name|length
argument_list|()
expr_stmt|;
comment|// calculate the sum of fileSizes[i,i+maxFilesToCompact-1) for algo
name|int
name|tooFar
init|=
name|i
operator|+
name|this
operator|.
name|maxFilesToCompact
operator|-
literal|1
decl_stmt|;
name|sumSize
index|[
name|i
index|]
operator|=
name|fileSizes
index|[
name|i
index|]
operator|+
operator|(
operator|(
name|i
operator|+
literal|1
operator|<
name|countOfFiles
operator|)
condition|?
name|sumSize
index|[
name|i
operator|+
literal|1
index|]
else|:
literal|0
operator|)
operator|-
operator|(
operator|(
name|tooFar
operator|<
name|countOfFiles
operator|)
condition|?
name|fileSizes
index|[
name|tooFar
index|]
else|:
literal|0
operator|)
expr_stmt|;
block|}
comment|/* Start at the oldest file and stop when you find the first file that        * meets compaction criteria:        *   (1) a recently-flushed, small file (i.e.<= minCompactSize)        *      OR        *   (2) within the compactRatio of sum(newer_files)        * Given normal skew, any newer files will also meet this criteria        *        * Additional Note:        * If fileSizes.size()>> maxFilesToCompact, we will recurse on        * compact().  Consider the oldest files first to avoid a        * situation where we always compact [end-threshold,end).  Then, the        * last file becomes an aggregate of the previous compactions.        */
while|while
condition|(
name|countOfFiles
operator|-
name|start
operator|>=
name|this
operator|.
name|minFilesToCompact
operator|&&
name|fileSizes
index|[
name|start
index|]
operator|>
name|Math
operator|.
name|max
argument_list|(
name|minCompactSize
argument_list|,
call|(
name|long
call|)
argument_list|(
name|sumSize
index|[
name|start
operator|+
literal|1
index|]
operator|*
name|r
argument_list|)
argument_list|)
condition|)
block|{
operator|++
name|start
expr_stmt|;
block|}
name|int
name|end
init|=
name|Math
operator|.
name|min
argument_list|(
name|countOfFiles
argument_list|,
name|start
operator|+
name|this
operator|.
name|maxFilesToCompact
argument_list|)
decl_stmt|;
name|long
name|totalSize
init|=
name|fileSizes
index|[
name|start
index|]
operator|+
operator|(
operator|(
name|start
operator|+
literal|1
operator|<
name|countOfFiles
operator|)
condition|?
name|sumSize
index|[
name|start
operator|+
literal|1
index|]
else|:
literal|0
operator|)
decl_stmt|;
name|filesToCompact
operator|=
name|filesToCompact
operator|.
name|subList
argument_list|(
name|start
argument_list|,
name|end
argument_list|)
expr_stmt|;
comment|// if we don't have enough files to compact, just wait
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|<
name|this
operator|.
name|minFilesToCompact
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipped compaction of "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|".  Only "
operator|+
operator|(
name|end
operator|-
name|start
operator|)
operator|+
literal|" file(s) of size "
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|totalSize
argument_list|)
operator|+
literal|" have met compaction criteria."
argument_list|)
expr_stmt|;
block|}
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
block|}
else|else
block|{
comment|// all files included in this compaction, up to max
if|if
condition|(
name|filesToCompact
operator|.
name|size
argument_list|()
operator|>
name|this
operator|.
name|maxFilesToCompact
condition|)
block|{
name|int
name|pastMax
init|=
name|filesToCompact
operator|.
name|size
argument_list|()
operator|-
name|this
operator|.
name|maxFilesToCompact
decl_stmt|;
name|filesToCompact
operator|.
name|subList
argument_list|(
literal|0
argument_list|,
name|pastMax
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|filesToCompact
return|;
block|}
comment|/**    * Do a minor/major compaction on an explicit set of storefiles in a Store.    * Uses the scan infrastructure to make it easy.    *    * @param filesToCompact which files to compact    * @param majorCompaction true to major compact (prune all deletes, max versions, etc)    * @param maxId Readers maximum sequence id.    * @return Product of compaction or null if all cells expired or deleted and    * nothing made it through the compaction.    * @throws IOException    */
specifier|private
name|StoreFile
operator|.
name|Writer
name|compactStore
parameter_list|(
specifier|final
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|filesToCompact
parameter_list|,
specifier|final
name|boolean
name|majorCompaction
parameter_list|,
specifier|final
name|long
name|maxId
parameter_list|)
throws|throws
name|IOException
block|{
comment|// calculate maximum key count after compaction (for blooms)
name|int
name|maxKeyCount
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|file
range|:
name|filesToCompact
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|file
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|!=
literal|null
condition|)
block|{
comment|// NOTE: getFilterEntries could cause under-sized blooms if the user
comment|//       switches bloom type (e.g. from ROW to ROWCOL)
name|long
name|keyCount
init|=
operator|(
name|r
operator|.
name|getBloomFilterType
argument_list|()
operator|==
name|family
operator|.
name|getBloomFilterType
argument_list|()
operator|)
condition|?
name|r
operator|.
name|getFilterEntries
argument_list|()
else|:
name|r
operator|.
name|getEntries
argument_list|()
decl_stmt|;
name|maxKeyCount
operator|+=
name|keyCount
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Compacting "
operator|+
name|file
operator|+
literal|", keycount="
operator|+
name|keyCount
operator|+
literal|", bloomtype="
operator|+
name|r
operator|.
name|getBloomFilterType
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|", size="
operator|+
name|StringUtils
operator|.
name|humanReadableInt
argument_list|(
name|r
operator|.
name|length
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// keep track of compaction progress
name|progress
operator|=
operator|new
name|CompactionProgress
argument_list|(
name|maxKeyCount
argument_list|)
expr_stmt|;
comment|// For each file, obtain a scanner:
name|List
argument_list|<
name|StoreFileScanner
argument_list|>
name|scanners
init|=
name|StoreFileScanner
operator|.
name|getScannersForStoreFiles
argument_list|(
name|filesToCompact
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|// Make the instantiation lazy in case compaction produces no product; i.e.
comment|// where all source cells are expired or deleted.
name|StoreFile
operator|.
name|Writer
name|writer
init|=
literal|null
decl_stmt|;
try|try
block|{
name|InternalScanner
name|scanner
init|=
literal|null
decl_stmt|;
try|try
block|{
name|Scan
name|scan
init|=
operator|new
name|Scan
argument_list|()
decl_stmt|;
name|scan
operator|.
name|setMaxVersions
argument_list|(
name|family
operator|.
name|getMaxVersions
argument_list|()
argument_list|)
expr_stmt|;
comment|/* include deletes, unless we are doing a major compaction */
name|scanner
operator|=
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|scanners
argument_list|,
operator|!
name|majorCompaction
argument_list|)
expr_stmt|;
if|if
condition|(
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|InternalScanner
name|cpScanner
init|=
name|region
operator|.
name|getCoprocessorHost
argument_list|()
operator|.
name|preCompact
argument_list|(
name|this
argument_list|,
name|scanner
argument_list|)
decl_stmt|;
comment|// NULL scanner returned from coprocessor hooks means skip normal processing
if|if
condition|(
name|cpScanner
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|scanner
operator|=
name|cpScanner
expr_stmt|;
block|}
name|int
name|bytesWritten
init|=
literal|0
decl_stmt|;
comment|// since scanner.next() can return 'false' but still be delivering data,
comment|// we have to use a do/while loop.
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
name|kvs
init|=
operator|new
name|ArrayList
argument_list|<
name|KeyValue
argument_list|>
argument_list|()
decl_stmt|;
comment|// Limit to "hbase.hstore.compaction.kv.max" (default 10) to avoid OOME
while|while
condition|(
name|scanner
operator|.
name|next
argument_list|(
name|kvs
argument_list|,
name|this
operator|.
name|compactionKVMax
argument_list|)
condition|)
block|{
if|if
condition|(
name|writer
operator|==
literal|null
operator|&&
operator|!
name|kvs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|writer
operator|=
name|createWriterInTmp
argument_list|(
name|maxKeyCount
argument_list|,
name|this
operator|.
name|compactionCompression
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|writer
operator|!=
literal|null
condition|)
block|{
comment|// output to writer:
for|for
control|(
name|KeyValue
name|kv
range|:
name|kvs
control|)
block|{
name|writer
operator|.
name|append
argument_list|(
name|kv
argument_list|)
expr_stmt|;
comment|// update progress per key
operator|++
name|progress
operator|.
name|currentCompactedKVs
expr_stmt|;
comment|// check periodically to see if a system stop is requested
if|if
condition|(
name|Store
operator|.
name|closeCheckInterval
operator|>
literal|0
condition|)
block|{
name|bytesWritten
operator|+=
name|kv
operator|.
name|getLength
argument_list|()
expr_stmt|;
if|if
condition|(
name|bytesWritten
operator|>
name|Store
operator|.
name|closeCheckInterval
condition|)
block|{
name|bytesWritten
operator|=
literal|0
expr_stmt|;
if|if
condition|(
operator|!
name|this
operator|.
name|region
operator|.
name|areWritesEnabled
argument_list|()
condition|)
block|{
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|writer
operator|.
name|getPath
argument_list|()
argument_list|,
literal|false
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|InterruptedIOException
argument_list|(
literal|"Aborting compaction of store "
operator|+
name|this
operator|+
literal|" in region "
operator|+
name|this
operator|.
name|region
operator|+
literal|" because user requested stop."
argument_list|)
throw|;
block|}
block|}
block|}
block|}
block|}
name|kvs
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|scanner
operator|!=
literal|null
condition|)
block|{
name|scanner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|writer
operator|!=
literal|null
condition|)
block|{
name|writer
operator|.
name|appendMetadata
argument_list|(
name|maxId
argument_list|,
name|majorCompaction
argument_list|)
expr_stmt|;
name|writer
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|writer
return|;
block|}
comment|/*    *<p>It works by processing a compaction that's been written to disk.    *    *<p>It is usually invoked at the end of a compaction, but might also be    * invoked at HStore startup, if the prior execution died midway through.    *    *<p>Moving the compacted TreeMap into place means:    *<pre>    * 1) Moving the new compacted StoreFile into place    * 2) Unload all replaced StoreFile, close and collect list to delete.    * 3) Loading the new TreeMap.    * 4) Compute new store size    *</pre>    *    * @param compactedFiles list of files that were compacted    * @param compactedFile StoreFile that is the result of the compaction    * @return StoreFile created. May be null.    * @throws IOException    */
specifier|private
name|StoreFile
name|completeCompaction
parameter_list|(
specifier|final
name|Collection
argument_list|<
name|StoreFile
argument_list|>
name|compactedFiles
parameter_list|,
specifier|final
name|StoreFile
operator|.
name|Writer
name|compactedFile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// 1. Moving the new files into place -- if there is a new file (may not
comment|// be if all cells were expired or deleted).
name|StoreFile
name|result
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|compactedFile
operator|!=
literal|null
condition|)
block|{
name|Path
name|p
init|=
literal|null
decl_stmt|;
try|try
block|{
name|p
operator|=
name|StoreFile
operator|.
name|rename
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|StoreFile
operator|.
name|getRandomFilename
argument_list|(
name|fs
argument_list|,
name|this
operator|.
name|homedir
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed move of compacted file "
operator|+
name|compactedFile
operator|.
name|getPath
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|result
operator|=
operator|new
name|StoreFile
argument_list|(
name|this
operator|.
name|fs
argument_list|,
name|p
argument_list|,
name|this
operator|.
name|conf
argument_list|,
name|this
operator|.
name|cacheConf
argument_list|,
name|this
operator|.
name|family
operator|.
name|getBloomFilterType
argument_list|()
argument_list|)
expr_stmt|;
name|result
operator|.
name|createReader
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
try|try
block|{
comment|// Change this.storefiles so it reflects new state but do not
comment|// delete old store files until we have sent out notification of
comment|// change in case old files are still being accessed by outstanding
comment|// scanners.
name|ArrayList
argument_list|<
name|StoreFile
argument_list|>
name|newStoreFiles
init|=
name|Lists
operator|.
name|newArrayList
argument_list|(
name|storefiles
argument_list|)
decl_stmt|;
name|newStoreFiles
operator|.
name|removeAll
argument_list|(
name|compactedFiles
argument_list|)
expr_stmt|;
name|filesCompacting
operator|.
name|removeAll
argument_list|(
name|compactedFiles
argument_list|)
expr_stmt|;
comment|// safe bc: lock.writeLock()
comment|// If a StoreFile result, move it into place.  May be null.
if|if
condition|(
name|result
operator|!=
literal|null
condition|)
block|{
name|newStoreFiles
operator|.
name|add
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|storefiles
operator|=
name|sortAndClone
argument_list|(
name|newStoreFiles
argument_list|)
expr_stmt|;
comment|// Tell observers that list of StoreFiles has changed.
name|notifyChangedReadersObservers
argument_list|()
expr_stmt|;
comment|// Finally, delete old store files.
for|for
control|(
name|StoreFile
name|hsf
range|:
name|compactedFiles
control|)
block|{
name|hsf
operator|.
name|deleteReader
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|=
name|RemoteExceptionHandler
operator|.
name|checkIOException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed replacing compacted files in "
operator|+
name|this
operator|.
name|storeNameStr
operator|+
literal|". Compacted file is "
operator|+
operator|(
name|result
operator|==
literal|null
condition|?
literal|"none"
else|:
name|result
operator|.
name|toString
argument_list|()
operator|)
operator|+
literal|".  Files replaced "
operator|+
name|compactedFiles
operator|.
name|toString
argument_list|()
operator|+
literal|" some of which may have been already removed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|// 4. Compute new store size
name|this
operator|.
name|storeSize
operator|=
literal|0L
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|=
literal|0L
expr_stmt|;
for|for
control|(
name|StoreFile
name|hsf
range|:
name|this
operator|.
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|hsf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|hsf
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|this
operator|.
name|storeSize
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
name|this
operator|.
name|totalUncompressedBytes
operator|+=
name|r
operator|.
name|getTotalUncompressedBytes
argument_list|()
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|writeLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
specifier|public
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|sortAndClone
parameter_list|(
name|List
argument_list|<
name|StoreFile
argument_list|>
name|storeFiles
parameter_list|)
block|{
name|Collections
operator|.
name|sort
argument_list|(
name|storeFiles
argument_list|,
name|StoreFile
operator|.
name|Comparators
operator|.
name|FLUSH_TIME
argument_list|)
expr_stmt|;
name|ImmutableList
argument_list|<
name|StoreFile
argument_list|>
name|newList
init|=
name|ImmutableList
operator|.
name|copyOf
argument_list|(
name|storeFiles
argument_list|)
decl_stmt|;
return|return
name|newList
return|;
block|}
comment|// ////////////////////////////////////////////////////////////////////////////
comment|// Accessors.
comment|// (This is the only section that is directly useful!)
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * @return the number of files in this store    */
specifier|public
name|int
name|getNumberOfstorefiles
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/*    * @param wantedVersions How many versions were asked for.    * @return wantedVersions or this families' {@link HConstants#VERSIONS}.    */
name|int
name|versionsToReturn
parameter_list|(
specifier|final
name|int
name|wantedVersions
parameter_list|)
block|{
if|if
condition|(
name|wantedVersions
operator|<=
literal|0
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Number of versions must be> 0"
argument_list|)
throw|;
block|}
comment|// Make sure we do not return more than maximum versions for this store.
name|int
name|maxVersions
init|=
name|this
operator|.
name|family
operator|.
name|getMaxVersions
argument_list|()
decl_stmt|;
return|return
name|wantedVersions
operator|>
name|maxVersions
condition|?
name|maxVersions
else|:
name|wantedVersions
return|;
block|}
specifier|static
name|boolean
name|isExpired
parameter_list|(
specifier|final
name|KeyValue
name|key
parameter_list|,
specifier|final
name|long
name|oldestTimestamp
parameter_list|)
block|{
return|return
name|key
operator|.
name|getTimestamp
argument_list|()
operator|<
name|oldestTimestamp
return|;
block|}
comment|/**    * Find the key that matches<i>row</i> exactly, or the one that immediately    * preceeds it. WARNING: Only use this method on a table where writes occur    * with strictly increasing timestamps. This method assumes this pattern of    * writes in order to make it reasonably performant.  Also our search is    * dependent on the axiom that deletes are for cells that are in the container    * that follows whether a memstore snapshot or a storefile, not for the    * current container: i.e. we'll see deletes before we come across cells we    * are to delete. Presumption is that the memstore#kvset is processed before    * memstore#snapshot and so on.    * @param row The row key of the targeted row.    * @return Found keyvalue or null if none found.    * @throws IOException    */
name|KeyValue
name|getRowKeyAtOrBefore
parameter_list|(
specifier|final
name|byte
index|[]
name|row
parameter_list|)
throws|throws
name|IOException
block|{
comment|// If minVersions is set, we will not ignore expired KVs.
comment|// As we're only looking for the latest matches, that should be OK.
comment|// With minVersions> 0 we guarantee that any KV that has any version
comment|// at all (expired or not) has at least one version that will not expire.
comment|// Note that this method used to take a KeyValue as arguments. KeyValue
comment|// can be back-dated, a row key cannot.
name|long
name|ttlToUse
init|=
name|this
operator|.
name|minVersions
operator|>
literal|0
condition|?
name|Long
operator|.
name|MAX_VALUE
else|:
name|this
operator|.
name|ttl
decl_stmt|;
name|KeyValue
name|kv
init|=
operator|new
name|KeyValue
argument_list|(
name|row
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
decl_stmt|;
name|GetClosestRowBeforeTracker
name|state
init|=
operator|new
name|GetClosestRowBeforeTracker
argument_list|(
name|this
operator|.
name|comparator
argument_list|,
name|kv
argument_list|,
name|ttlToUse
argument_list|,
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
argument_list|)
decl_stmt|;
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// First go to the memstore.  Pick up deletes and candidates.
name|this
operator|.
name|memstore
operator|.
name|getRowKeyAtOrBefore
argument_list|(
name|state
argument_list|)
expr_stmt|;
comment|// Check if match, if we got a candidate on the asked for 'kv' row.
comment|// Process each store file. Run through from newest to oldest.
for|for
control|(
name|StoreFile
name|sf
range|:
name|Lists
operator|.
name|reverse
argument_list|(
name|storefiles
argument_list|)
control|)
block|{
comment|// Update the candidate keys from the current map file
name|rowAtOrBeforeFromStoreFile
argument_list|(
name|sf
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
return|return
name|state
operator|.
name|getCandidate
argument_list|()
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Check an individual MapFile for the row at or before a given row.    * @param f    * @param state    * @throws IOException    */
specifier|private
name|void
name|rowAtOrBeforeFromStoreFile
parameter_list|(
specifier|final
name|StoreFile
name|f
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|f
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|f
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// TODO: Cache these keys rather than make each time?
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKV
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
name|KeyValue
name|firstOnRow
init|=
name|state
operator|.
name|getTargetKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|lastKV
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
block|{
comment|// If last key in file is not of the target table, no candidates in this
comment|// file.  Return.
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|lastKV
argument_list|)
condition|)
return|return;
comment|// If the row we're looking for is past the end of file, set search key to
comment|// last key. TODO: Cache last and first key rather than make each time.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|lastKV
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
block|}
comment|// Get a scanner that caches blocks and that uses pread.
name|HFileScanner
name|scanner
init|=
name|r
operator|.
name|getHFileReader
argument_list|()
operator|.
name|getScanner
argument_list|(
literal|true
argument_list|,
literal|true
argument_list|)
decl_stmt|;
comment|// Seek scanner.  If can't seek it, return.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
return|return;
comment|// If we found candidate on firstOnRow, just return. THIS WILL NEVER HAPPEN!
comment|// Unlikely that there'll be an instance of actual first row in table.
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
return|return;
comment|// If here, need to start backing up.
while|while
condition|(
name|scanner
operator|.
name|seekBefore
argument_list|(
name|firstOnRow
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|firstOnRow
operator|.
name|getKeyLength
argument_list|()
argument_list|)
condition|)
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|state
operator|.
name|isTargetTable
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
if|if
condition|(
operator|!
name|state
operator|.
name|isBetterCandidate
argument_list|(
name|kv
argument_list|)
condition|)
break|break;
comment|// Make new first on row.
name|firstOnRow
operator|=
operator|new
name|KeyValue
argument_list|(
name|kv
operator|.
name|getRow
argument_list|()
argument_list|,
name|HConstants
operator|.
name|LATEST_TIMESTAMP
argument_list|)
expr_stmt|;
comment|// Seek scanner.  If can't seek it, break.
if|if
condition|(
operator|!
name|seekToScanner
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|firstKV
argument_list|)
condition|)
break|break;
comment|// If we find something, break;
if|if
condition|(
name|walkForwardInSingleRow
argument_list|(
name|scanner
argument_list|,
name|firstOnRow
argument_list|,
name|state
argument_list|)
condition|)
break|break;
block|}
block|}
comment|/*    * Seek the file scanner to firstOnRow or first entry in file.    * @param scanner    * @param firstOnRow    * @param firstKV    * @return True if we successfully seeked scanner.    * @throws IOException    */
specifier|private
name|boolean
name|seekToScanner
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|KeyValue
name|firstKV
parameter_list|)
throws|throws
name|IOException
block|{
name|KeyValue
name|kv
init|=
name|firstOnRow
decl_stmt|;
comment|// If firstOnRow< firstKV, set to firstKV
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|firstKV
argument_list|,
name|firstOnRow
argument_list|)
operator|==
literal|0
condition|)
name|kv
operator|=
name|firstKV
expr_stmt|;
name|int
name|result
init|=
name|scanner
operator|.
name|seekTo
argument_list|(
name|kv
operator|.
name|getBuffer
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyOffset
argument_list|()
argument_list|,
name|kv
operator|.
name|getKeyLength
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|result
operator|>=
literal|0
return|;
block|}
comment|/*    * When we come in here, we are probably at the kv just before we break into    * the row that firstOnRow is on.  Usually need to increment one time to get    * on to the row we are interested in.    * @param scanner    * @param firstOnRow    * @param state    * @return True we found a candidate.    * @throws IOException    */
specifier|private
name|boolean
name|walkForwardInSingleRow
parameter_list|(
specifier|final
name|HFileScanner
name|scanner
parameter_list|,
specifier|final
name|KeyValue
name|firstOnRow
parameter_list|,
specifier|final
name|GetClosestRowBeforeTracker
name|state
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|foundCandidate
init|=
literal|false
decl_stmt|;
do|do
block|{
name|KeyValue
name|kv
init|=
name|scanner
operator|.
name|getKeyValue
argument_list|()
decl_stmt|;
comment|// If we are not in the row, skip.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
operator|<
literal|0
condition|)
continue|continue;
comment|// Did we go beyond the target row? If so break.
if|if
condition|(
name|state
operator|.
name|isTooFar
argument_list|(
name|kv
argument_list|,
name|firstOnRow
argument_list|)
condition|)
break|break;
if|if
condition|(
name|state
operator|.
name|isExpired
argument_list|(
name|kv
argument_list|)
condition|)
block|{
continue|continue;
block|}
comment|// If we added something, this row is a contender. break.
if|if
condition|(
name|state
operator|.
name|handle
argument_list|(
name|kv
argument_list|)
condition|)
block|{
name|foundCandidate
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
do|while
condition|(
name|scanner
operator|.
name|next
argument_list|()
condition|)
do|;
return|return
name|foundCandidate
return|;
block|}
specifier|public
name|boolean
name|canSplit
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// Not splitable if we find a reference store file present in the store.
for|for
control|(
name|StoreFile
name|sf
range|:
name|storefiles
control|)
block|{
if|if
condition|(
name|sf
operator|.
name|isReference
argument_list|()
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|sf
operator|+
literal|" is not splittable"
argument_list|)
expr_stmt|;
block|}
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Determines if Store should be split    * @return byte[] if store should be split, null otherwise.    */
specifier|public
name|byte
index|[]
name|getSplitPoint
parameter_list|()
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// sanity checks
if|if
condition|(
name|this
operator|.
name|storefiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
comment|// Should already be enforced by the split policy!
assert|assert
operator|!
name|this
operator|.
name|region
operator|.
name|getRegionInfo
argument_list|()
operator|.
name|isMetaRegion
argument_list|()
assert|;
comment|// Not splitable if we find a reference store file present in the store.
name|long
name|maxSize
init|=
literal|0L
decl_stmt|;
name|StoreFile
name|largestSf
init|=
literal|null
decl_stmt|;
for|for
control|(
name|StoreFile
name|sf
range|:
name|storefiles
control|)
block|{
if|if
condition|(
name|sf
operator|.
name|isReference
argument_list|()
condition|)
block|{
comment|// Should already be enforced since we return false in this case
assert|assert
literal|false
operator|:
literal|"getSplitPoint() called on a region that can't split!"
assert|;
return|return
literal|null
return|;
block|}
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|sf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|sf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|long
name|size
init|=
name|r
operator|.
name|length
argument_list|()
decl_stmt|;
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
comment|// This is the largest one so far
name|maxSize
operator|=
name|size
expr_stmt|;
name|largestSf
operator|=
name|sf
expr_stmt|;
block|}
block|}
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|largestSf
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Storefile "
operator|+
name|largestSf
operator|+
literal|" Reader is null"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// Get first, last, and mid keys.  Midkey is the key that starts block
comment|// in middle of hfile.  Has column and timestamp.  Need to return just
comment|// the row we want to split on as midkey.
name|byte
index|[]
name|midkey
init|=
name|r
operator|.
name|midkey
argument_list|()
decl_stmt|;
if|if
condition|(
name|midkey
operator|!=
literal|null
condition|)
block|{
name|KeyValue
name|mk
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|midkey
argument_list|,
literal|0
argument_list|,
name|midkey
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|fk
init|=
name|r
operator|.
name|getFirstKey
argument_list|()
decl_stmt|;
name|KeyValue
name|firstKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|fk
argument_list|,
literal|0
argument_list|,
name|fk
operator|.
name|length
argument_list|)
decl_stmt|;
name|byte
index|[]
name|lk
init|=
name|r
operator|.
name|getLastKey
argument_list|()
decl_stmt|;
name|KeyValue
name|lastKey
init|=
name|KeyValue
operator|.
name|createKeyValueFromKey
argument_list|(
name|lk
argument_list|,
literal|0
argument_list|,
name|lk
operator|.
name|length
argument_list|)
decl_stmt|;
comment|// if the midkey is the same as the first and last keys, then we cannot
comment|// (ever) split this region.
if|if
condition|(
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|firstKey
argument_list|)
operator|==
literal|0
operator|&&
name|this
operator|.
name|comparator
operator|.
name|compareRows
argument_list|(
name|mk
argument_list|,
name|lastKey
argument_list|)
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"cannot split because midkey is the same as first or "
operator|+
literal|"last row"
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
return|return
name|mk
operator|.
name|getRow
argument_list|()
return|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed getting store size for "
operator|+
name|this
operator|.
name|storeNameStr
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
comment|/** @return aggregate size of all HStores used in the last compaction */
specifier|public
name|long
name|getLastCompactSize
parameter_list|()
block|{
return|return
name|this
operator|.
name|lastCompactSize
return|;
block|}
comment|/** @return aggregate size of HStore */
specifier|public
name|long
name|getSize
parameter_list|()
block|{
return|return
name|storeSize
return|;
block|}
name|void
name|triggerMajorCompaction
parameter_list|()
block|{
name|this
operator|.
name|forceMajor
operator|=
literal|true
expr_stmt|;
block|}
name|boolean
name|getForceMajorCompaction
parameter_list|()
block|{
return|return
name|this
operator|.
name|forceMajor
return|;
block|}
comment|//////////////////////////////////////////////////////////////////////////////
comment|// File administration
comment|//////////////////////////////////////////////////////////////////////////////
comment|/**    * Return a scanner for both the memstore and the HStore files    * @throws IOException    */
specifier|public
name|KeyValueScanner
name|getScanner
parameter_list|(
name|Scan
name|scan
parameter_list|,
specifier|final
name|NavigableSet
argument_list|<
name|byte
index|[]
argument_list|>
name|targetCols
parameter_list|)
throws|throws
name|IOException
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
return|return
operator|new
name|StoreScanner
argument_list|(
name|this
argument_list|,
name|scan
argument_list|,
name|targetCols
argument_list|)
return|;
block|}
finally|finally
block|{
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|this
operator|.
name|storeNameStr
return|;
block|}
comment|/**    * @return Count of store files    */
name|int
name|getStorefilesCount
parameter_list|()
block|{
return|return
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
comment|/**    * @return The size of the store files, in bytes, uncompressed.    */
name|long
name|getStoreSizeUncompressed
parameter_list|()
block|{
return|return
name|this
operator|.
name|totalUncompressedBytes
return|;
block|}
comment|/**    * @return The size of the store files, in bytes.    */
name|long
name|getStorefilesSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|length
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * @return The size of the store file indexes, in bytes.    */
name|long
name|getStorefilesIndexSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
if|if
condition|(
name|r
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"StoreFile "
operator|+
name|s
operator|+
literal|" has a null Reader"
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|size
operator|+=
name|r
operator|.
name|indexSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * Returns the total size of all index blocks in the data block indexes,    * including the root level, intermediate levels, and the leaf level for    * multi-level indexes, or just the root level for single-level indexes.    *    * @return the total size of block indexes in the store    */
name|long
name|getTotalStaticIndexSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|size
operator|+=
name|s
operator|.
name|getReader
argument_list|()
operator|.
name|getUncompressedDataIndexSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * Returns the total byte size of all Bloom filter bit arrays. For compound    * Bloom filters even the Bloom blocks currently not loaded into the block    * cache are counted.    *    * @return the total size of all Bloom filters in the store    */
name|long
name|getTotalStaticBloomSize
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|StoreFile
name|s
range|:
name|storefiles
control|)
block|{
name|StoreFile
operator|.
name|Reader
name|r
init|=
name|s
operator|.
name|getReader
argument_list|()
decl_stmt|;
name|size
operator|+=
name|r
operator|.
name|getTotalBloomSize
argument_list|()
expr_stmt|;
block|}
return|return
name|size
return|;
block|}
comment|/**    * @return The priority that this store should have in the compaction queue    */
specifier|public
name|int
name|getCompactPriority
parameter_list|()
block|{
return|return
name|this
operator|.
name|blockingStoreFileCount
operator|-
name|this
operator|.
name|storefiles
operator|.
name|size
argument_list|()
return|;
block|}
name|HRegion
name|getHRegion
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
return|;
block|}
name|HRegionInfo
name|getHRegionInfo
parameter_list|()
block|{
return|return
name|this
operator|.
name|region
operator|.
name|regionInfo
return|;
block|}
comment|/**    * Increments the value for the given row/family/qualifier.    *    * This function will always be seen as atomic by other readers    * because it only puts a single KV to memstore. Thus no    * read/write control necessary.    *    * @param row    * @param f    * @param qualifier    * @param newValue the new value to set into memstore    * @return memstore size delta    * @throws IOException    */
specifier|public
name|long
name|updateColumnValue
parameter_list|(
name|byte
index|[]
name|row
parameter_list|,
name|byte
index|[]
name|f
parameter_list|,
name|byte
index|[]
name|qualifier
parameter_list|,
name|long
name|newValue
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
name|long
name|now
init|=
name|EnvironmentEdgeManager
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
return|return
name|this
operator|.
name|memstore
operator|.
name|updateColumnValue
argument_list|(
name|row
argument_list|,
name|f
argument_list|,
name|qualifier
argument_list|,
name|newValue
argument_list|,
name|now
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Adds or replaces the specified KeyValues.    *<p>    * For each KeyValue specified, if a cell with the same row, family, and    * qualifier exists in MemStore, it will be replaced.  Otherwise, it will just    * be inserted to MemStore.    *<p>    * This operation is atomic on each KeyValue (row/family/qualifier) but not    * necessarily atomic across all of them.    * @param kvs    * @return memstore size delta    * @throws IOException    */
specifier|public
name|long
name|upsert
parameter_list|(
name|List
argument_list|<
name|KeyValue
argument_list|>
name|kvs
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|lock
argument_list|()
expr_stmt|;
try|try
block|{
comment|// TODO: Make this operation atomic w/ RWCC
return|return
name|this
operator|.
name|memstore
operator|.
name|upsert
argument_list|(
name|kvs
argument_list|)
return|;
block|}
finally|finally
block|{
name|this
operator|.
name|lock
operator|.
name|readLock
argument_list|()
operator|.
name|unlock
argument_list|()
expr_stmt|;
block|}
block|}
specifier|public
name|StoreFlusher
name|getStoreFlusher
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
return|return
operator|new
name|StoreFlusherImpl
argument_list|(
name|cacheFlushId
argument_list|)
return|;
block|}
specifier|private
class|class
name|StoreFlusherImpl
implements|implements
name|StoreFlusher
block|{
specifier|private
name|long
name|cacheFlushId
decl_stmt|;
specifier|private
name|SortedSet
argument_list|<
name|KeyValue
argument_list|>
name|snapshot
decl_stmt|;
specifier|private
name|StoreFile
name|storeFile
decl_stmt|;
specifier|private
name|TimeRangeTracker
name|snapshotTimeRangeTracker
decl_stmt|;
specifier|private
name|StoreFlusherImpl
parameter_list|(
name|long
name|cacheFlushId
parameter_list|)
block|{
name|this
operator|.
name|cacheFlushId
operator|=
name|cacheFlushId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|prepare
parameter_list|()
block|{
name|memstore
operator|.
name|snapshot
argument_list|()
expr_stmt|;
name|this
operator|.
name|snapshot
operator|=
name|memstore
operator|.
name|getSnapshot
argument_list|()
expr_stmt|;
name|this
operator|.
name|snapshotTimeRangeTracker
operator|=
name|memstore
operator|.
name|getSnapshotTimeRangeTracker
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|flushCache
parameter_list|(
name|MonitoredTask
name|status
parameter_list|)
throws|throws
name|IOException
block|{
name|storeFile
operator|=
name|Store
operator|.
name|this
operator|.
name|flushCache
argument_list|(
name|cacheFlushId
argument_list|,
name|snapshot
argument_list|,
name|snapshotTimeRangeTracker
argument_list|,
name|status
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|commit
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|storeFile
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Add new file to store files.  Clear snapshot too while we have
comment|// the Store write lock.
return|return
name|Store
operator|.
name|this
operator|.
name|updateStorefiles
argument_list|(
name|storeFile
argument_list|,
name|snapshot
argument_list|)
return|;
block|}
block|}
comment|/**    * See if there's too much store files in this store    * @return true if number of store files is greater than    *  the number defined in minFilesToCompact    */
specifier|public
name|boolean
name|needsCompaction
parameter_list|()
block|{
return|return
operator|(
name|storefiles
operator|.
name|size
argument_list|()
operator|-
name|filesCompacting
operator|.
name|size
argument_list|()
operator|)
operator|>
name|minFilesToCompact
return|;
block|}
specifier|public
specifier|static
specifier|final
name|long
name|FIXED_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|ClassSize
operator|.
name|OBJECT
operator|+
operator|(
literal|17
operator|*
name|ClassSize
operator|.
name|REFERENCE
operator|)
operator|+
operator|(
literal|7
operator|*
name|Bytes
operator|.
name|SIZEOF_LONG
operator|)
operator|+
operator|(
literal|1
operator|*
name|Bytes
operator|.
name|SIZEOF_DOUBLE
operator|)
operator|+
operator|(
literal|7
operator|*
name|Bytes
operator|.
name|SIZEOF_INT
operator|)
operator|+
operator|(
literal|1
operator|*
name|Bytes
operator|.
name|SIZEOF_BOOLEAN
operator|)
argument_list|)
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|long
name|DEEP_OVERHEAD
init|=
name|ClassSize
operator|.
name|align
argument_list|(
name|FIXED_OVERHEAD
operator|+
name|ClassSize
operator|.
name|OBJECT
operator|+
name|ClassSize
operator|.
name|REENTRANT_LOCK
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP
operator|+
name|ClassSize
operator|.
name|CONCURRENT_SKIPLISTMAP_ENTRY
operator|+
name|ClassSize
operator|.
name|OBJECT
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|long
name|heapSize
parameter_list|()
block|{
return|return
name|DEEP_OVERHEAD
operator|+
name|this
operator|.
name|memstore
operator|.
name|heapSize
argument_list|()
return|;
block|}
specifier|public
name|KeyValue
operator|.
name|KVComparator
name|getComparator
parameter_list|()
block|{
return|return
name|comparator
return|;
block|}
block|}
end_class

end_unit

